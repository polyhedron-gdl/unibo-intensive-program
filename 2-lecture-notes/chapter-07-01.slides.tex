\documentclass[11pt]{beamer}
\usetheme{CambridgeUS}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{pgfpages}
\usepackage{framed}
\usepackage{xcolor}
\usepackage[most]{tcolorbox}
\usepackage{soul}
\usepackage{empheq}
\usepackage{minted}
\usepackage{xmpmulti}

% The replacement character ï¿½ (often displayed as a black rhombus with a white
% question mark) is a symbol found in the Unicode standard at code point U
% +FFFD in the Specials table. It is used to indicate problems when a system 
% is unable to render a stream of data to a correct symbol.[4] It is usually 
% seen when the data is invalid and does not match any character. For this 
% reason we map explicitly this character to a blanck space.
\DeclareUnicodeCharacter{FFFD}{ }

\newcommand*{\itemimg}[1]{%
  \raisebox{-.3\baselineskip}{%
    \includegraphics[
      height=\baselineskip,
      width=\baselineskip,
      keepaspectratio,
    ]{#1}%
  }%
}

\newtcbox{\mymath}[1][]{%
    nobeforeafter, math upper, tcbox raise base,
    enhanced, colframe=blue!30!black,
    colback=blue!10, boxrule=1pt,
    #1}

\newcommand{\highlight}[1]{%
  \colorbox{yellow!100}{$\displaystyle#1$}}

\author{Giovanni Della Lunga\\{\footnotesize giovanni.dellalunga@gmail.com}}
%\title{4.1 - Linear and Logistic Regression}
%\title{4.2 - Decision Trees}
\title{Introduction to Deep Learning}
%\title{6 - Text Vectorization}
%\title{7 - Classification for Text Analysis}
%\title{8 - Clustering for Text Similarity}
%\title{9 - Information Extraction}
\subtitle{} % (optional)
\setbeamercovered{transparent} 
\institute{ADVANCED TOPICS IN ARTIFICIAL INTELLIGENCE} 
\date{Bologna March 04-22, 2024} 
\begin{document}


\begin{frame}
\titlepage
\end{frame}

\AtBeginSection[]
{
  %\begin{frame}<beamer>
  %\footnotesize	
  %\frametitle{Outline}
  %\begin{multicols}{2}
  %\tableofcontents[currentsection]
  %\end{multicols}	  
  %\normalsize
  %\end{frame}
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}  	\usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\AtBeginSubsection{\frame{\subsectionpage}}

% INSERT HERE

\begin{frame}{We will talk about...}
In this lesson, we will cover the following topics:
\begin{itemize}
\item Introduction to Deep Learning. We will start by introducing the concept of deep learning, its history, and its applications.
\item What is a Neural Network? We will delve into the fundamental building block of deep learning, the neural network. We will cover the architecture of a neural network and its different components.
\item The Feed Forward Architecture. We will explore the simplest type of neural network, the feed-forward architecture, and learn how it can be used to solve a variety of problems. 
\item  We will introduce Keras, a popular deep learning library, and learn how to use it to build and train neural networks.
\item We will cover the basics of sequential data, such as time series, and explore how neural networks can be used to model and predict such data.
\end{itemize}
\end{frame}
%..................................................................
%=====================================================================
\section{What is Deep Learning?}
%=====================================================================
%______________________________________________________________________________
%
%\subsection{Machine Learning and Deep Learning}
%______________________________________________________________________________
%
\begin{frame}{Introduction: Machine Learning and Deep Learning}
	\begin{itemize}
		\item A deep-learning model transforms its input data into meaningful outputs, a process that is \textbf{learned} from exposure to known examples of inputs and outputs. 
		\item Therefore, the central problem in deep learning is to \textbf{meaningfully transform data}: in other words, \textbf{to learn useful representations of the input data at hand, representations that get us closer to the expected output}. 
		\item What is a \textbf{representation}? At its core, it is simply a different way to look at data, to represent or encode data.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Introduction: Machine Learning and Deep Learning}
The central problem in deep learning is to \textbf{meaningfully transform data}

	\begin{center}
	\includegraphics[scale=0.6]{../5-pictures/03_intro_to_deep_learning_pic_25.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Machine Learning and Deep Learning}
\begin{itemize}
\item From a more formal poin of view, deep learning learning process involves input variables, which we call $X$, and output variables, which we call $Y$. 

\item We use neural network \highlight{\text{to learn the mapping function}} from the input to the output. 

\item In simple mathematics, the output $Y$ is a dependent variable of input $X$ as illustrated by:

$$Y = f(X)$$
\end{itemize}

\begin{tcolorbox}
Here, our end goal is to try to \textbf{approximate the mapping function} $f$, so that we can \textbf{predict} the output variables $Y$ when we have new input data $X$.
\end{tcolorbox}
\end{frame}
%..................................................................
\begin{frame}{Introduction: Machine Learning and Deep Learning}
\footnotesize{
	Neural networks are typically composed of several layers of interconnected nodes, with each layer performing a specific transformation on the data. The input data is fed into the first layer, and the output of that layer becomes the input to the next layer, and so on, until the final output is produced...}
	\begin{center}
	\includegraphics[scale=0.7]{../5-pictures/03_intro_to_deep_learning_pic_1.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Introduction: Machine Learning and Deep Learning}
\footnotesize{
	Neural networks are typically composed of several layers of interconnected nodes, with each layer performing a specific transformation on the data. The input data is fed into the first layer, and the output of that layer becomes the input to the next layer, and so on, until the final output is produced...}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/03_intro_to_deep_learning_pic_2.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Introduction: Machine Learning and Deep Learning}
\footnotesize{
	Neural networks are typically composed of several layers of interconnected nodes, with each layer performing a specific transformation on the data. The input data is fed into the first layer, and the output of that layer becomes the input to the next layer, and so on, until the final output is produced...}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/03_intro_to_deep_learning_pic_3.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Introduction: Machine Learning and Deep Learning}
\footnotesize{
	Neural networks are typically composed of several layers of interconnected nodes, with each layer performing a specific transformation on the data. The input data is fed into the first layer, and the output of that layer becomes the input to the next layer, and so on, until the final output is produced...}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_26.png}
	\end{center}
\end{frame}
%=====================================================================
\section{Neural Networks}
%=====================================================================
%______________________________________________________________________________
%
\subsection{The McCulloch-Pitts Neuron \\ \scalebox{0.8}{	}}
%______________________________________________________________________________
%
\begin{frame}{Mc-Culloch and Pitts Neuron}
\begin{itemize}
\item The McCulloch-Pitts neuron, also known as the binary threshold neuron, is a simple mathematical model of a biological neuron that was introduced by Warren McCulloch and Walter Pitts in 1943. It is one of the earliest and most fundamental models of artificial neural networks.

\item The McCulloch-Pitts neuron is a computational unit that takes in one or more binary inputs, sums them up, and applies a binary threshold function to produce a binary output. The threshold function is such that if the weighted sum of the inputs is greater than a certain threshold value, the neuron outputs a 1 (i.e., it fires), and otherwise, it outputs a 0 (i.e., it remains inactive).
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Mc-Culloch and Pitts Neuron}
\begin{itemize}
\item The McCulloch-Pitts neuron can be used to model basic logic operations, such as AND, OR, and NOT, and can be combined in various ways to build more complex computational systems. While the McCulloch-Pitts neuron is a highly simplified model of a biological neuron, it serves as a foundation for more sophisticated neural network models, such as the perceptron, which was introduced a few years later by Frank Rosenblatt.
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Mc-Culloch and Pitts Neuron}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/03_intro_to_deep_learning_pic_4.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_5.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_6.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_7.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Mc-Culloch and Pitts Neuron}
From a functional point of view
	\begin{itemize}
		\item an input signal formally present but associated with a zero weight is equivalent to an absence of signal;
		\item the threshold can be considered as an additional synapse, connected in input with a fixed weight equal to 1;
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_8.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_9.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_10.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_11.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
\begin{equation} a = \sum\limits_{i=1}^n w_i \, x_i - \theta \end{equation}
\begin{equation} y = f(a) = \begin{cases} 0, \quad \text{if} \, a \le 0 \\ 1, \quad \text{if} \, a > 0\end{cases} \end{equation}
	\begin{itemize}
		\item The function $f$ is called the response or activation function:
		\item in the McCulloch and Pitts neuron $f$ is simply the step function, so the answer is binary: it is $1$ if the weighted sum of the stimuli exceeds the internal threshold; $0$ otherwise.
		\item Other models of artificial neurons predict continuous response functions
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_12.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_13.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
\begin{itemize}
\item The non-linear nature of these functions is essential for neural networks to learn from complex data. 

\item If we only used linear activation functions, no matter how many layers we stacked, the network would behave just like a single-layer perceptron because the composition of linear functions is still a linear function. 

\item This limits the complexity of tasks the network can learn. 

\item Non-linear activation functions, on the other hand, enable the network to learn complex patterns and solve intricate problems by adding layers of abstraction.
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_27.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}

For the derivatives of the activation functions the following rules apply (see notebook chapter-07-01 appendix for details):\\

\textbf{Derivative of Hyperbolic Tangent}

\begin{equation}
\tanh x = \frac{{{e^x} â {e^{ â x}}}}{{{e^x} + {e^{ â x}}}} 
\Rightarrow 
\frac{d}{{dx}}\tanh x =  1 - {\left(\tanh x \right)}^2 
\end{equation}\\

\textbf{Derivative of Sigmoid Function} 

\begin{equation}
\sigma(x) =  \left[ \dfrac{1}{1 + e^{-x}} \right]  \Rightarrow
\dfrac{d}{dx} \sigma(x) =  \sigma(x) \cdot (1 - \sigma(x))
\end{equation}

\end{frame}
%
\subsection{The Feed-Forward Neural Network}
%______________________________________________________________________________
%
\begin{frame}{Neural Network Basic Constituents}
More generally, a neural network consists of:
	\begin{itemize}
		\item A set of nodes (neurons), or units connected by links.
		\item A set of \textbf{weights} associated with links.
		\item A set of thresholds or activation levels.
\end{itemize}
Neural network design requires:
\begin{itemize}
		\item 1. The choice of the number and type of units.
		\item 2. The determination of the morphological structure.
		\item 3. Coding of training examples, in terms of network inputs and outputs.
		\item 4. Initialization and training of weights on interconnections, through the set of learning examples.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{A Simple Neural Network}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
\begin{itemize}
		\item Feedforward neural networks are what weâve primarily been focusing on within this section. 
		
		\item They are comprised of an input layer, a hidden layer or layers, and an output layer. 
		
		\item Data is fed into these models through the input layer.  
		
\end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../5-pictures/03_intro_to_deep_learning_pic_18.png}
    %}
\end{column}%
\end{columns}
\end{frame}
%..................................................................
\begin{frame}{Implementing a Single Layer NN}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
\begin{itemize}
\item In each hidden unit, take $a_1$ as example, \textbf{a linear operation followed by an activation function, $f$, is performed}. 
\item So given input $x = (x_1, x_2)$, \textbf{inside node $a_1$}, we have:
\begin{align*}
&z_1 = w_{11}x_1 + w_{12}x_2 + b_1 \\
&a_1 = f(w_{11}x_1 + w_{12}x_2 + b_1) = f(z_1) 
\end{align*}
\end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../5-pictures/03_intro_to_deep_learning_pic_18.png}
    %}
\end{column}%
\end{columns}
\end{frame}
%..................................................................
\begin{frame}{Implementing a Single Layer NN}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
\begin{itemize}
\item Same for node $a_2$, it would have:
\begin{align*}
&z_2 = w_{21}x_1 + w_{22}x_2 + b_2    \\
&a_2  = f(w_{21}x_1 + w_{22}x_2 + b_2) = f(z_2) 
\end{align*}	
\item And same for $a_3$ and $a_4$ and so on

\end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../5-pictures/03_intro_to_deep_learning_pic_19.png}
    %}
\end{column}%
\end{columns}
\end{frame}
%..................................................................
\begin{frame}{Implementing a Single Layer NN}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
\begin{itemize}
\item The output is one single value $y_1$ in $[0, 1]$. 
\item We can think of this simple example as a binary classification 
task with a prediction of probability
\end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../5-pictures/03_intro_to_deep_learning_pic_19.png}
    %}
\end{column}%
\end{columns}
\end{frame}
%..................................................................
\begin{frame}{Implementing a single Layer NN}
We can also write in a more compact form the computation of the pre-activation function performed by the hidden layer:
\begin{equation}
\begin{pmatrix}
z_1 \\ z_2 \\ z_3 \\ z_4
\end{pmatrix} =
\begin{pmatrix}
w_{11} & w_{12} \\ w_{21} & w_{22} \\ w_{31} & w_{32} \\ w_{41} & w_{42}
\end{pmatrix} 
\cdot 
\begin{pmatrix}
x_1 \\ x_2 
\end{pmatrix}
+
\begin{pmatrix}
b_1 \\ b_2 \\ b_3 \\ b_4
\end{pmatrix} 
\Rightarrow Z^{[1]} = W^{[1]} \cdot X + B^{[1]} 
\end{equation}

\end{frame}
%..................................................................
\begin{frame}{Implementing a single Layer NN}
Let's assume that the first activation function is the $\tanh$ and the output activation function is the $sigmoid$. So the result of the hidden layer is:
$$ A^{[1]} = \tanh{Z^{[1]}} $$
This result is applied to the output node which will perform another linear operation with a different set of weights, $W^{[2]}$:
$$ Z^{[2]} = W^{[2]} \cdot A^{[1]} + B^{[2]} $$
and the final output will be the result of the application of the output node activation function (the sigmoid) to this value:
$$ \hat{y} = \sigma({Z^{[2]}}) = A^{[2]}$$
\end{frame}
%..................................................................
\begin{frame}{Implementing a single Layer NN}
\begin{center}
\includegraphics[scale=.4]{../5-pictures/03_intro_to_deep_learning_pic_18_b.png} \end{center}
\end{frame}
%..................................................................
\begin{frame}{Implementing a single Layer NN}
\begin{center}
\includegraphics[scale=.4]{../5-pictures/03_intro_to_deep_learning_pic_18_c.png} \end{center}
\end{frame}
%..................................................................
\begin{frame}{Implementing a single Layer NN}
\begin{center}
\includegraphics[scale=.4]{../5-pictures/03_intro_to_deep_learning_pic_18_d.png} \end{center}
\end{frame}
%..................................................................
\begin{frame}{Implementing a single Layer NN}
\begin{center}
\includegraphics[scale=.4]{../5-pictures/03_intro_to_deep_learning_pic_18_e.png} \end{center}
\end{frame}
%..................................................................
\begin{frame}{Neural Network Basic Constituents}
\scriptsize

\begin{align*}
\hat y &= A^{[2]} = \sigma\left(Z^{[2]}\right) = \sigma \left( W^{[2]} \cdot A^{[1]} + B^{[2]} \right) \\
&= \sigma \left( W^{[2]} \cdot \tanh{Z^{[1]}} + B^{[2]} \right) \\
&= \sigma \left[ W^{[2]} \cdot \tanh\left( W^{[1]} \cdot X + B^{[1]} \right) + B^{[2]} \right]
\end{align*}

\rule{\textwidth}{1pt}
\begin{itemize}
		\item The specification of what a layer does to its input data is stored in the layer's weights, which in essence are a bunch of numbers. 
		\item In technical terms, we could say that the transformation implemented by a layer is parameterized by its weights (Weights are also sometimes called the parameters of a layer.) 
		\item In this context, \textbf{learning means finding a set of values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets}.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Loss Function}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item To control the output of a neural network, you need to be able to measure how far this output is from what you expected. 
		\item This is the job of the \textbf{loss function} of the network, also called the objective function. 
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../5-pictures/03_intro_to_deep_learning_pic_14.png}
    %}
\end{column}%
\end{columns}
\end{frame}
%..................................................................
\begin{frame}{Loss Function}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item The loss function takes the predictions of the network and the true target (what you wanted the network to output) and \textbf{computes a distance score, capturing how well the network has done on this specific example}
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../5-pictures/03_intro_to_deep_learning_pic_15.png}
    %}
\end{column}%
\end{columns}
\end{frame}
%..................................................................
\begin{frame}{Backpropagation}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item The fundamental trick in deep learning is to use this score as a feedback signal to adjust the value of the weights a little, in a direction that will lower the loss score for the current example. 
		\item This adjustment is the job of the optimizer, which implements what is called the Backpropagation algorithm: the central algorithm in deep learning.
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../5-pictures/03_intro_to_deep_learning_pic_17.png}
    %}
\end{column}%
\end{columns}
\end{frame}
%..................................................................
\begin{frame}{Loss Function}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/03_intro_to_deep_learning_pic_16.png}
	\end{center}
\end{frame}
%______________________________________________________________________________
%
%\subsection{Implementing a Single Layer NN}
%______________________________________________________________________________
%
\begin{frame}[fragile]
\frametitle{Weights Initialization}
\begin{itemize}
\item Our neural network has 1 hidden layer and 2 layers in total (hidden layer + output layer), so there are 4 weight matrices to initialize ( $W^{[1]}$,$b^{[1]}$  and  $W^{[2]}$,$b^{[2]}$). 
\item Notice that the weights are initialized relatively small so that the gradients would be higher thus learning faster in the beginning phase.
\end{itemize}
\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

def init_weights(n_input, n_hidden, n_output):
    params = {}
    params['W1'] = np.random.randn(n_hidden, n_input) * 0.01
    params['b1'] = np.zeros((n_hidden, 1))
    params['W2'] = np.random.randn(n_output, n_hidden) * 0.01
    params['b2'] = np.zeros((n_output, 1))
    
    return params
\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Weights Initialization}
\begin{itemize}
\item Our neural network has 1 hidden layer and 2 layers in total (hidden layer + output layer), so there are 4 weight matrices to initialize ( $W^{[1]}$,$b^{[1]}$  and  $W^{[2]}$,$b^{[2]}$). 
\item Notice that the weights are initialized relatively small so that the gradients would be higher thus learning faster in the beginning phase.
\end{itemize}
\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

params = init_weights(20, 10, 1)

print('W1 shape', params['W1'].shape)
print('b1 shape', params['b1'].shape)
print('W2 shape', params['W2'].shape)
print('b2 shape', params['b2'].shape)

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Forward Propagation}
\scriptsize

\begin{align*}
\hat y &= A^{[2]} = \sigma\left(Z^{[2]}\right) = \sigma \left( W^{[2]} \cdot A^{[1]} + B^{[2]} \right) \\
&= \sigma \left( W^{[2]} \cdot \tanh{Z^{[1]}} + B^{[2]} \right) \\
&= \sigma \left[ W^{[2]} \cdot \tanh\left( W^{[1]} \cdot X + B^{[1]} \right) + B^{[2]} \right]
\end{align*}

\rule{\textwidth}{1pt}
\begin{minted}{python}

def forward(X, params):
    W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']
    A0 = X
    cache = {}
    Z1 = np.dot(W1, A0) + b1
    A1 = tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    cache['Z1'] = Z1
    cache['A1'] = A1
    cache['Z2'] = Z2
    cache['A2'] = A2
    return  cache

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}{Activation Functions}
Function tanh and sigmoid looks as below. Notice that the only difference of these functions is the scale of y.
	\begin{center}
	\includegraphics[scale=0.7]{../5-pictures/03_intro_to_deep_learning_pic_19_b.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Logistic Loss Function}
Since we have a binary classification problem, we can assume a Logistic Loss Function (see the problem of logistic regression)
\begin{equation}
L(y, \hat{y}) = 
\begin{cases} 
-\log{\hat{y}} & \text{when}\, y = 1 \\ -\log(1 - \hat{y}) & \text{when}\, y = 0 
\end{cases} 
\end{equation}
$$ L(y, \hat{y}) = -[y\log{\hat{y}} + (1 - y)\log{(1 - \hat{y})}] $$

Where $\hat y$ is our \highlight{\text{prediction}} ranging in $[0, 1]$ and $y$ is the \highlight{\text{true}} value. 

\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Logistic Loss Function}

$$ L(y, \hat{y}) = -[y\log{\hat{y}} + (1 - y)\log{(1 - \hat{y})}] $$

\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

def loss(Y, Y_hat):
    """
    Y: vector of true value
    Y_hat: vector of predicted value
    """
    assert Y.shape[0] == 1
    assert Y.shape == Y_hat.shape
    m = Y.shape[1]
    s = Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)
    loss = -np.sum(s) / m
    return loss

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
\begin{itemize}
\item Given a generic actual value $y$, we want to minimize the loss $L$, and the technic we are going to apply here is gradient descent; 
\item basically what we need to do is to apply derivative to our variables and move them slightly down to the optimum. 
\item Here we have 2 variables, $W$ and $b$, and for this example, the update formula of them would be:

\begin{align*}
W_{new} = W_{old} - \frac{\partial L}{\partial W} &\Rightarrow \Delta W = - \frac{\partial L}{\partial W} \\
b_{new} = b_{old} - \frac{\partial L}{\partial b} &\Rightarrow \Delta b = - \frac{\partial L}{\partial b} \\
\end{align*}

\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
\begin{itemize}
\item The delta rule algorithm works by computing the gradient of the loss function with respect to each weight. 
\item Remember that
\begin{align*}
\hat y &= A^{[2]} = \sigma\left(Z^{[2]}\right) = \sigma \left( W^{[2]} \cdot A^{[1]} + B^{[2]} \right) \\
&= \sigma \left( W^{[2]} \cdot \tanh{Z^{[1]}} + B^{[2]} \right) \\
&= \sigma \left[ W^{[2]} \cdot \tanh\left( W^{[1]} \cdot X + B^{[1]} \right) + B^{[2]} \right]
\end{align*}
\end{itemize}
\begin{tcolorbox}
As you can see $\hat y$ depends on both $W^{[1]}$ and $W^{[2]}$. The specification of what a layer does to its input data is stored in the layer's weights. Remember once again that \textbf{learning means finding a set of values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets} 
\end{tcolorbox}
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
The delta rule algorithm works by computing the gradient of the loss function with respect to each weight. In order to get the derivative of our targets, chain rules would be applied:

\begin{align}
& \Delta W^{[1]} = - \frac{\partial L}{\partial W^{[1]}} \\
& \Delta W^{[2]} = - \frac{\partial L}{\partial W^{[2]}} \\
& \frac{\partial L}{\partial W^{[i]}} =  \frac{\partial L}{\partial \hat y} \frac{\partial \hat y}{\partial Z^{[2]}} \frac{\partial Z^{[2]}}{\partial W^{[i]}} 
\end{align}

where $i=1,2$
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
Given the loss function $L$ we defined above, we can easily compute the first partial derivative with respect to $\hat y$:

\begin{align*}
L(y, \hat{y}) &= -[y\log{\hat{y}} + (1 - y)\log{(1 - \hat{y})}]\\ 
&\Downarrow \\
\frac{\partial L}{\partial \hat y} &= -\frac{y}{\hat y} + \frac{1-y}{1-\hat y} = \frac{\hat y - y}{\hat y(1 - \hat y)}
\end{align*}

Remeber that in our notation we have:

$$\hat y = \sigma\left( Z^{[2]} \right) = A^{[2]}$$
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
So we can write for the two first partial derivatives of our chain:

\begin{align*}
&\frac{\partial L}{\partial \hat y} =  \frac{A^{[2]} - y}{A^{[2]}\left(1 - A^{[2]}\right)}
\end{align*}

\begin{align*}
\frac{\partial \hat y}{\partial Z^{[2]}} &= \frac{\partial \sigma\left( Z^{[2]} \right)}{\partial Z^{[2]}} \\
&= \sigma\left(Z^{[2]}\right) \cdot \left[1 - \sigma\left(Z^{[2]}\right)\right]\\
&= {\hat y}\, (1 - \hat y) = A^{[2]} \left( 1 - A^{[2]} \right)
\end{align*}
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
putting it all together

\begin{align*}
\frac{\partial L}{\partial W^{[i]}} 
& = \frac{\partial L}{\partial \hat y} \cdot \frac{\partial \hat y}{\partial Z^{[2]}} \cdot \frac{\partial Z^{[2]}}{\partial W^{[i]}} \\
& = \frac{A^{[2]} - y}{A^{[2]}\left(1 - A^{[2]}\right)} \cdot A^{[2]} \left( 1 - A^{[2]} \right) \cdot  \frac{\partial Z^{[2]}}{\partial W^{[i]}} \\
& = \left( A^{[2]} - y \right) \cdot \frac{\partial Z^{[2]}}{\partial W^{[i]}}
\end{align*}

where $i=1,2$
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
Now we have only to compute the partial derivatives of the pre-activation function $Z^{[2]}$ with respect to $W^{[1]}$ and $W^{[2]}$. \\

Remember that 

$$Z^{[2]} = W^{[2]} \cdot A^{[1]}  + B^{[2]} = W^{[2]} \cdot \tanh\left( W^{[1]} \cdot X + B^{[1]} \right) + B^{[2]}$$

where

$$A^{[1]} = \tanh\left( W^{[1]} \cdot X + B^{[1]}\right) = \tanh\left( Z^{[1]} \right)$$

\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
We have

\begin{align*}
& \frac{\partial Z^{[2]}}{\partial W^{[2]}} = A^{[1]} \\
& \frac{\partial Z^{[2]}}{\partial W^{[1]}} = W^{[2]} \cdot \frac{\partial \tanh\left(Z^{[1]} \right)}{\partial Z^{[1]}} \cdot \frac{\partial Z^{[1]}}{\partial W^{[1]}}
\end{align*}

\begin{equation}
\left\{
\begin{aligned}
& \frac{\partial \tanh\left(Z^{[1]} \right)}{\partial Z^{[1]}}  = 1 - \left[ \tanh\left(Z^{[1]} \right) \right]^2 = 
1 - \left( A^{[1]} \right)^2 \\
& \frac{\partial Z^{[1]}}{\partial W^{[1]}} = X
\end{aligned}
\right.
\end{equation} 

\begin{align*}
\frac{\partial Z^{[2]}}{\partial W^{[1]}} = W^{[2]} \cdot X \cdot \left[ 1 - \left( A^{[1]} \right)^2 \right]
\end{align*}

\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
Now we have all the elements to calculate the derivative of the loss function with respect to the weights \\\\

\begin{tcolorbox}
\begin{align*}
& \frac{\partial L}{\partial W^{[1]}} = \left( A^{[2]} - y \right) \cdot \frac{\partial Z^{[2]}}{\partial W^{[1]}} = \left( A^{[2]} - y \right) \cdot W^{[2]} \cdot X \cdot \left[ 1 - \left( A^{[1]} \right)^2 \right] \\
& \frac{\partial L}{\partial W^{[2]}} = \left( A^{[2]} - y \right) \cdot \frac{\partial Z^{[2]}}{\partial W^{[2]}} = 
\left( A^{[2]} - y \right) \cdot A^{[1]}
\end{align*}
\end{tcolorbox}
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
In the python code we put also:

\begin{align*} 
& \Delta^{[2]} = A^{[2]} - y  \\
& \Delta^{[1]} = \Delta^{[2]} \cdot W^{[2]} \cdot (1 - A^{[1]^2})
\end{align*}

So we finally have

\begin{align*}
& \Delta W^{[2]} = -\frac{\partial L}{\partial W^{[2]}} = - \Delta^{[2]} \cdot A^{[1]} \\
& \Delta W^{[1]} = -\frac{\partial L}{\partial W^{[1]}} = - \Delta^{[1]} \cdot X
\end{align*}

\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Weights Update}
\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

def backward(params, cache, X, Y):
    m = X.shape[1]
    W1 = params['W1']
    W2 = params['W2']
    A1 = cache['A1']
    A2 = cache['A2']
    DL2 = A2 - Y
    dW2 = (1 / m) * np.dot(DL2, A1.T)
    db2 = (1 / m) * np.sum(DL2, axis=1, keepdims=True)
    DL1 = np.multiply(np.dot(W2.T, DL2), 1 - np.power(A1, 2))
    dW1 = (1 / m) * np.dot(DL1, X.T)
    db1 = (1 / m) * np.sum(DL1, axis=1, keepdims=True)
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}

    return grads
\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}{Batch Training}
\begin{itemize}
\item In actual training processes, a batch is trained instead of 1 at a time. The change applied in the formula is trivial, we just need to replace the single vector $x$ with a matrix $X$ with size $n \times m$, where $n$ is number of features and $m$ is the the batch size, samples are stacked column wise, and the following result matrix are applied likewise.
\item Also the loss function is the same as logistic regression, but for batch training, we'll take the average loss for all training samples.
\item The same it's true for all the other calculation, this explain the presence of the factor $1/m$;
\end{itemize}
\end{frame}
%______________________________________________________________________________
%
\subsection{Keras \\ \scalebox{0.8}{}}
%______________________________________________________________________________
%
\begin{frame}{Keras}
Keras is a popular open-source deep learning library that is written in Python. It is designed to be user-friendly, modular, and extensible, which makes it a great tool for beginners and experts alike. Here are some key points about Keras:
\begin{itemize}
\item User-friendly API: Keras provides a high-level API that makes it easy to build and train deep learning models. It supports both CPU and GPU computations, and it runs seamlessly on different platforms.
\item Modular architecture: Keras is built on a modular architecture, which means that it provides a set of building blocks that you can combine to create complex deep learning models. This allows you to experiment with different architectures and see what works best for your problem.
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Keras}
\begin{itemize}
\item Supports multiple backends: Keras supports multiple backends, including TensorFlow, CNTK, and Theano. This allows you to choose the backend that works best for your needs.
\item Pre-built models and layers: Keras provides a set of pre-built models and layers that you can use to jumpstart your deep learning projects. These models and layers are optimized for different tasks, such as image recognition, natural language processing, and time series analysis.
\item Easy customization: Keras allows you to customize your models and layers easily. You can add or remove layers, change the hyperparameters, and fine-tune the models to improve their performance.
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Keras}
\begin{itemize}
\item Built-in utilities: Keras provides a set of built-in utilities that make it easy to preprocess your data, visualize your models, and monitor your training progress. These utilities can save you a lot of time and effort when working on deep learning projects.

\item Large community: Keras has a large and active community of users and contributors, who share their knowledge and experience through forums, blogs, and tutorials. This means that you can find resources and help easily when you run into issues or have questions.

\item Integration with other libraries: Keras integrates seamlessly with other Python libraries, such as NumPy, Pandas, and Scikit-learn. This allows you to use these libraries together with Keras to preprocess your data, analyze your results, and build end-to-end deep learning pipelines.
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Introduction to keras: a practical problem}
\begin{itemize}
\item The problem of automatic recognition of handwritten numbers, also known as digit recognition, is a classic problem in the field of pattern recognition and computer vision. It involves the task of automatically identifying and classifying handwritten digits into their corresponding numerical values.
\item The challenge of digit recognition arises due to the variability in handwriting styles and the different ways in which people write the same digit. This variation can be caused by factors such as different writing instruments, writing speed, and writing pressure. Additionally, digits can be written in different sizes, orientations, and positions, further adding to the complexity of the problem.
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Introduction to keras: a practical problem}

	\begin{itemize}
	\item The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9). 
	\item The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been around for almost as long as the field itself and has been very intensively studied. 
	\item It's a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. 
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Introduction to keras: a practical problem}
	\begin{itemize}
		\item As we have said, in the MNIST dataset each digit is stored in a grayscale image with a size of 28x28 pixels. 
		\item In the following you can see the first 10 digits from the training set:
	\end{itemize}
	\begin{center}
	\includegraphics[scale=.75]{../5-pictures/chapter-1-add_pic_0.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Introduction to keras: Loading the dataset}
\begin{itemize}
\item Keras provides seven different datasets, which can be loaded in using Keras directly. 
\item These include image datasets as well as a house price and a movie review datasets.
\item The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays
\end{itemize}
\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}
import keras

from keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}{Introduction to keras}
\textbf{The typical Keras workflows}
\vspace{0.5cm}
	\begin{itemize}
		\item Define your training data: input tensor and target tensor
		\item Define a network of layers(or model ) that maps input to our targets.
		\item Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor.
		\item Iterate your training data by calling the fit() method of your model.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Introduction to keras: Layers}
	\begin{itemize}
		\item This is the building block of neural networks which are stacked or combined together to form a neural network model.
		\item It is a data-preprocessing module that takes one or more input tensors and outputs one or more tensors. 
		\item These layers together contain the network's knowledge. 
		\item Different layers are made for different tensor formats and data processing.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Creating a model with the sequential API }
	\begin{itemize}
		\item The easiest way of creating a model in Keras is by using the sequential API, which lets you stack one layer after the other. 
		\item The problem with the sequential API is that it doesn't allow models to have multiple inputs or outputs, which are needed for some problems.
		\item Nevertheless, the sequential API is a perfect choice for most problems.
	\end{itemize}
\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

from keras import models
from keras import layers

network = models.Sequential()
network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))
network.add(layers.Dense(10, activation='softmax'))

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Introduction to keras}
Letâs go through this code line by line:
\begin{itemize}
\item The first line creates a Sequential model. 
\item This is the simplest kind of Keras model, for neural networks that are just composed of a single stack of layers, connected sequentially. 
\item This is called the \textbf{sequential} API.
\end{itemize}

\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

from keras import models
from keras import layers

network = models.Sequential()

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Introduction to keras}

Next, we build the first layer and add it to the model. 
\begin{itemize}
\item It is \textbf{\textit{Dense}} hidden layer with 512 neurons. 
\item It will use the ReLU activation function. 
\item Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. 
\item It also manages a vector of bias terms (one per neuron). 
\item When it receives some input data, it computes 

$$\phi \left( Z^{[1]} = W^{[1]} \cdot X + B^{[1]} \right), \quad \phi(z) = \textit{ReLU}(z)$$
\end{itemize}
\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))

\end{minted}
\rule{\textwidth}{1pt}

\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Introduction to keras}

Finally, we add a Dense output layer with 10 neurons (one per class). 
\begin{itemize}
\item Using a 10-way "softmax" layer means that it will return an array of 10 probability scores (summing to 1). 
\item Each score will be the probability that the current digit image belongs to one of our 10 digit classes.
\end{itemize}

\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

network.add(layers.Dense(10, activation='softmax'))

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Introduction to keras}

\begin{itemize}
\item The modelâs summary() method displays all the modelâs layers, including each layerâs name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. 
\end{itemize}
\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

network.summary()

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}{Introduction to keras}
	\begin{itemize}
\item The summary ends with the total number of parameters, including trainable and non-trainable parameters. 
\item Here we only have trainable parameters.	
	\end{itemize}
	\begin{center}
	\includegraphics[scale=.6]{../5-pictures/03_intro_to_deep_learning_pic_24.png} 
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Compile a Model}
	\begin{itemize}
		\item Before we can start training our model we need to configure the learning process. 
		\item For this, we need to specify an optimizer, a loss function and optionally some metrics like accuracy.
		\item The \textbf{loss function} is a measure on how good our model is at achieving the given objective.
		\item An \textbf{optimizer} is used to minimize the loss(objective) function by updating the weights using the gradients.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Loss Function}
	\begin{itemize}
		\item Choosing the right Loss Function for the problem is very important, the neural network can take any shortcut to minimize the loss. 
		\item So, if the objective doesn't fully correlate with success for the task at hand, your network will end up doing things you may not have wanted.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Loss Function}
For common problems like Classification, Regression and Sequence prediction, they are simple guidelines to choose a loss function. 

\vspace{0.5cm}
For: \begin{itemize}
\item Two- Class classification you can choose binary cross-entropy
\item Multi-Class Classification you can choose Categorical Cross-entropy.
\item Regression Problem you can choose Mean-Squared Error
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
\textbf{Softmax Activation Function} 
\vspace{0.5cm}
\begin{itemize}
\item Softmax is an activation function that scales numbers/logits into probabilities. 
\item The output of a Softmax is a vector (say $v$) with probabilities of each possible outcome. 
\item The probabilities in vector $v$ sums to one for all possible outcomes or classes 
\item It is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes
\item You can think of softmax function as a sort of generalization to multiple dimension of the logistic function
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
\textbf{Softmax Activation Function.} 
Mathematically, softmax is defined as
\begin{equation}
S(y)_i= \frac{\exp{y_i}}{\sum\limits_{j=1}^n \, \exp{y_j}}
\end{equation}
\begin{center}
\includegraphics[scale=.45]{../5-pictures/softmax_1.jpeg} 
\end{center}
\end{frame}
%..................................................................
\begin{frame}{Loss Function}
\textbf{Categorical Cross-Entropy} 
\begin{itemize}
\item Remember the logistic Loss Function:
$$
L(y, \hat y) = - y \, \log \hat y - (1-y) \, \log(1-\hat y) = -\sum\limits_{j=1}^2 \, y_j \log \hat y_j
$$
\item Generalizing this result to the n-class classification problem we have
\begin{equation}
L = -\sum\limits_{j=1}^{N_C} \, y_j \log \hat y_j
\end{equation}
This last equation define the so called \textit{cross-entropy}
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Loss Function}
\textbf{Categorical Cross-Entropy} 
\begin{itemize}
\item In the previous example, Softmax converts logits into probabilities. 
\item The purpose of the Cross-Entropy is to take the output probabilities ($P$) and measure the distance from the truth values

\end{itemize}
\begin{center}
\includegraphics[scale=.45]{../5-pictures/softmax_2.jpeg} 
\end{center}
\end{frame}
%..................................................................
\begin{frame}{Loss Function}
\textbf{Categorical Cross-Entropy} 
\begin{itemize}
\item The categorical cross-entropy is computed as follows
\end{itemize}
\begin{center}
\includegraphics[scale=.45]{../5-pictures/softmax_3.jpeg} 
\end{center}
\end{frame}
%..................................................................
\begin{frame}{Optimizer}
\textbf{RMSProp} 
\scriptsize
\begin{itemize}
\item Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the minimum of the function.

\item A limitation of gradient descent is that it uses the same step size (learning rate) for each input variable. 

\item AdaGrad is an extension of the gradient descent optimization algorithm that allows the step size in each dimension used by the optimization algorithm to be automatically adapted based on the gradients seen for the variable (partial derivatives) over the course of the search.

\item A limitation of AdaGrad is that it can result in a very small step size for each parameter by the end of the search that can slow the progress of the search down too much and may mean not locating the optima.

\item Root Mean Squared Propagation, or RMSProp, is an extension of gradient descent and the AdaGrad version of gradient descent that uses a decaying average of partial gradients in the adaptation of the step size for each parameter. 

\item The use of a decaying moving average allows the algorithm to forget early gradients and focus on the most recently observed partial gradients seen during the progress of the search, overcoming the limitation of AdaGrad. 
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Compile the model}
\scriptsize

So, to make our network ready for training, we need to pick three things, as part of "compilation" step:
\begin{itemize}
\item A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be 
able to steer itself in the right direction.
\item  An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.
\item Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly 
classified).
\end{itemize}

\rule{\textwidth}{1pt}
\begin{minted}{python}

network.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
                
\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Introduction to keras: Training}
\scriptsize

	\begin{itemize}
	\item 
Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in 
the `[0, 1]` interval. 
\item Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with 
values in the `[0, 255]` interval. 
\item We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1.
	\end{itemize}
\rule{\textwidth}{1pt}
\begin{minted}{python}

train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255

test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Introduction to keras: Training}

We also need to categorically encode the labels:

\rule{\textwidth}{1pt}
\scriptsize
\begin{minted}{python}

from keras.utils.np_utils import to_categorical

train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Introduction to keras: Training}
\textbf{What is an epoc?}
\footnotesize
	\begin{itemize}
	\item 
An epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. Datasets are usually grouped into batches (especially when the amount of data is very large). Some people use the term iteration loosely and refer to putting one batch through the model as an iteration.   
\item
If the batch size is the whole training dataset then the number of epochs is the number of iterations. For practical reasons, this is usually not the case. Many models are created with more than one epoch. The general relation where dataset size is $d$, number of epochs is $e$, number of iterations is $i$, and batch size is $b$ would be $d \cdot e = i \cdot b$. 
\item 
Determining how many epochs a model should run to train is based on many parameters related to both the data itself and the goal of the model, and while there have been efforts to turn this process into an algorithm, often a deep understanding of the data itself is indispensable.

	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Introduction to keras: Training}
We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network: we "fit" the model to its training data.
\vspace{0.5cm}
\scriptsize
\rule{\textwidth}{1pt}
\begin{minted}{python}

history = network.fit(train_images, train_labels, epochs=5, batch_size=128)

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Visualizing the training process }
\scriptsize
	\begin{itemize}
		\item We can visualize our training and testing accuracy and loss for each epoch so we can get intuition about the performance of our model. 
		\item The accuracy and loss over epochs are saved in the history variable we got whilst training and we will use Matplotlib to visualize this data.
	\end{itemize}

\rule{\textwidth}{1pt}
\begin{minted}{python}

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('model mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%=====================================================================
\section{Other Neural Network Architectures}
%=====================================================================
\begin{frame}{Other Neural Network Architectures}
\begin{itemize}
\item Up to this point, we've described a specific neural network architecture (feed forward NN) where values flow forward linearly through a network, and gradients flow linearly backwards through a network. 

\item However, this is just the tip of the iceberg when it comes to the field of neural networks. 

\item While feed forward neural networks have been incredibly successful in a wide range of applications, many other types of neural network architectures exist that can be used to solve different types of problems. 

\item In particular in this lesson we are going to describe Convolutional Neural Network and Recurrent Neural Network
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Convolutional Neural Network}
\begin{itemize}
\item Convolutional Neural Networks (CNNs) are specifically designed for processing spatial data, such as images. 

\item Unlike feed-forward networks, CNNs use special convolutional layers to scan and identify local patterns within the input. 

\item Imagine a grid sliding across an image, identifying patterns. 

\item This makes them more efficient for image recognition, object detection, and other computer vision tasks, where spatial information is crucial. 

\item They are also used for sequential tasks, such as time-series applications.
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Recurrent Neural Network}
\begin{itemize}
\item Recurrent Neural Networks (RNNs) differ from feed-forward neural networks as they have a built-in memory, allowing them to process sequences of data. 

\item This makes RNNs well-suited for tasks like natural language processing and time series prediction. 

\item They can learn patterns in sequences by connecting the output from one time step to the input of the next, remembering previous information (the recurrence in the namesake).
\end{itemize}
\end{frame}
%______________________________________________________________________________
%
\subsection{Convolutional Neural Network \\ \scalebox{0.8}{}}
%______________________________________________________________________________
%
\begin{frame}{Convolutional Neural Network}
\begin{itemize}
\item A convolutional neural network, or CNN for short, is a type of artificial neural network that is commonly used for image recognition and classification tasks. 
\item It is inspired by the organization of the visual cortex in animals and is designed to process visual data in a way that is similar to how the human brain processes visual information.

\item The main idea behind a CNN is to use a series of \textbf{convolutional layers} to automatically learn features from the input image, such as edges, lines, and shapes. 
\item These features are then combined and fed into a series of fully connected layers that perform the final classification task.
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Convolutional Layer}
\footnotesize{
\begin{itemize}
\item A \textbf{convolutional layer} is a key building block in convolutional neural networks (CNNs) used for image recognition and computer vision tasks. 
\item It performs a mathematical operation called convolution on the input image to extract features that are useful for the given task.
\end{itemize}}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/chapter Sequential NN_pic_9.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{What is Convolution}
\footnotesize{
\begin{itemize}
\item The convolution operation involves sliding a small matrix of numbers called a \textbf{kernel} or \textbf{filter} over the input image. 
\item The kernel is typically much smaller than the input image and contains learnable weights that are optimized during the training process. 
\end{itemize}}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/chapter Sequential NN_pic_9.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{What is a Convolution}
\footnotesize{
Convolutional layers typically have multiple kernels that are applied to the input image to extract different features. These features may include edges, textures, and patterns that are relevant to the given image recognition task.}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/chapter Sequential NN_pic_10.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{What is Convolution}
\footnotesize{
\begin{itemize}
\item An easy way to understand this is if you were a detective and you are came across a large image or a picture in dark, how will you identify the image?

\item You will use you flashlight and scan across the entire image. This is exactly what we do in convolutional layer.

\item Kernel K, which is a feature detector is equivalent of the flashlight on image I, and we are trying to detect feature to help us identify or classify the image.
\end{itemize}}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/chapter Sequential NN_pic_9.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{What is Convolution}
\footnotesize{
\begin{itemize}
\item As the kernel slides over the input image, it computes a dot product between its weights and the pixel values of the corresponding region of the image. 
\item The output of this dot product is a single number that is used to create a new output image or feature map.
\item Just like the weights in a fully connected layer, the kernel weights are learned during training, and adjusted after each training iteration through backpropagation.
\end{itemize}}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/chapter Sequential NN_pic_9.png}
	\end{center}
\end{frame}
%..................................................................
%\begin{frame}
%        \transduration<0-15>{1}
%        \multiinclude[<+->][format=png, graphics={width=\textwidth}]{../5-pictures/cnn_filter_1/cnn_filter_1}
%    \end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-0.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-1.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-2.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-3.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-4.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-5.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-6.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-7.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-8.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-9.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-10.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-11.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-12.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-13.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-14.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_1/cnn_filter_1-15.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_2/cnn_filter_2-1.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_2/cnn_filter_2-2.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_2/cnn_filter_2-3.png}
	\end{center}
\end{frame}
\begin{frame}
	\begin{center}
	\includegraphics[width=\textwidth]{../5-pictures/cnn_filter_2/cnn_filter_2-4.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Producing Features Map}
\footnotesize{
Convolutional layers typically have multiple kernels that are applied to the input image to extract different features. These features may include edges, textures, and patterns that are relevant to the given image recognition task.}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/chapter Sequential NN_pic_10.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Producing Features Map}
Let's take an image \textbf{x}, which is a 2D array of pixels with different color channels(Red,Green and Blue-RGB). If we have a  \textbf{ feature detector or kernel w} then the output we get after applying the previous mathematical operation is called a  \textbf{ feature map}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/chapter Sequential NN_pic_8.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Producing Features Map}
Finally, though the weights of the filters are the main parameters that are trained, there are also hyper-parameters that can be tuned for CNNs:
	\begin{itemize}
		\item number of filters in a layer
		\item dimensions of filters
		\item stride (number of pixels a filter moves each step)
		\item padding (how the filter handles boundaries of images)
	\end{itemize}
We won't get into the details of these hyperparameters, since this isn't intended to be a comprehensive CNN walk-through, but these are important factors to be aware of.
\end{frame}
%..................................................................
\begin{frame}{CCN Architecture}
\begin{itemize}
\item The convolutional layer is only the beginning...
\item In comparison to feed-forward networks, like the one we developed in the previous part of these lessons, CNN have different architecture, and \textbf{are composed of different types of layers}. 

\item In the figure below, we can see the general architecture of a typical CNN, including the different types of layers it can contain. 
\end{itemize}
	
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/chapter Sequential NN_pic_11.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{CCN Architecture}
The three types of layers usually present in a Convolutional Network are:
	\begin{itemize}
		\item Convolutional Layers (red dashed outline)
		\item Pooling Layers (blue dashed outline)
		\item Fully Connected Layers (Red and Purple solid outlines)
	\end{itemize}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/chapter Sequential NN_pic_12.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Pooling}

	\begin{itemize}
		\item Pooling layers are similar to convolutional layers, in that a filter convolves over the input data (usually a feature map that was output from a convolutional layer). 
		\item However, rather than feature detection, the function of pooling layers is dimensionality reduction or downsampling. 
	\end{itemize}
	\begin{center}
	\includegraphics[scale=0.35]{../5-pictures/pooling_1.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Pooling}
\footnotesize{
	\begin{itemize}
		\item The two most common types of pooling used are Max Pooling and Average Pooling. 
		\item With Max Pooling, the filter slides across the input, and at each step will select the pixel with the largest value as the output. 
		\item In Average Pooling, the filter will output the average value of the pixels that the filter is passing over.
	\end{itemize}}
	\begin{center}
	\includegraphics[scale=0.25]{../5-pictures/pooling_2.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{CNN Output}
	\begin{itemize}
		\item The last layer of a CNN is the classification layer which determines the predicted value based on the activation map. 
		\item If you pass a handwriting sample to a CNN, the classification layer will tell you what letter is in the image. 
		\item This is what autonomous vehicles use to determine whether an object is another car, a person, or some other obstacle.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Training}
	\begin{itemize}
		\item Training a CNN is similar to training many other machine learning algorithms. 
		\item You'll start with some training data that is separate from your test data and you'll tune your weights based on the accuracy of the predicted values. 
		\item Just be careful that you don't overfit your model.
	\end{itemize}
\end{frame}
%______________________________________________________________________________
%
\subsection{Sequential Data \\ \scalebox{0.8}{}}
%______________________________________________________________________________
%
\begin{frame}{What are Sequential Data}
	\begin{itemize}
		\item Sequential data refers to data that has a natural ordering to it, meaning that \textbf{each data point is dependent on the data that came before it}. In other words, \textbf{the sequence of the data matters}.
		\item Examples of sequential data include time series data such as stock prices, weather data, or audio signals, where each point in time is dependent on the previous point. Other examples include text data such as sentences or paragraphs, where the meaning of each word is dependent on the context of the words that came before it.
		\item Sequential data can be modeled and analyzed using various techniques, including Hidden Markov Models (HMMs), and autoregressive models such as ARIMA.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{What are Sequential Data}
	\begin{itemize}
		\item In the context of machine learning, sequential data presents unique challenges because the order of the data is important and traditional machine learning models such as decision trees and linear regression do not take this into account. 
		\item Models such as RNNs (Recurrent Neural Network) are specifically designed to handle sequential data by processing the data one element at a time and retaining information about the previous elements to make predictions about future elements in the sequence.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{What are Sequential Data}
	\begin{center}
	\includegraphics[scale=0.55]{../5-pictures/chapter Sequential NN_pic_0.png}
	\end{center}
\end{frame}
%______________________________________________________________________________
%
\subsection{Recurrent Neural Networks \\ \scalebox{0.8}{}}
%______________________________________________________________________________
%
\begin{frame}{What are Recurrent Neural Networks}
	\begin{itemize}
		\item A major characteristic of all neural networks you have seen so far, such as densely connected networks and convnets, is that they have no memory. 
		\item Each input shown to them is processed independently, with no state kept in between inputs.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{What are Recurrent Neural Network}
	\begin{center}
	\includegraphics[scale=0.35]{../5-pictures/chapter Sequential NN_pic_1.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{What are Recurrent Neural Network}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item In the picture, there are some distinct components, from which the most important are:
		\item \textbf{x}: The input. It can be a word in a sentence or some other type of sequential data
		\item \textbf{O}: The output. For instance, what the network thinks the next word on a sentence should be given the previous words
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../5-pictures/chapter Sequential NN_pic_2.png}
    %}
\end{column}%
\end{columns}
\end{frame}
\begin{frame}{What are Recurrent Neural Network}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item In the picture, there are some distinct components, from which the most important are:
		\item \textbf{h}: The main block of the RNN. It contains the weights and the activation functions of the network
		\item \textbf{V}: Represents the communication from one time-step to the other.
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../5-pictures/chapter Sequential NN_pic_3.png}
    %}
\end{column}%
\end{columns}
\end{frame}
\begin{frame}{What are Recurrent Neural Networks}
	\begin{center}
	\includegraphics[scale=0.65]{../5-pictures/chapter Sequential NN_pic_4.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{What are Recurrent Neural Networks}
	\begin{itemize}
		\item A recurrent neural network (RNN) processes sequences by iterating through the sequence elements and maintaining a state containing information relative to what it has seen so far. 
		\item In effect, an RNN is a type of neural network that has an internal loop: \textbf{the network internally loops over sequence elements}.
	\end{itemize}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/chapter Sequential NN_pic_5.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{What are Recurrent Neural Networks}
	\begin{itemize}
		\item The transformation of the input and state into an output will be parameterized by two matrices, $W$ and $U$, and a bias vector. 
		\item It is similar to the transformation operated by a densely connected layer in a feedforward network.
	\end{itemize}
	\begin{center}
	\includegraphics[scale=0.4]{../5-pictures/chapter Sequential NN_pic_6.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{What are Recurrent Neural Networks}
	\begin{itemize}
		\item In this very simple example, the final output is a $2D$ tensor of shape $(timesteps, output\_features)$, where each timestep is the output of the loop at time $t$.
		\item Each timestep $t$ in the output tensor contains information about timesteps $0$ to $t$ in the input sequence about the entire past. 
		\item For this reason, in many cases, you don`t need this full sequence of outputs; you just need the last output ($output\_t$ at the end of the loop), because it already contains information about the entire sequence.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Recurrent layer in Keras}
The SimpleRNN layer:
\newline
\newline
\rule{\textwidth}{1pt}
\begin{minted}{python}
from keras.layers import SimpleRNN
\end{minted}
\rule{\textwidth}{1pt}
\newline
\newline
SimpleRNN processes batches of sequences, like all other
Keras layers. This means it takes inputs
of shape $(batch\_size, timesteps, input\_features)$, rather than $(timesteps,
Input\_features)$.
\end{frame}
%______________________________________________________________________________
%
\subsection{Long short-term memory NN \\ \scalebox{0.8}{}}
%______________________________________________________________________________
%
\begin{frame}{Problems with RNN}
	\begin{itemize}
		\item SimpleRNN is not the only recurrent layer available in Keras. 
		\item There are two others: \textbf{LSTM}and \textbf{GRU}. 
		\item In practice, you will always use one of these, because SimpleRNN is generally too simplistic to be of real use. 
		\item SimpleRNN has a major issue: although it should theoretically be able to retain at time $t$ information about inputs seen many timesteps before, \textbf{in practice, such long-term dependencies are impossible to learn}. 
		\item This is due to the \textbf{\textit{vanishing gradient problem}}, an effect that is similar to what is observed with non-recurrent networks (feedforward networks) that are many layers deep: as you keep adding layers to a network, the network eventually becomes untrainable. 
		\item The theoretical reasons for this effect were studied by Hochreiter, Schmidhuber, and Bengio in the early 1990s.
		\item The LSTM and GRU layers are designed to solve this problem.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{LSTM layer in Keras}
	\begin{itemize}
		\item Let`s consider the LSTM layer. 
		\item This layer is a variant of the SimpleRNN layer you already know about; it adds a way to carry information across many timesteps.
	\end{itemize}
	\begin{center}
	\includegraphics[scale=0.5]{../5-pictures/chapter Sequential NN_pic_7.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{LSTM layer in Keras}
Now the subtlety: the way the next value of the carry dataflow is computed. It involves three distinct transformations. All three have the form of a SimpleRNN cell:
\newline
\newline
\rule{\textwidth}{1pt}
\begin{minted}{python}
y = activation(dot(state_t, U) + dot(input_t, W) + b)
\end{minted}
\rule{\textwidth}{1pt}
\newline
\newline
\end{frame}
%..................................................................
\begin{frame}[fragile]
\frametitle{Pseudocode details of the LSTM architecture}
\rule{\textwidth}{1pt}
\begin{minted}{python}
output_t = activation(dot(state_t, Uo) + \\
                      dot(input_t, Wo) + \\
                      dot(C_t, Vo) + bo)
i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)
f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)
k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)
#
# You obtain the new carry state (the next c_t) by combining 
# i_t, f_t, and k_t
#
c_t+1 = i_t * k_t + c_t * f_t
\end{minted}
\rule{\textwidth}{1pt}
\end{frame}
%______________________________________________________________________________
%
\section{Credits and References \\ \scalebox{0.8}{}}
%______________________________________________________________________________
%
\begin{frame}{Credits and References}
	\begin{itemize}
		\item Renu Khandelwal, Convolutional Neural Network(CNN) Simplified 
		\item https://medium.datadriveninvestor.com/convolutional-neural-network-cnn-simplified-ecafd4ee52c5
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Credits and References}
	\begin{itemize}
		\item Nafiu, Stock market prediction using LSTM; will the price go up tomorrow. Practical guide (https://medium.com/@nafiu.dev/stock-market-prediction-using-lstm-will-the-price-go-up-tomorrow-practical-guide-d1df2d54a517)
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Exercises}
	https://sh-tsang.medium.com/tutorial-a-good-toy-dataset-for-lstm-model-89e99063610c
\end{frame}


\end{document}

%..................................................................
\begin{frame}{Delta Rule}
\begin{itemize}
\item In order to get the derivative of our targets, chain rules would be applied:
\begin{align*}
&\frac{\partial L}{\partial W} =  \frac{\partial L}{\partial \hat y} \frac{\partial \hat y}{\partial Z} \frac{\partial Z}{\partial W} \\
&\frac{\partial L}{\partial b} =  \frac{\partial L}{\partial \hat y} \frac{\partial \hat y}{\partial Z} \frac{\partial Z}{\partial b} 
\end{align*}
\item Let's focus only on the calculation of the derivative with respect to $W$ since the calculation of the other derivative (with respect to $b$) is completely equivalent...
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
\begin{center}
\includegraphics[scale=.5]{../5-pictures/03_intro_to_deep_learning_pic_20.png} 
\end{center}
The derivative of the Loss Function with respect to $\hat y$ is very easy and can be calculated once for all because it does not depend on the particular layer:
\begin{align*}
&L(y, \hat{y}) = -[y\log{\hat{y}} + (1 - y)\log{(1 - \hat{y})}] \\
&\frac{\partial L}{\partial \hat y} = -\frac{y}{\hat y} + \frac{1-y}{1-\hat y} = \frac{\hat y - y}{\hat y(1 - \hat y)} \Rightarrow \\
&\frac{\partial L}{\partial A^{[2]}} = \frac{A^{[2]} - y}{A^{[2]}(1 - A^{[2]})}
\end{align*}
\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation}
\begin{center}
\includegraphics[scale=.5]{../5-pictures/03_intro_to_deep_learning_pic_20_b.png} 
\end{center}
\textbf{Hidden Layer Activation Function (Hyperbolic Tangent)}
\begin{equation}
\tanh x = \frac{{{e^x} -{e^{- x}}}}{{{e^x} + {e^{ -x}}}} 
\Rightarrow 
\frac{d}{{dx}}\tanh x =  1-{\left(\tanh x \right)}^2 
\end{equation}
\textbf{Output Layer Activation Function (Sigmoid Function)} 
\begin{equation}
\sigma(x) =  \left[ \dfrac{1}{1 + e^{-x}} \right]  \Rightarrow
\dfrac{d}{dx} \sigma(x) =  \sigma(x) \cdot (1 - \sigma(x))
\end{equation}
\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation: $W^{[2]}$ Update}
\begin{align*}
& \frac{\partial L}{\partial A^{[2]}} = \frac{A^{[2]} - y}{A^{[2]}(1 - A^{[2]})} \notag\\
& \frac{\partial A^{[2]}}{\partial Z^{[2]}} = \frac{\partial \sigma(Z^{[2]})}{\partial Z^{[2]}} = \sigma(Z^{[2]}) \cdot (1 - \sigma(Z^{[2]})) 
= A^{[2]} (1 - A^{[2]}) \\
& \frac{\partial Z^{[2]}}{\partial W^{[2]}} = A^{[1]} 
\end{align*}
So the complete gradient is:

\begin{tcolorbox}
\begin{align*}
\frac{\partial L}{\partial W^{[2]}} &=  
\frac{A^{[2]} - y}{A^{[2]}(1 - A^{[2]})} \cdot 
A^{[2]} (1 - A^{[2]}) \cdot {A^{[1]}}^T \\
&= (A^{[2]} - y) \cdot {A^{[1]}}^T
\end{align*}
\end{tcolorbox}

\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation: $W^{[1]}$ Update}

\begin{align*}
& \frac{\partial L}{\partial A^{[2]}} = \frac{A^{[2]} - y}{A^{[2]}(1 - A^{[2]})} \notag\\
& \frac{\partial A^{[2]}}{\partial Z^{[2]}} = \frac{\partial \sigma(Z^{[2]})}{\partial Z^{[2]}} = \sigma(Z^{[2]}) \cdot (1 - \sigma(Z^{[2]})) 
= A^{[2]} (1 - A^{[2]}) \\
& \frac{\partial Z^{[2]}}{\partial W^{[1]}} = \mathbf{?} 
\end{align*}
So the complete gradient is:
\begin{align*}
\frac{\partial L}{\partial W^{[1]}} &=  
\frac{A^{[2]} - y}{A^{[2]}(1 - A^{[2]})} \cdot 
A^{[2]} (1 - A^{[2]}) \cdot \frac{\partial Z^{[2]}}{\partial W^{[1]}} \\
&= (A^{[2]} - y) \cdot \frac{\partial Z^{[2]}}{\partial W^{[1]}}
\end{align*}
\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation: $W^{[1]}$ Update}

Now we have to calculate
$$\frac{\partial Z^{[2]}}{\partial W^{[1]}}$$
Remember that
$$Z^{[2]} = W^{[2]} \cdot tanh\left( W^{[1]} \cdot X + b^{[1]} \right) + b^{[2]}$$
and
$$\frac{\partial Z^{[2]}}{\partial W^{[1]}} = W^{[2]} \cdot \frac{\partial \, tanh(\dots)}{\partial W^{[1]}} \cdot X = W^{[2]} \cdot \left( 1 - tanh^2(\dots) \right) \cdot X$$
\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation: $W^{[1]}$ Update}

Finally
\begin{tcolorbox}
\begin{align*}
\frac{\partial L}{\partial W^{[1]}} &= (A^{[2]} - y) \cdot W^{[2]} \cdot X \cdot \left( 1 - tanh^2(\dots) \right) \\
&= (A^{[2]} - y) \cdot W^{[2]} \cdot X \cdot \left( 1 - {A^{[1]}}^2 \right)
\end{align*}
\end{tcolorbox}
\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation: $W^{[2]}$ Update}

Since

\begin{equation}
\frac{\partial L}{\partial W^{[2]}} = (A^{[2]} - y) \cdot {A^{[1]}}^T
\end{equation}

We have

\begin{align}
& \Delta W^{[2]} = \frac{1}{m}\left[A^{[2]} - Y \right]A^{[1]^T} = \frac{1}{m}\Delta^{[2]}A^{[1]^T} \\
& \Delta b^{[2]} = \frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True) 
\end{align}

Where
\begin{equation} 
\Delta^{[2]} = A^{[2]} - Y  
\end{equation}
\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation: $W^{[1]}$ Update}

\begin{align}
\Delta W^{[1]} &= \frac{1}{m} \left[ A^{[2]} - Y \right] \cdot  X^{T} \cdot W^{[2]T} \cdot (1 - A^{[1]^2}) \notag\\
         &= \frac{1}{m} \Delta^{[2]} \cdot W^{[2]T} \cdot (1 - A^{[1]^2}) \cdot  X^{T}   \notag\\
         &= \frac{1}{m} \Delta^{[1]} \cdot  X^{T} 
\end{align}
\begin{equation} 
\Delta b^{[1]} = \frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)  
\end{equation}
Where
$$\Delta^{[1]} = \Delta^{[2]} \cdot W^{[2]T} \cdot (1 - A^{[1]^2})$$
\end{frame}
