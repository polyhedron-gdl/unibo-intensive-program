{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f009dd",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/unibo-intensive-program-2024/blob/main/1-notebooks/chapter-07-01.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f90012",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc5622",
   "metadata": {},
   "source": [
    "## What is a Neural Network\n",
    "\n",
    "Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. \n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20afaefd",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-07-01-00.jpg\" width=\"500\" height=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3aca2",
   "metadata": {},
   "source": [
    "![ch-01-01-20.jpg](./pics/ch-07-01-00.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc14f47",
   "metadata": {},
   "source": [
    "Neural networks rely on **training data** to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the most well-known neural networks is Google’s search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef72727",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "A Neural Network transforms its input data into meaningful outputs, a process that is **learned** from exposure to known examples of inputs and outputs. Therefore, the central problem in deep learning is to **meaningfully transform data**: in other words, **to learn useful representations of the input data at hand, representations that get us closer to the expected output**. \n",
    "\n",
    "What is a representation? At its core, it is simply a different way to look at data, to represent or encode data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be1774d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20   \n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0  \\\n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv('./data/mnist_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a941930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7178\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = np.array(data.iloc[:,1:]) \n",
    "random_index =7178 #np.random.randint(0,40000)\n",
    "img = x_train[random_index].reshape(28,28)\n",
    "plt.imshow(img, cmap = \"gray\")\n",
    "print(random_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b59c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./data/number.csv',img, delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c38bf0c",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-07-01-04.jpg\" width=\"500\" height=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac12eb",
   "metadata": {},
   "source": [
    "![ch-01-01-20.jpg](./pics/ch-07-01-04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf5bb95",
   "metadata": {},
   "source": [
    "## Mc Culloch and Pitts Artificial Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71432a50",
   "metadata": {},
   "source": [
    "The McCulloch and Pitts neuron is one of the oldest neural network. It has a single neuron and is the simplest form of a neural network. So it is very important to learn how it works because it is the most fundamental unit of a deep neural networks. \n",
    "\n",
    "The artificial neuron receives one or more inputs and sums them to produce an output. Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c485553c",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-07-01-01.jpg\" width=\"500\" height=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ac39c",
   "metadata": {},
   "source": [
    "![ch-01-01-20.jpg](./pics/ch-07-01-01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482272f3",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb92e7d",
   "metadata": {},
   "source": [
    "Feedforward neural networks, or multi-layer perceptrons (MLPs), are what we’ve primarily been focusing on within this notebook. They are comprised of an input layer, a hidden layer or layers, and an output layer. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks. \n",
    "\n",
    "The  simplest kind of feedforward neural network is a single-layer network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node, and if the value is above some threshold the neuron fires and takes the activated value; otherwise it takes the deactivated value. \n",
    "\n",
    "In the following we are going to implement a very simple neural network from scratch without any library. In my opinion this is very usefull because most people consider neural networks as a black-box and use libraries like Keras, TensorFlow and PyTorch which provide, among other things, automatic differentiation without a real understanding of how a neural network really works. Though it is not necessary to write your own code on how to compute gradients and backprop errors, having knowledge on it helps you in understanding a few concepts which can help you a lot in understanding how a neural networks works.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e2819b",
   "metadata": {},
   "source": [
    "### One Hidden Layer NN\n",
    "\n",
    "We will build a shallow dense neural network with one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b5fe7",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-07-01-02.jpg\" width=\"300\" height=\"300\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7157566",
   "metadata": {},
   "source": [
    "![ch-01-01-20.jpg](./pics/ch-07-01-02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f71ae3",
   "metadata": {},
   "source": [
    "Where in the graph above, we have a input vector $x = (x_1, x_2)$, containing 2 features and 4 hidden nodes $a_1, a_2, a_3$ and $a_4$, and only one value in output $y_1 \\in [0, 1]$ (consider this a binary classification task with a prediction of probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f772c",
   "metadata": {},
   "source": [
    "In each hidden unit, take $a_1$ as example, a linear operation followed by an activation function, $f$, is performed. So given input $x = (x_1, x_2)$, inside node $a_1$, we have:\n",
    "\n",
    "$$z_1 = w_{11}x_1 + w_{12}x_2 + b_1$$\n",
    "\n",
    "$$a_1 = f(w_{11}x_1 + w_{12}x_2 + b_1) = f(z_1) $$\n",
    "\n",
    "Here $w_{11}$ denotes weight 1 of node 1, $w_{12}$ denotes weight 2 of node 1. Same for node $a_2$, it would have:\n",
    "\n",
    "$$z_2 = w_{21}x_1 + w_{22}x_2 + b_2$$\n",
    "\n",
    "$$a_2  = f(w_{21}x_1 + w_{22}x_2 + b_2) = f(z_2)$$\n",
    "\n",
    "And same for $a_3$ and $a_4$ and so on ...\n",
    "\n",
    "We can also write in a more compact form\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "z_1 \\\\ z_2 \\\\ z_3 \\\\ z_4\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "w_{11} & w_{12} \\\\ w_{21} & w_{22} \\\\ w_{31} & w_{32} \\\\ w_{41} & w_{42}\n",
    "\\end{pmatrix} \n",
    "\\cdot \n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\ x_2 \n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4\n",
    "\\end{pmatrix} \n",
    "\\Rightarrow Z^{[1]} = W^{[1]} \\cdot X + B^{[1]} \n",
    "\\end{equation}\n",
    "\n",
    "Note that superscript $[i]$ denotes the $ith$ layer. Let's assume that the first activation function is the $\\tanh$ and the output activation function is the $sigmoid$. So the result of the hidden layer is:\n",
    "\n",
    "$$ A^{[1]} = \\tanh{Z^{[1]}} $$\n",
    "\n",
    "This result is applied to the output node which will perform another linear operation with a different set of weights, $W^{[2]}$:\n",
    "\n",
    "$$ Z^{[2]} = W^{[2]} \\cdot A^{[1]} + B^{[2]} $$\n",
    "\n",
    "and the final output will be the result of the application of the output node activation function (the sigmoid) to this value:\n",
    "\n",
    "$$ \\hat{y} = \\sigma({Z^{[2]}})$$\n",
    "\n",
    "For the dimension of each matrix, we have:\n",
    "\n",
    "- $ W^{[1]}$ in the case above would have dimension $4 \\times 2$, with each $ith$ row is the weight of node $i$\n",
    "- $B^{[1]}$ has dimension $4 \\times 1$\n",
    "- $Z^{[1]}$ and $A^{[1]}$ both have dimention $4 \\times 1$\n",
    "- $W^{[2]}$ has dimension $1 \\times 4$\n",
    "- consequently, $Z^{[2]}$ and $A^{[2]}$ would have dimensition $1 \\times 1$, which is a single value\n",
    "\n",
    "Function $\\tanh$ and $sigmoid$ looks as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b38605f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49ef82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a8f7055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tanh')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10, 4])\n",
    "x = np.linspace(-10, 10)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title('sigmoid')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, tanh(x))\n",
    "plt.title('tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e038d6",
   "metadata": {},
   "source": [
    "Notice that the only difference of these functions is the scale of y\n",
    "\n",
    "For the derivatives of the activation functions the following rules apply (see notebook chapter-07-01 appendix for details) :\n",
    "\n",
    "**Derivative of Hyperbolic Tangent**\n",
    "\n",
    "\\begin{equation}\n",
    "\\tanh x = \\frac{{{e^x} – {e^{ – x}}}}{{{e^x} + {e^{ – x}}}} \n",
    "\\Rightarrow \n",
    "\\frac{d}{{dx}}\\tanh x =  1 - {\\left(\\tanh x \\right)}^2 \n",
    "\\end{equation}\n",
    "\n",
    "**Derivative of Sigmoid Function** \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x) =  \\left[ \\dfrac{1}{1 + e^{-x}} \\right]  \\Rightarrow\n",
    "\\dfrac{d}{dx} \\sigma(x) =  \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ead11",
   "metadata": {},
   "source": [
    "### The Loss Function\n",
    " \n",
    "Remember that Linear regression uses Least Squared Error as loss function that gives a convex graph and then we can complete the optimization by finding its vertex as global minimum. However, it’s not an option for logistic regression anymore. Since the hypothesis is changed, Least Squared Error will result in a non-convex graph with local minimums by calculating with sigmoid function applied on raw model output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c45ee",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-07-01-03.jpg\" >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcbb628",
   "metadata": {},
   "source": [
    "Furthermore using logistic regression, means that we are focusing on binary classification, we have class 0 and class 1. To compare with the target, we want to constrain predictions to some values between 0 and 1. That’s why Sigmoid Function is applied on the raw model output and provides the ability to predict with probability. So will follow a different path. \n",
    "\n",
    "Intuitively, we want to assign more punishment when predicting 1 while the actual is 0 and when predict 0 while the actual is 1. The loss function of logistic regression is doing this exactly which is called Logistic Loss. If y = 1,  when prediction = 1, the cost must be = 0, otherwise, when prediction = 0, the learning algorithm is punished by a very large cost. Similarly, if y = 0, predicting 0 has no punishment but predicting 1 has a large value of cost. In formula we have\n",
    "\n",
    "\\begin{equation}\n",
    "L(y, \\hat{y}) = \n",
    "\\begin{cases} \n",
    "-\\log{\\hat{y}} & \\text{when}\\, y = 1 \\\\ -\\log(1 - \\hat{y}) & \\text{when}\\, y = 0 \n",
    "\\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "Another advantage of this loss function is that although we are looking at it by y = 1 and y = 0 separately, it can be written as one single formula which brings convenience for calculation:\n",
    "\n",
    "$$ L(y, \\hat{y}) = -[y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc565da0",
   "metadata": {},
   "source": [
    "### Formula of Batch Training\n",
    " \n",
    "The above shows the formula of a single input vector, however in actual training processes, a batch is trained instead of 1 at a time. The change applied in the formula is trivial, we just need to replace the single vector $x$ with a matrix $X$ with size $n \\times m$, where $n$ is number of features and $m$ is the the batch size, samples are stacked column wise, and the following result matrix are applied likewise. We have the same formulas as before ... \n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "& Z^{[1]} = W^{[1]}X + b^{[1]}  \\\\\n",
    "& A^{[1]} = \\tanh{Z^{[1]}}  \n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "& Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\\\\n",
    "& A^{[2]} =  \\sigma({Z^{[2]}}) = \\hat{y}   \n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "... but for the dimension of each matrix taken in this example now we have:\n",
    "\n",
    "- $X$ has dimension $2 \\times m$, as here there are 2 features and $m$ is the batch size\n",
    "- $W^{[1]}$ in the case above would have dimension $4 \\times 2$, with each $ith$ row is the weight of node $i$\n",
    "- $b^{[1]}$ has dimension $4 \\times 1$\n",
    "- $Z^{[1]}$ and $A^{[1]}$ both have dimension $4 \\times m$\n",
    "- $W^{[2]}$ has dimension $1 \\times 4$\n",
    "- consequently, $Z^{[2]}$ and $A^{[2]}$ would have dimension $1 \\times m$\n",
    "\n",
    "The loss function is the same as logistic regression, but for batch training, we'll take the average loss for all training samples.\n",
    "\n",
    "$$ J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m} \\sum_{i}^{m}L(y^{(i)}, \\hat{y}^{(i)})  $$\n",
    "\n",
    "\n",
    "This is all for the forward propagation. To activate our neurons to learn, we need to get derivative of weight parameters and update them use gradient descent.\n",
    "\n",
    "But now it is enough for us to implement the forward propagation first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ac3ff",
   "metadata": {},
   "source": [
    "### Generate Sample Dataset\n",
    " \n",
    "Scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. Here we generate a simple **binary classification task** with 5000 data points and `n_features` features for later model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5009dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 20\n",
    "n_hidden   = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2902f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (4000, 20)\n",
      "test shape (1000, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "# Define the parameters for generating synthetic data using the make_classification function. \n",
    "# The function is invoked with specific arguments:\n",
    "# - n_samples    = 5000 : It generates a dataset with 5000 samples.\n",
    "# - random_state = 123  : It sets the random seed to ensure reproducibility.\n",
    "# - X is a NumPy array containing the feature data.\n",
    "# - y is a NumPy array containing the corresponding target labels.\n",
    "X, y = datasets.make_classification(n_samples=5000, n_features=n_features, n_informative=n_features, n_redundant=0, random_state=123)\n",
    "# Split the dataset into training and testing subsets: X_train and y_train contain 4000 samples, \n",
    "# which are used for training the machine learning model. X_test and y_test contain 1000 samples, \n",
    "# which are used for testing the model's \n",
    "X_train, X_test = X[:4000], X[4000:]\n",
    "y_train, y_test = y[:4000], y[4000:]\n",
    "\n",
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a6c914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.28726786  4.48392645 -0.19663023  0.52875293 -1.44603544  3.33013291\n",
      " -0.49828395 -1.38991547  3.86964059  0.0259337   1.41931003 -0.26677859\n",
      "  3.21847861 -0.67232203 -2.7307319  -0.85791875  0.0631985   2.39609383\n",
      " -0.39872069 -3.49423158]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b6ef9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac4643",
   "metadata": {},
   "source": [
    "### Weights Initialization\n",
    " \n",
    "Our neural network has 1 hidden layer and 2 layers in total(hidden layer + output layer), so there are 4 weight matrices to initialize ($W^{[1]}, b^{[1]}$ and $W^{[2]}, b^{[2]}$). Notice that the weights are initialized relatively small so that the gradients would be higher thus learning faster in the beginning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69b629ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code defines a Python function init_weights that initializes the weights and biases for a neural \n",
    "network with a specified architecture. The function takes three arguments:\n",
    "- n_input: An integer representing the number of input features.\n",
    "- n_hidden: An integer representing the number of neurons (units) in the hidden layer.\n",
    "- n_output: An integer representing the number of neurons in the output layer.\n",
    "\n",
    "The function returns a dictionary params containing the initialized weights and biases for the neural network.\n",
    "'''\n",
    "def init_weights(n_input, n_hidden, n_output):\n",
    "    # Initializes an empty dictionary called params where we will store the weight and bias parameter\n",
    "    params = {}\n",
    "    #  Initializes the weight matrix W1 for the first layer. It uses NumPy's np.random.randn function \n",
    "    # to generate random numbers from a standard normal distribution (mean 0, standard deviation 1) \n",
    "    # and multiplies them by 0.01 to scale the values. The shape of W1 is (n_hidden, n_input), \n",
    "    # which corresponds to the number of hidden units and input features.\n",
    "    params['W1'] = np.random.randn(n_hidden, n_input) * 0.01\n",
    "    #  Initializes the bias vector b1 for the first layer. It sets all the bias values to zero. \n",
    "    # The shape of b1 is (n_hidden, 1).\n",
    "    params['b1'] = np.zeros((n_hidden, 1))\n",
    "    # Initializes the weight matrix W2 for the second (output) layer in a similar way as W1. \n",
    "    # The shape of W2 is (n_output, n_hidden).\n",
    "    params['W2'] = np.random.randn(n_output, n_hidden) * 0.01\n",
    "    #  Initializes the bias vector b2 for the second layer, also setting all the bias values to zero. \n",
    "    # The shape of b2 is (n_output, 1).\n",
    "    params['b2'] = np.zeros((n_output, 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae7db928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape (10, 20)\n",
      "b1 shape (10, 1)\n",
      "W2 shape (1, 10)\n",
      "b2 shape (1, 1)\n"
     ]
    }
   ],
   "source": [
    "params = init_weights(n_features, n_hidden, 1)\n",
    "\n",
    "print('W1 shape', params['W1'].shape)\n",
    "print('b1 shape', params['b1'].shape)\n",
    "print('W2 shape', params['W2'].shape)\n",
    "print('b2 shape', params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c045503",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "Let's implement the forward process following equations $(5) \\sim (8)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15e28483",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code defines a Python function named forward that performs the forward propagation step of a neural network. \n",
    "Forward propagation computes the activations of each layer given the input data and the network's parameters. \n",
    "The function takes two arguments:\n",
    "\n",
    "- X      : A NumPy array representing the input data. The shape is expected to be (n_features x m_samples), \n",
    "           where n_features is the number of input features, and m_samples is the number of data samples.\n",
    "\n",
    "- params : A dictionary containing the network's parameters, including the weight matrices and bias vectors for \n",
    "           each layer. These parameters are typically initialized using the init_weights function.\n",
    "'''\n",
    "def forward(X, params):\n",
    "    # Unpacks the parameters from the params dictionary\n",
    "    W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n",
    "    # Initializes A0 as the input data\n",
    "    A0 = X\n",
    "    # Initializes a dictionary cache to store intermediate values during forward propagation\n",
    "    cache = {}\n",
    "    #\n",
    "    # Computes the linear transformation and activation for the first layer (hidden layer):\n",
    "    #\n",
    "    # Compute the linear transformation Z1 by taking the dot product of the weight matrix W1 and the \n",
    "    # input A0, and adding the bias vector b1:\n",
    "    Z1 = np.dot(W1, A0) + b1\n",
    "    # Compute the activation A1 of the first layer by applying the hyperbolic tangent (tanh) \n",
    "    # activation function to Z1:\n",
    "    A1 = tanh(Z1)\n",
    "    #\n",
    "    # Computes the linear transformation and activation for the second layer (output layer):\n",
    "    #\n",
    "    # Compute the linear transformation Z2 by taking the dot product of the weight matrix W2 and the \n",
    "    # activation A1 from the first layer, and adding the bias vector b2:\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    # Compute the activation A2 of the second layer by applying the sigmoid activation function to Z2:\n",
    "    A2 = sigmoid(Z2)\n",
    "    # Stores the computed values in the cache dictionary for later use, typically in the backpropagation step\n",
    "    cache['Z1'] = Z1\n",
    "    cache['A1'] = A1\n",
    "    cache['Z2'] = Z2\n",
    "    cache['A2'] = A2\n",
    "    return  cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d8d06d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 shape (10, 100)\n",
      "A1 shape (10, 100)\n",
      "Z2 shape (1, 100)\n",
      "A2 shape (1, 100)\n"
     ]
    }
   ],
   "source": [
    "# get 100 samples\n",
    "inp = X[:100].T\n",
    "\n",
    "cache = forward(inp, params)\n",
    "\n",
    "print('Z1 shape', cache['Z1'].shape)\n",
    "print('A1 shape', cache['A1'].shape)\n",
    "print('Z2 shape', cache['Z2'].shape)\n",
    "print('A2 shape', cache['A2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20caac1",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "Following equation $(9)$, let's calculate the loss of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9716334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The loss function calculates the binary cross-entropy loss, which is commonly used as a loss function in \n",
    "binary classification problems. It quantifies the dissimilarity between the true values and the predicted \n",
    "values, with lower values indicating a better match between predictions and ground truth.\n",
    "\n",
    "The function takes two arguments:\n",
    "\n",
    "- Y    : A NumPy array representing the true values. It is expected to be a vector (1-dimensional array).\n",
    "- Y_hat: A NumPy array representing the predicted values. It is also expected to be a vector with the same shape as Y.\n",
    "'''\n",
    "def loss(Y, Y_hat):\n",
    "    # This assertion checks that Y is a row vector (1 row, multiple columns). It ensures that the true \n",
    "    # values are organized as a row vector.\n",
    "    assert Y.shape[0] == 1\n",
    "    # This assertion checks that the shape of Y matches the shape of Y_hat, which means they should have \n",
    "    # the same number of elements. This is essential to compare true and predicted values.\n",
    "    assert Y.shape == Y_hat.shape\n",
    "    # calculates the number of samples, denoted as m, by getting the number of columns in the Y array:\n",
    "    m = Y.shape[1]\n",
    "    # computes the binary cross-entropy loss. We calculate the loss for each individual sample in the \n",
    "    # vectors Y and Y_hat. Then we sum the log likelihood of the true class (if Y is 1) and the log \n",
    "    # likelihood of the complementary class (if Y is 0) for each sample. Finally, we compute the overall \n",
    "    # loss by taking the negative sum of the s values and dividing by the number of samples m. \n",
    "    # This is the average loss over all samples:\n",
    "    s = Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)\n",
    "    loss = -np.sum(s) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42c1b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.0113017144599339\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([np.random.choice([0, 1]) for i in range(10)]).reshape(1, -1)\n",
    "Y_hat = np.random.uniform(0, 1, 10).reshape(1, -1)\n",
    "\n",
    "l = loss(Y, Y_hat)\n",
    "print(f'loss {l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd519697",
   "metadata": {},
   "source": [
    "### Delta Rule\n",
    "\n",
    "Now it comes to the so called ***delta rule*** which is the key to our weights update. With *delta rule* we compute the gradient of the loss function with respect to the weights of the network for a single input–output example.\n",
    "\n",
    "Given a generic actual value $y$, we want to minimize the loss $L$, and the technic we are going to apply here is gradient descent, basically what we need to do is to apply derivative to our variables and move them slightly down to the optimum. Here we have 2 variables, $W$ and $b$, and for this example, the update formula of them would be:\n",
    "\n",
    "\\begin{align}\n",
    "& W_{new} = W_{old} - \\frac{\\partial L}{\\partial W} \\Rightarrow \\Delta W = - \\frac{\\partial L}{\\partial W} \\\\\n",
    "& b_{new} = b_{old} - \\frac{\\partial L}{\\partial b} \\Rightarrow \\Delta b = - \\frac{\\partial L}{\\partial b}\n",
    "\\end{align}\n",
    "\n",
    "The delta rule algorithm works by computing the gradient of the loss function with respect to each weight. In order to get the derivative of our targets, chain rules would be applied:\n",
    "\n",
    "\\begin{align}\n",
    "& \\Delta W^{[1]} = - \\frac{\\partial L}{\\partial W^{[1]}} \\\\\n",
    "& \\Delta W^{[2]} = - \\frac{\\partial L}{\\partial W^{[2]}} \\\\\n",
    "& \\frac{\\partial L}{\\partial W^{[i]}} =  \\frac{\\partial L}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial W^{[i]}} \n",
    "\\end{align}\n",
    "\n",
    "where $i=1,2$\n",
    "\n",
    "Given the loss function $L$ we defined above, we can easily compute the first partial derivative with respect to $\\hat y$:\n",
    "\n",
    "$$L(y, \\hat{y}) = -[y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})}] \\Rightarrow \n",
    "\\frac{\\partial L}{\\partial \\hat y} = -\\frac{y}{\\hat y} + \\frac{1-y}{1-\\hat y} = \\frac{\\hat y - y}{\\hat y(1 - \\hat y)}$$\n",
    "\n",
    "Remeber that in our notation we have:\n",
    "\n",
    "$$\\hat y = \\sigma\\left( Z^{[2]} \\right) = A^{[2]}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741dad7",
   "metadata": {},
   "source": [
    "So we can write for the two first partial derivatives of our chain:\n",
    "\n",
    "\\begin{align}\n",
    "& \\frac{\\partial L}{\\partial \\hat y} =  \\frac{A^{[2]} - y}{A^{[2]}\\left(1 - A^{[2]}\\right)} \\\\\n",
    "& \\frac{\\partial \\hat y}{\\partial Z^{[2]}} = \\frac{\\partial \\sigma\\left( Z^{[2]} \\right)}{\\partial Z^{[2]}} \n",
    "= \\sigma\\left(Z^{[2]}\\right) \\cdot \\left[1 - \\sigma\\left(Z^{[2]}\\right)\\right] = {\\hat y}\\, (1 - \\hat y) = A^{[2]} \\left( 1 - A^{[2]} \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a762170",
   "metadata": {},
   "source": [
    "putting it all together\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial W^{[i]}} \n",
    "& = \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial Z^{[2]}} \\cdot \\frac{\\partial Z^{[2]}}{\\partial W^{[i]}} \\\\\n",
    "& = \\frac{A^{[2]} - y}{A^{[2]}\\left(1 - A^{[2]}\\right)} \\cdot A^{[2]} \\left( 1 - A^{[2]} \\right) \\cdot  \\frac{\\partial Z^{[2]}}{\\partial W^{[i]}} \\\\\n",
    "& = \\left( A^{[2]} - y \\right) \\cdot \\frac{\\partial Z^{[2]}}{\\partial W^{[i]}}\n",
    "\\end{align}\n",
    "\n",
    "where $i=1,2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f97274",
   "metadata": {},
   "source": [
    "Now we have only to compute the partial derivatives of the pre-activation function $Z^{[2]}$ with respect to $W^{[1]}$ and $W^{[2]}$. \n",
    "\n",
    "Remember that \n",
    "\n",
    "$$Z^{[2]} = W^{[2]} \\cdot A^{[1]}  + B^{[2]} = W^{[2]} \\cdot \\tanh\\left( W^{[1]} \\cdot X + B^{[1]} \\right) + B^{[2]}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$A^{[1]} = \\tanh\\left( W^{[1]} \\cdot X + B^{[1]}\\right) = \\tanh\\left( Z^{[1]} \\right)$$\n",
    "\n",
    "We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b97d53",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "& \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} = A^{[1]} \\\\\n",
    "& \\frac{\\partial Z^{[2]}}{\\partial W^{[1]}} = W^{[2]} \\cdot \\frac{\\partial \\tanh\\left(Z^{[1]} \\right)}{\\partial Z^{[1]}} \\cdot \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7828b18",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial \\tanh\\left(Z^{[1]} \\right)}{\\partial Z^{[1]}}  = 1 - \\left[ \\tanh\\left(Z^{[1]} \\right) \\right]^2 = \n",
    "1 - \\left( A^{[1]} \\right)^2 \\\\\n",
    "& \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} = X\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{equation} \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Z^{[2]}}{\\partial W^{[1]}} = W^{[2]} \\cdot X \\cdot \\left[ 1 - \\left( A^{[1]} \\right)^2 \\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c627b",
   "metadata": {},
   "source": [
    "Now we have all the elements to calculate the derivative of the loss function with respect to the weights \n",
    "\n",
    "\\begin{align}\n",
    "& \\frac{\\partial L}{\\partial W^{[1]}} = \\left( A^{[2]} - y \\right) \\cdot \\frac{\\partial Z^{[2]}}{\\partial W^{[1]}} = \\left( A^{[2]} - y \\right) \\cdot W^{[2]} \\cdot X \\cdot \\left[ 1 - \\left( A^{[1]} \\right)^2 \\right] \\\\\n",
    "& \\frac{\\partial L}{\\partial W^{[2]}} = \\left( A^{[2]} - y \\right) \\cdot \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} = \n",
    "\\left( A^{[2]} - y \\right) \\cdot A^{[1]}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920270a",
   "metadata": {},
   "source": [
    "In the python code we put also:\n",
    "\n",
    "\\begin{align} \n",
    "& \\Delta^{[2]} = A^{[2]} - y  \\\\\n",
    "& \\Delta^{[1]} = \\Delta^{[2]} \\cdot W^{[2]} \\cdot (1 - A^{[1]^2})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9274b",
   "metadata": {},
   "source": [
    "So we finally have\n",
    "\n",
    "\\begin{align}\n",
    "& \\Delta W^{[2]} = -\\frac{\\partial L}{\\partial W^{[2]}} = - \\Delta^{[2]} \\cdot A^{[1]} \\\\\n",
    "& \\Delta W^{[1]} = -\\frac{\\partial L}{\\partial W^{[1]}} = - \\Delta^{[1]} \\cdot X\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3dab9",
   "metadata": {},
   "source": [
    "For completeness we also report the formulas for updating the bias vectors\n",
    "\n",
    "\\begin{align}\n",
    "& \\Delta b^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True) \n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation} \n",
    "\\Delta b^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)  \n",
    "\\end{equation}\n",
    "\n",
    "In summary:\n",
    "\n",
    "- Error is calculated between the expected outputs and the outputs forward propagated from the network.\n",
    "- These errors are then propagated backward through the network from the output layer to the hidden layer, assigning a penalty for the error and updating weights as they go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195e125",
   "metadata": {},
   "source": [
    "Let's break down the shape of each element, given number of each layer equals `(n_x, n_h, n_y)` and batch size equals `m`:\n",
    "\n",
    "- $A^{[2]}$, $Y$ and $dZ^{[2]}$ has shape `(n_y, m)`\n",
    "- Because $A^{[1]}$ has shape `(n_h, m)`, $dW^{[2]}$ would have shape `(n_y, n_h)`\n",
    "- $db^{[2]}$ has shape `(n_y, 1)`\n",
    "\n",
    "- Because $dZ^{[2]}$ has shape `(n_y, m)`, $W^{[2]}$ has shape`(n_y, n_h)`, $dZ^{[1]}$ would have shape `(n_h, m)`\n",
    "- In equation $(5)$, $X$ has shape `(n_x, m)`, so $dW^{[1]}$ has shape `(n_h, n_x)`\n",
    "- $db^{[1]}$ has shape `(n_h, 1)`\n",
    "\n",
    "\n",
    "Once we understand the formula, implementation should come with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e95dc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(params, cache, X, Y):\n",
    "    \"\"\"\n",
    "    [From coursera deep-learning course]\n",
    "    params: we initiate above with W1, b1, W2, b2\n",
    "    cache: the intermediate caculation we saved with Z1, A1, Z2, A2\n",
    "    X: shape of (n_x, m)\n",
    "    Y: shape (n_y, m)\n",
    "    \"\"\"\n",
    "    # Calculate the number of training examples m from the shape of the input data X:\n",
    "    m = X.shape[1]\n",
    "    # Extract the necessary parameters and intermediate values from the provided params and cache dictionaries\n",
    "    W1 = params['W1']\n",
    "    W2 = params['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    #\n",
    "    # Compute the gradients for the parameters in the backward pass of backpropagation\n",
    "    #\n",
    "    # DL2 represents the derivative of the loss with respect to the activations of the output \n",
    "    # layer (A2) and is calculated as the difference between A2 and Y:\n",
    "    DL2 = A2 - Y\n",
    "    # dW2 represents the gradient of the loss with respect to the weights of the second layer\n",
    "    dW2 = (1 / m) * np.dot(DL2, A1.T)\n",
    "    # db2 represents the gradient of the loss with respect to the biases of the second layer \n",
    "    db2 = (1 / m) * np.sum(DL2, axis=1, keepdims=True)\n",
    "    # DL1 represents the derivative of the loss with respect to the activations of the first \n",
    "    # hidden layer (A1) and is computed using the chain rule. It involves the element-wise product \n",
    "    # of the transpose of W2 and the derivative of the activation function applied to A1:\n",
    "    DL1 = np.multiply(np.dot(W2.T, DL2), 1 - np.power(A1, 2))\n",
    "    # dW1 represents the gradient of the loss with respect to the weights of the first layer \n",
    "    dW1 = (1 / m) * np.dot(DL1, X.T)\n",
    "    # db1 represents the gradient of the loss with respect to the biases of the first layer\n",
    "    db1 = (1 / m) * np.sum(DL1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c031d",
   "metadata": {},
   "source": [
    "### Batch Loader\n",
    "\n",
    "Now let's ensemble everything into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0c7a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNN:\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.params = {}\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Y: vector of true value\n",
    "        Y_hat: vector of predicted value\n",
    "        \"\"\"\n",
    "        assert Y.shape[0] == 1\n",
    "        assert Y.shape == Y_hat.shape\n",
    "        m = Y.shape[1]\n",
    "        s = Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)\n",
    "        loss = -np.sum(s) / m\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.params['W1'] = np.random.randn(self.n_hidden, self.n_input) * 0.01\n",
    "        self.params['b1'] = np.zeros((self.n_hidden, 1))\n",
    "        self.params['W2'] = np.random.randn(self.n_output, self.n_hidden) * 0.01\n",
    "        self.params['b2'] = np.zeros((self.n_output, 1))\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: need to have shape (n_features x m_samples)\n",
    "        \"\"\"\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        A0 = X\n",
    "\n",
    "        Z1 = np.dot(W1, A0) + b1\n",
    "        A1 = tanh(Z1)\n",
    "        Z2 = np.dot(W2, A1) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        self.cache['Z1'] = Z1\n",
    "        self.cache['A1'] = A1\n",
    "        self.cache['Z2'] = Z2\n",
    "        self.cache['A2'] = A2\n",
    "     \n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        [From coursera deep-learning course]\n",
    "        params: we initiate above with W1, b1, W2, b2\n",
    "        cache: the intermediate caculation we saved with Z1, A1, Z2, A2\n",
    "        X: shape of (n_x, m)\n",
    "        Y: shape (n_y, m)\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[1]\n",
    "\n",
    "        W1 = self.params['W1']\n",
    "        W2 = self.params['W2']\n",
    "        A1 = self.cache['A1']\n",
    "        A2 = self.cache['A2']\n",
    "\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "        dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        self.grads = {\"dW1\": dW1,\n",
    "                      \"db1\": db1,\n",
    "                      \"dW2\": dW2,\n",
    "                      \"db2\": db2}\n",
    "\n",
    "        \n",
    "    def get_batch_indices(self, X_train, batch_size):\n",
    "        n = X_train.shape[0]\n",
    "        indices = [range(i, i+batch_size) for i in range(0, n, batch_size)]\n",
    "        return indices\n",
    "    \n",
    "    \n",
    "    def update_weights(self, lr):\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        dW1, db1, dW2, db2 = self.grads['dW1'], self.grads['db1'], self.grads['dW2'], self.grads['db2']\n",
    "        self.params['W1'] -= dW1\n",
    "        self.params['W2'] -= dW2\n",
    "        self.params['b1'] -= db1\n",
    "        self.params['b2'] -= db2\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train, batch_size=32, n_iterations=100, lr=0.01):\n",
    "        self.init_weights()\n",
    "        \n",
    "        indices = self.get_batch_indices(X_train, batch_size)\n",
    "        for i in range(n_iterations):\n",
    "            for ind in indices:\n",
    "                X = X_train[ind, :].T\n",
    "                Y = y_train[ind].reshape(1, batch_size)\n",
    "                \n",
    "                self.forward(X)\n",
    "                self.backward(X, Y)\n",
    "                self.update_weights(lr)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                Y_hat = self.cache['A2']\n",
    "                loss = self.compute_loss(Y, Y_hat)\n",
    "                print(f'iteration {i}: loss {loss}')\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        A0 = X\n",
    "\n",
    "        Z1 = np.dot(W1, A0) + b1\n",
    "        A1 = tanh(Z1)\n",
    "        Z2 = np.dot(W2, A1) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        return A2\n",
    "\n",
    "    \n",
    "def accuracy(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_pred: vector of predicted value\n",
    "    \"\"\"\n",
    "    def _to_binary(x):\n",
    "        return 1 if x > .5 else 0\n",
    "\n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape    == Y_pred.shape\n",
    "    Y_pred = np.vectorize(_to_binary)(Y_pred)\n",
    "    acc = float(np.dot(Y, Y_pred.T) + np.dot(1 - Y, 1 - Y_pred.T))/Y.size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b46709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShallowNN(n_features, n_hidden, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9fe4786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.42147746598052555\n",
      "iteration 10: loss 0.1901101601962515\n",
      "iteration 20: loss 0.15626532588360817\n",
      "iteration 30: loss 0.14470635638636606\n",
      "iteration 40: loss 0.14852334807135942\n",
      "iteration 50: loss 0.13027184786614096\n",
      "iteration 60: loss 0.13472349198789163\n",
      "iteration 70: loss 0.12052915369513048\n",
      "iteration 80: loss 0.12676933747218413\n",
      "iteration 90: loss 0.11729025107809435\n",
      "iteration 100: loss 0.11364812594564792\n",
      "iteration 110: loss 0.12557974001276015\n",
      "iteration 120: loss 0.11827348223626151\n",
      "iteration 130: loss 0.10607862228992683\n",
      "iteration 140: loss 0.11802256774756886\n",
      "iteration 150: loss 0.11523306234640965\n",
      "iteration 160: loss 0.1214301412374162\n",
      "iteration 170: loss 0.11254566659994955\n",
      "iteration 180: loss 0.13385976267850247\n",
      "iteration 190: loss 0.11075181299051788\n",
      "iteration 200: loss 0.13265456887004898\n",
      "iteration 210: loss 0.11711061808925197\n",
      "iteration 220: loss 0.12471366505549523\n",
      "iteration 230: loss 0.11340277526737609\n",
      "iteration 240: loss 0.11215853181398637\n",
      "iteration 250: loss 0.11326409446052274\n",
      "iteration 260: loss 0.11115007456437377\n",
      "iteration 270: loss 0.11499048908025164\n",
      "iteration 280: loss 0.11252617264788413\n",
      "iteration 290: loss 0.11232982350922231\n",
      "iteration 300: loss 0.1116244200325167\n",
      "iteration 310: loss 0.11105092094457618\n",
      "iteration 320: loss 0.11055705782618523\n",
      "iteration 330: loss 0.11067126936903762\n",
      "iteration 340: loss 0.11073913368863005\n",
      "iteration 350: loss 0.11070815278245519\n",
      "iteration 360: loss 0.11066857160832994\n",
      "iteration 370: loss 0.11064190820804716\n",
      "iteration 380: loss 0.11063494983780968\n",
      "iteration 390: loss 0.11064744821000101\n",
      "iteration 400: loss 0.11066889927789397\n",
      "iteration 410: loss 0.1106770094507932\n",
      "iteration 420: loss 0.11064842824981684\n",
      "iteration 430: loss 0.11057594264934778\n",
      "iteration 440: loss 0.1104706498153865\n",
      "iteration 450: loss 0.11034770443090737\n",
      "iteration 460: loss 0.11021723071687804\n",
      "iteration 470: loss 0.11008481940157903\n",
      "iteration 480: loss 0.10995422943587776\n",
      "iteration 490: loss 0.10982921177581599\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=100, n_iterations=500, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fd3baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 93.89999999999999%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9412\\3223901223.py:134: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  acc = float(np.dot(Y, Y_pred.T) + np.dot(1 - Y, 1 - Y_pred.T))/Y.size\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(X_test.T)\n",
    "\n",
    "acc = accuracy(y_test.reshape(1, -1), y_preds)\n",
    "print(f'accuracy: {acc*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc6691",
   "metadata": {},
   "source": [
    "## Keras: the Python Deep Learning API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42086087",
   "metadata": {},
   "source": [
    "In this chapter we will present the code samples found in Chapter 2, Section 1 of **Deep Learning with Python** by François Chollet the creator of ***Keras***.\n",
    "\n",
    "***Keras*** is a high-level Deep Learning API that allows you to easily build, train, evaluate and execute all sorts of neural networks. Its documentation (or specification) is available at https://keras.io. It was developed by ***François Chollet*** as part of a research project and released as an open source project in March 2015. It quickly gained popularity owing to its ease-of-use, flexibility and beautiful design. To perform the heavy computations required by neural networks, keras-team relies on a computation backend. At the present, you can choose from three popular open source deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or Theano.\n",
    "\n",
    "The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been around for almost as long as the field itself and has been very intensively studied. It's a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of \"solving\" MNIST as the \"Hello World\" of deep learning -- it's what you do to verify that your algorithms are working as expected. As you become a machine learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3ff39",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-07-01-05.jpg\" width=\"300\" height=\"300\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f787e",
   "metadata": {},
   "source": [
    "![ch-01-01-20.jpg](./pics/ch-07-01-05.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c70b4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c1412",
   "metadata": {},
   "source": [
    "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d32e0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452dfbd",
   "metadata": {},
   "source": [
    "`train_images` and `train_labels` form the \"training set\", the data that the model will learn from. The model will then be tested on the \n",
    "\"test set\", `test_images` and `test_labels`. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging \n",
    "from 0 to 9. There is a one-to-one correspondence between the images and the labels.\n",
    "\n",
    "Let's have a look at the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f0133e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "394ffe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ca70a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69675a12",
   "metadata": {},
   "source": [
    "Let's have a look at the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59bc8cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b82d045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(test_labels))\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8fce9981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d392af",
   "metadata": {},
   "source": [
    "Our workflow will be as follow: first we will present our neural network with the training data, `train_images` and `train_labels`. The \n",
    "network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for `test_images`, and we \n",
    "will verify if these predictions match the labels from `test_labels`.\n",
    "\n",
    "Let's build our network. The code below defines a simple neural network model with two dense layers. The first layer has 512 neurons with ReLU activation, and the second layer has 10 neurons with softmax activation, which makes it suitable for multi-class classification tasks. This model can be used for tasks such as image classification, where input images are 28x28 pixels and there are 10 possible classes to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f495b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# This line creates a sequential model, which is a linear stack of layers. In a sequential model, you \n",
    "# can add layers one after another, and the data flows through the layers from input to output.\n",
    "network = models.Sequential()\n",
    "# This line adds a dense layer to the model. Here's what each argument means:\n",
    "# - 512                    : This is the number of neurons (units) in the layer. The layer has 512 neurons, \n",
    "#                            and this is the output dimension of the layer.\n",
    "# - activation='relu'      : This specifies that the ReLU (Rectified Linear Unit) activation function is used \n",
    "#                            for this layer. ReLU is a popular activation function that introduces non-linearity \n",
    "#                            into the network.\n",
    "# - input_shape=(28 * 28,) : This specifies the shape of the input data that the model expects. In this case, \n",
    "#                            it's a flattened 28x28 image, so it has 784 input features.\n",
    "# This dense layer takes the 784 input features, processes them through 512 neurons with ReLU activation, \n",
    "# and produces a 512-dimensional output.\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "# This line adds a second dense layer to the model with 10 neurons. This layer uses the softmax activation function.\n",
    "# The softmax activation is often used in the output layer of a classification model. It converts the network's \n",
    "# raw output values into a probability distribution over multiple classes. In this case, it's used for multi-class \n",
    "# classification, where the network is expected to predict one of 10 possible classes.\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb88c62",
   "metadata": {},
   "source": [
    "Let’s go through this code line by line:\n",
    "\n",
    "- The first line creates a Sequential model. This is the simplest kind of Keras model, for neural networks that are just composed of a single stack of layers, connected sequentially. This is called the sequential API.\n",
    "\n",
    "- Next, we build the first layer and add it to the model. It is ***Dense*** hidden layer with 512 neurons. It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes \n",
    "\n",
    "$$\\phi \\left( Z^{[1]} = W^{[1]} \\cdot X + B^{[1]} \\right), \\quad \\phi(z) = \\textit{ReLU}(z)$$\n",
    "\n",
    "- Finally, we add a Dense output layer with 10 neurons (one per class). Using a 10-way \"softmax\" layer means that it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c4021",
   "metadata": {},
   "source": [
    ">**NOTE** - **The ReLU Activation Function** The Rectified Linear Unit (ReLU) activation function is a simple but widely used nonlinear activation function in artificial neural networks. It is defined as:\n",
    ">\n",
    ">$$f(x) = max(0, x)$$\n",
    ">\n",
    ">In other words, ReLU returns 0 for any negative input, and for positive input, it returns the same value. This function introduces sparsity in the network, as it zeros out negative values, which can help in reducing the likelihood of vanishing gradients during training. ReLU is computationally efficient and has been found to be effective in many deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d81d48",
   "metadata": {},
   "source": [
    ">**NOTE** - **The softmax Activation Function** The softmax activation function is commonly used in multi-class classification problems. It takes a vector of arbitrary real-valued scores as input and converts them into probabilities that sum up to 1. Mathematically, the softmax function is defined as follows:\n",
    ">\n",
    ">$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}$$\n",
    ">\n",
    ">where $x_i$ represents the input score for class $i$, $N$ is the total number of classes, and $e$ is Euler's number (approximately 2.71828). The softmax function essentially ***transforms the input scores into a probability distribution over multiple classes***. This is achieved by exponentiating each score to ensure non-negativity and then normalizing the resulting values to sum up to 1. The higher the input score for a particular class relative to the other classes, the higher the probability assigned to that class by the softmax function. Softmax is typically used as the output activation function in the last layer of a neural network for multi-class classification tasks. It helps in making the network output interpretable as probabilities, which facilitates decision-making based on the most likely class prediction. Additionally, softmax enables the use of cross-entropy loss, which is commonly used as the objective function for training classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c477d",
   "metadata": {},
   "source": [
    "The model’s summary() method displays all the model’s layers, including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4eac9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407050 (1.55 MB)\n",
      "Trainable params: 407050 (1.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339e159",
   "metadata": {},
   "source": [
    "Note that Dense layers often have a lot of parameters. For example, **the first hidden\n",
    "layer has 784 × 512 connection weights, plus 512 bias terms, which adds up to\n",
    "401920 parameters!** This gives the model quite a lot of flexibility to fit the training\n",
    "data, but it also means that the model runs the risk of overfitting, especially when you\n",
    "do not have a lot of training data. We will come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5721740",
   "metadata": {},
   "source": [
    "To make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n",
    "\n",
    "* A **loss function**: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be \n",
    "able to steer itself in the right direction.\n",
    "* An **optimizer**: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "* **Metrics** to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly \n",
    "classified)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d178b0",
   "metadata": {},
   "source": [
    "see [here](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a) for a description of rmsprop and [here](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) for crossentropy, note that for a binary classification, where the number of classes M equals 2, cross-entropy is exactly the loss function of the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "995ee548",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30536f",
   "metadata": {},
   "source": [
    "\n",
    "Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in \n",
    "the `[0, 1]` interval. Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with \n",
    "values in the `[0, 255]` interval. We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e1a4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf5e74",
   "metadata": {},
   "source": [
    "We also need to categorically encode the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ba334e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "292ed459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5571f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e8a77",
   "metadata": {},
   "source": [
    "**What is an epoch?**\n",
    "\n",
    "An epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. Datasets are usually grouped into batches (especially when the amount of data is very large). Some people use the term iteration loosely and refer to putting one batch through the model as an iteration.   \n",
    "\n",
    "If the batch size is the whole training dataset then the number of epochs is the number of iterations. For practical reasons, this is usually not the case. Many models are created with more than one epoch. The general relation where dataset size is d, number of epochs is e, number of iterations is i, and batch size is b would be d*e = i*b. \n",
    "\n",
    "Determining how many epochs a model should run to train is based on many parameters related to both the data itself and the goal of the model, and while there have been efforts to turn this process into an algorithm, often a deep understanding of the data itself is indispensable.\n",
    "\n",
    "We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network: \n",
    "we \"fit\" the model to its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7335e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2624 - accuracy: 0.9236\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1062 - accuracy: 0.9689\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0701 - accuracy: 0.9792\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0502 - accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0374 - accuracy: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1cc0594b9a0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1195088",
   "metadata": {},
   "source": [
    "Two quantities are being displayed during training: the \"loss\" of the network over the training data, and the accuracy of the network over \n",
    "the training data.\n",
    "\n",
    "We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let's check that our model performs well on the test set too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e598b630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0712 - accuracy: 0.9790\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a188c203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.9789999723434448\n"
     ]
    }
   ],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836ecdb",
   "metadata": {},
   "source": [
    "Our test set accuracy turns out to be 97.8% -- that's quite a bit lower than the training set accuracy. \n",
    "This gap between training accuracy and test accuracy is an example of \"overfitting\", \n",
    "the fact that machine learning models tend to perform worse on new data than on their training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e1c84",
   "metadata": {},
   "source": [
    "**Accessing single layer weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8d639f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_4\n",
      "2\n",
      "dense_5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for lay in network.layers:\n",
    "    print(lay.name)\n",
    "    print(len(lay.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6a52247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(512, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "out_layer    = network.layers[1]\n",
    "out_weights  = out_layer.get_weights()\n",
    "\n",
    "print(len(out_weights))\n",
    "print(out_weights[0].shape) # W matrix\n",
    "print(out_weights[1].shape) # b array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc209e10",
   "metadata": {},
   "source": [
    "**Use of History property**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bfc0957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.concatenate((train_images, test_images))\n",
    "Y = np.concatenate((train_labels, test_labels))\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "974ef2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1466/1466 [==============================] - 12s 8ms/step - loss: 0.0431 - accuracy: 0.9861 - val_loss: 0.0598 - val_accuracy: 0.9815\n",
      "Epoch 2/5\n",
      "1466/1466 [==============================] - 11s 8ms/step - loss: 0.0329 - accuracy: 0.9895 - val_loss: 0.0688 - val_accuracy: 0.9808\n",
      "Epoch 3/5\n",
      "1466/1466 [==============================] - 12s 8ms/step - loss: 0.0259 - accuracy: 0.9918 - val_loss: 0.0689 - val_accuracy: 0.9813\n",
      "Epoch 4/5\n",
      "1466/1466 [==============================] - 12s 8ms/step - loss: 0.0180 - accuracy: 0.9945 - val_loss: 0.0692 - val_accuracy: 0.9819\n",
      "Epoch 5/5\n",
      "1466/1466 [==============================] - 12s 8ms/step - loss: 0.0143 - accuracy: 0.9959 - val_loss: 0.0676 - val_accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "history = network.fit(X, Y, validation_split=0.33, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d71410c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09877d1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# summarize history for accuracy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2f538",
   "metadata": {},
   "source": [
    "This concludes our very first example: you just saw how we could build and a train a neural network to classify handwritten digits, in less than 20 lines of Python code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5520a43",
   "metadata": {},
   "source": [
    "## Other types of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816cff40",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNNs) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.\n",
    "\n",
    "Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71498a8b",
   "metadata": {},
   "source": [
    "## Appendix - Calculation of Activation Functions Derivatives\n",
    "\n",
    "### Derivative of the Hyperbolic Tangent\n",
    "\n",
    "$$\\frac{d}{{dx}}\\tanh x = \\frac{d}{{dx}}\\left( {\\frac{{{e^x} – {e^{ – x}}}}{{{e^x} + {e^{ – x}}}}} \\right)\n",
    "= \\frac{{\\left( {{e^x} + {e^{ – x}}} \\right)\\frac{d}{{dx}}\\left( {{e^x} – {e^{ – x}}} \\right) – \\left( {{e^x} + {e^{ – x}}} \\right)\\frac{d}{{dx}}\\left( {{e^x} – {e^{ – x}}} \\right)}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}}$$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered} \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{\\left( {{e^x} + {e^{ – x}}} \\right)\\left( {{e^x} + {e^{ – x}}} \\right) – \\left( {{e^x} + {e^{ – x}}} \\right)\\left( {{e^x} + {e^{ – x}}} \\right)}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2} – {{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{\\left( {{e^{2x}} + {e^{ – 2x}} + 2{e^x}{e^{ – x}}} \\right) – \\left( {{e^{2x}} + {e^{ – 2x}} – 2{e^x}{e^{ – x}}} \\right)}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{\\left( {{e^{2x}} + {e^{ – 2x}} + 2} \\right) – \\left( {{e^{2x}} + {e^{ – 2x}} – 2} \\right)}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{{{e^{2x}} + {e^{ – 2x}} + 2 – {e^{2x}} – {e^{ – 2x}} + 2}}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = \\frac{4}{{{{\\left( {{e^x} + {e^{ – x}}} \\right)}^2}}} \\\\ \\Rightarrow \\frac{d}{{dx}}\\left( {\\tanh x} \\right) = 1 - {\\left( {\\frac{e^x - e^{-x}}{{{e^x} + {e^{ – x}}}}} \\right)^2} = 1 - {\\left(\\tanh x \\right)}^2 \\\\ \\end{gathered}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Derivative of the Sigmoid\n",
    "\n",
    "\\begin{align*}\n",
    "\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\notag \\\\\n",
    "&= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\notag\\\\\n",
    "&= -(1 + e^{-x})^{-2}(-e^{-x}) \\notag\\\\\n",
    "&= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\notag\\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}}  \\notag\\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\notag\\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\notag\\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\notag\\\\\n",
    "&= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170a8d3",
   "metadata": {},
   "source": [
    "## References and Credits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dec0c6",
   "metadata": {},
   "source": [
    "***Chollet F.***, \"*Deep Learning with Python*\" Manning (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31661bd4",
   "metadata": {},
   "source": [
    "***Jeremy Z.***, \"*Build a Shallow Neural Network*\" click [here](https://towardsdatascience.com/building-a-shallow-neural-network-a4e2728441e0)  for the original post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
