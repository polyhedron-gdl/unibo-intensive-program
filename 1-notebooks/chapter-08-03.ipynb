{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14a7f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/unibo-intensive-program-2024/blob/main/1-notebooks/chapter-08-03.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d972216",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning in Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64426062",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7cab4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "- Generative Adversarial Networks (GANs) represent one of the most important development in the field of artificial intelligence, particularly in the domain of generative modeling. \n",
    "\n",
    "- Introduced by Ian Goodfellow and his colleagues in 2014, GANs have revolutionized the way machines can generate realistic images, texts, and other forms of data. \n",
    "\n",
    "- The essence of GANs lies in their unique architecture and training methodology, which involves a dueling process between two neural networks: the Generator and the Discriminator.\n",
    "\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-03-00.jpg\" width=\"500\" height=\"400\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318cb195",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Architecture of GANs\n",
    "\n",
    "The architecture of a GAN is composed of two main components: the Generator and the Discriminator, each playing a distinct role in the generative process.\n",
    "\n",
    "1. **Generator**: The Generator network takes random noise as input and transforms it into data that resemble the target distribution. The goal of the Generator is to produce outputs indistinguishable from real data, essentially \"fooling\" the Discriminator. It learns to generate more realistic data over time as it adjusts its parameters based on the feedback from the Discriminator.\n",
    "\n",
    "2. **Discriminator**: The Discriminator network acts as a classifier, tasked with distinguishing between real data (from the training set) and fake data produced by the Generator. It is trained to improve its accuracy in detecting the Generator's outputs as fake.\n",
    "\n",
    "These two networks engage in a continuous game, with the Generator striving to produce increasingly convincing data, while the Discriminator becomes better at distinguishing real from fake. This adversarial process drives the improvement of both networks, leading to the generation of highly realistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a8c83",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training Phase\n",
    "\n",
    "Training a GAN involves alternating between training the Discriminator and the Generator, with both networks being updated based on their performance in each iteration.\n",
    "\n",
    "1. **Training the Discriminator**: In this step, the Discriminator is trained with a batch of real data labeled as real and a batch of fake data generated by the Generator, labeled as fake. The goal is to maximize its accuracy in correctly classifying real and fake data. This is typically done using a binary cross-entropy loss function.\n",
    "\n",
    "2. **Training the Generator**: After updating the Discriminator, the Generator is trained. The key difference here is that the Generator's output is passed to the Discriminator, and the Generator is updated based on how well the Discriminator was able to distinguish its output from real data. The Generator's objective is to minimize the chance of the Discriminator correctly identifying its outputs as fake.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3dcf4b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation of Error During Training\n",
    "\n",
    "The backpropagation algorithm plays a crucial role in updating the weights of both the Generator and Discriminator networks. During the training phase, backpropagation is used to calculate the gradients of the loss function with respect to the network parameters, allowing for the optimization of these parameters.\n",
    "\n",
    "1. **For the Discriminator**, backpropagation calculates the gradient of the loss function that measures its ability to distinguish between real and fake data. The weights are then updated to improve its classification accuracy.\n",
    "\n",
    "2. **For the Generator**, the process is slightly more complex because its success is measured by the Discriminator's failure to recognize its outputs as fake. Thus, the gradient of the Discriminator's loss function with respect to the Generator's weights is computed, essentially \"tricking\" the Discriminator. The Generator's weights are updated to produce outputs that are more likely to be classified as real by the Discriminator.\n",
    "\n",
    "This iterative process of backpropagation and updating continues until a stopping criterion is met, which could be a certain number of iterations or when the Discriminator's accuracy reaches a specific threshold, indicating that the Generator produces sufficiently realistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141cfbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generative Vs Discriminative Models\n",
    "\n",
    "- If you have delved into neural networks, it's probable that the majority of the examples you encountered utilized ***discriminative*** models. Conversely, generative adversarial networks belong to a distinct category known as ***generative*** models.\n",
    "\n",
    "- Discriminative models are typically employed for most tasks involving supervised learning, whether for classification or regression. Consider a scenario where the objective is to develop a model capable of identifying handwritten digits ranging from 0 to 9. In this case, one would employ a dataset comprising images of handwritten digits, each tagged with a label that denotes the represented digit.\n",
    "\n",
    "- Throughout the model training phase, an algorithm would be applied to tweak the model's parameters. The aim here is to reduce a loss function to a minimum, enabling the model to grasp the ***probability distribution of the output based on the given input***. Once the training is completed, the model can then be used to identify new images of handwritten digits by predicting the digit that most likely matches the input, as depicted in the following illustration:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e39a69",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-03-01.jpg\" width=\"700\" height=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66fa9d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*image source: RealPython see References and Credits Section*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8f0a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can think of discriminative models for classification tasks as structures that leverage training data to identify the dividing lines between different classes. These models then apply these learned boundaries to distinguish an input and assign it to a specific category. \n",
    "\n",
    "- In terms of mathematics, discriminative models are adept at **learning the conditional probability $P(y \\vert x)$**, which is the probability of the output y given the input x.\n",
    "\n",
    "- Beyond neural networks, discriminative models can also take the form of logistic regression models and support vector machines (SVMs).\n",
    "\n",
    "- On the other side, generative models such as GANs are developed to model the way a dataset is produced through a probabilistic framework. By drawing samples from a generative model, it becomes possible to create new pieces of data. While discriminative models find their application in supervised learning, generative models are typically employed with datasets that do not have labels, representing a type of unsupervised learning.\n",
    "\n",
    "- For instance, using a dataset of handwritten digits, a generative model could be trained to produce new digit images. During its training, an algorithm would be utilized to refine the modelâ€™s parameters with the aim of reducing a loss function and capturing the probability distribution of the dataset. Following the training, this model would be capable of generating new digit samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a50e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-03-02.jpg\" width=\"700\" height=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5940a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*image source: RealPython see References and Credits Section*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30dab5f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Generative models typically incorporate a stochastic, or random, factor to produce new samples. \n",
    "\n",
    "- This randomness is essential for the variety in the samples created by the model. The random inputs fueling the generator come from what is known as a latent space, where the vectors serve as a condensed version of the samples that are generated.\n",
    "\n",
    "- In contrast to discriminative models, generative models are focused on learning the **probability $P(x)$** of the input data $x$. With this understanding of the input data's distribution, they have the capability to generate new instances of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db803a59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Simple Toy Model\n",
    "\n",
    "- As we have already said, generative adversarial networks consist of an overall structure composed of two neural networks, one called the generator and the other called the discriminator.\n",
    "\n",
    "- The role of the generator is to estimate the probability distribution of the real samples in order to provide generated samples resembling real data. The discriminator, in turn, is trained to estimate the probability that a given sample came from the real data rather than being provided by the generator.\n",
    "\n",
    "- These structures are called generative adversarial networks because the generator and discriminator are trained to compete with each other: the generator tries to get better at fooling the discriminator, while the discriminator tries to get better at identifying generated samples.\n",
    "\n",
    "- To understand how GAN training works, consider a toy example with a dataset composed of two-dimensional samples $(x_1, x_2)$, with $x_1$ in the interval from $0$ to $2\\pi$ and $x_2 = \\sin(x_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135774c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-03-03.jpg\" width=\"700\" height=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df90d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To train D, at each iteration you label some real samples taken from the training data as 1 and some generated samples provided by G as 0. This way, you can use a conventional supervised training framework to update the parameters of D in order to minimize a loss function, as shown in the following scheme:\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-03-04.jpg\" width=\"700\" height=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe8a30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For each batch of training data containing labeled real and generated samples, you update the parameters of D to minimize a loss function. After the parameters of D are updated, you train G to produce better generated samples. The output of G is connected to D, whose parameters are kept frozen, as depicted here:\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-03-05.jpg\" width=\"700\" height=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb0060",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that you know how GANs work, youâ€™re ready to implement your own using PyTorch..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ca29c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First of all as usual we have to import python libraries. The `torch` library and its neural network module `nn`, along with the `math` and `matplotlib.pyplot` libraries. `torch` is a popular deep learning library offering rich functionalities for tensor operations, automatic differentiation, and neural network design. The `nn` module within `torch` provides essential building blocks for creating neural networks, such as layers and activation functions. The `math` library is used for mathematical functions, and `matplotlib.pyplot` is a plotting library for visualizing data, which is useful for tasks like displaying the results of model training or data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87107bc1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded11667",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The code `torch.manual_seed(111)` sets the seed for generating random numbers in PyTorch to `111`. This ensures that the random numbers generated by PyTorch functions are deterministic and reproducible across runs of the program. Setting a manual seed is crucial for experiments where reproducibility is necessary, as it ensures that the random elements of the computations (such as weight initialization in neural networks) are consistent each time the code is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfabcf3a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c253dd5410>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39407844",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Preparing the Training Data**\n",
    "\n",
    "Now we are going to create a dataset for a PyTorch training session, where the objective is to learn the sine function from its input. The following code initializes a dataset with 1024 samples (`train_data_length`), where each sample has two dimensions. The first dimension is randomly generated angles (in radians), and the second dimension is the sine of these angles. In other words, the training data is composed of pairs $(x_1, x_2)$ so that $x_2$ consists of the value of the sine of $x_1$ for $x_1$ in the interval from $0$ to $2\\pi$. Labels for this dataset are initialized to zeros (`train_labels`), although they're not used further in this snippet. Finally, it creates a list of tuples (`train_set`), where each tuple contains a data point and its corresponding label, ready for use in training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a13c0e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Set the total number of data points in the training set\n",
    "train_data_length = 1024\n",
    "\n",
    "# Initialize a tensor of zeros with shape (1024, 2) to hold the training data\n",
    "train_data = torch.zeros((train_data_length, 2))\n",
    "\n",
    "# Populate the first column of train_data with random angles between 0 and 2Ï€\n",
    "train_data[:, 0] = 2 * math.pi * torch.rand(train_data_length)\n",
    "\n",
    "# Calculate the sine of the angles (first column) and store the results in the second column\n",
    "train_data[:, 1] = torch.sin(train_data[:, 0])\n",
    "\n",
    "# Initialize a tensor of zeros to serve as labels for the training data. Note that \n",
    "# the tensor of labels, is required by PyTorchâ€™s data loader. Since GANs make use \n",
    "# of unsupervised learning techniques, the labels can be anything. They wonâ€™t be used, after all.\n",
    "train_labels = torch.zeros(train_data_length)\n",
    "\n",
    "# Combine the training data and labels into a list of tuples, creating the final training dataset\n",
    "train_set = [\n",
    "    (train_data[i], train_labels[i]) for i in range(train_data_length)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5cd257",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You can examine the training data by plotting each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7bcc9c4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c25f34a1d0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_data[:, 0], train_data[:, 1], \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151f4b7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we initialize a DataLoader for the training data with a specified batch size of 32. The DataLoader is a PyTorch utility that provides an iterable over the given dataset (`train_set` in this case), allowing for easy access to batches of data. The `batch_size` parameter defines how many samples per batch to load, and `shuffle=True` ensures that the data is shuffled at every epoch, which helps in reducing model overfitting by providing a more generalized learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07bc80c7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b1dbb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    ">**Note**. *A PyTorch DataLoader is a powerful tool designed to simplify data manipulation during the training of neural networks. It allows you to efficiently manage datasets by automating data batching, shuffling, and distribution across multiple processes. DataLoaders are particularly useful when dealing with large datasets that cannot fit into memory, as they enable seamless and on-the-fly data loading and transformation, thereby optimizing the training process and improving model performance*. \n",
    "\n",
    ">*For a detailed understanding of PyTorch DataLoaders, the official PyTorch documentation is the best reference. It provides comprehensive insights into DataLoader functionalities, including examples and guidelines on how to customize data loading for different scenarios. This resource is invaluable for developers looking to leverage DataLoaders for efficient data handling in neural network training. Visit the [PyTorch DataLoader documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for more information*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da0ab2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Implementing the Discriminator**\n",
    "\n",
    "In PyTorch, to build neural network models, you create classes derived from `nn.Module`. This approach requires you to define a class for the discriminator, following principles of Object-Oriented Programming (OOP) in Python 3. The discriminator, designed to differentiate between real and generated data, accepts 2D inputs to output a probability indicating if the input is from the actual dataset. The following example illustrates constructing such a discriminator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23968216",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Discriminator class which is a subclass of nn.Module\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize the superclass nn.Module\n",
    "        super().__init__()\n",
    "        \n",
    "        # Construct the neural network model\n",
    "        self.model = nn.Sequential(\n",
    "            # First linear layer taking a 2D input and outputting 256 features\n",
    "            nn.Linear(2, 256),\n",
    "            # ReLU activation function to introduce non-linearity\n",
    "            nn.ReLU(),\n",
    "            # Dropout layer to prevent overfitting by randomly setting input units to 0 with a probability of 0.3\n",
    "            nn.Dropout(0.3),\n",
    "            # Second linear layer reducing the feature size from 256 to 128\n",
    "            nn.Linear(256, 128),\n",
    "            # ReLU activation function\n",
    "            nn.ReLU(),\n",
    "            # Another dropout layer for regularization\n",
    "            nn.Dropout(0.3),\n",
    "            # Third linear layer reducing the feature size from 128 to 64\n",
    "            nn.Linear(128, 64),\n",
    "            # ReLU activation function\n",
    "            nn.ReLU(),\n",
    "            # Final dropout layer\n",
    "            nn.Dropout(0.3),\n",
    "            # Final linear layer reducing the feature size from 64 to 1\n",
    "            nn.Linear(64, 1),\n",
    "            # Sigmoid activation function to output a probability between 0 and 1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    # Define the forward pass through the network\n",
    "    def forward(self, x):\n",
    "        # Pass the input x through the sequential model and return the output\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21639cd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For more detailed explanations and tutorials on how to use PyTorch's `nn.Module` to create models, you can refer to the [official PyTorch documentation](https://pytorch.org/docs/stable/nn.html). This resource provides comprehensive guidance on building neural networks with PyTorch, including various modules and functions available within the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b49753",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a91cc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Implementing the Generator**\n",
    "\n",
    "As we have explained, in Generative Adversarial Networks (GANs), the generator is a neural model designed to produce data similar to the training set, using inputs sampled from a latent space. In our example, this involves a model that takes two-dimensional random vectors as input and outputs data points aiming to mimic the real dataset. Like the discriminator's setup, creating a generator involves defining a Generator class that extends `nn.Module`, encapsulating the network's architecture. Subsequently, an instance of this Generator class is created for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f3c482",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Generator class which inherits from nn.Module\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize the superclass (nn.Module)\n",
    "        # Define the model architecture using a Sequential container\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16),  # First linear transformation from 2D input to 16D\n",
    "            nn.ReLU(),         # ReLU activation function for non-linearity\n",
    "            nn.Linear(16, 32), # Second linear transformation from 16D to 32D\n",
    "            nn.ReLU(),         # Another ReLU activation function\n",
    "            nn.Linear(32, 2),  # Final linear transformation from 32D back to 2D\n",
    "        )\n",
    "\n",
    "    # Define the forward pass method\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)  # Pass the input x through the sequential model\n",
    "        return output           # Return the model output\n",
    "\n",
    "# Instantiate the Generator class\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdf697",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Training the Models**\n",
    "\n",
    "Before training the models, you need to set up some parameters to use during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44dfc18",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# sets the learning rate (lr), which youâ€™ll use to adapt the network weights\n",
    "lr            = 0.001\n",
    "# sets the number of epochs (num_epochs), which defines how many repetitions \n",
    "# of training using the whole training set will be performed.\n",
    "num_epochs    = 1000\n",
    "#  assigns the variable loss_function to the binary cross-entropy function BCELoss(), \n",
    "# which is the loss function that youâ€™ll use to train the models.\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624a127",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    ">**REMIND** *Binary Cross Entropy (BCE) is a loss function used in binary classification tasks in neural networks. It measures the difference between the true labels and the predicted probabilities of the positive class. BCE encourages the model to output probabilities close to 0 or 1, penalizing predictions that are confident but wrong. It's particularly useful for models ending with a sigmoid activation function, optimizing the parameters to minimize the discrepancy between actual and predicted labels, thus guiding the training towards more accurate predictions*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd6be8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "These lines initialize optimizers for both the discriminator and generator models in a GAN setup using the Adam optimization algorithm. Each optimizer is configured to update the respective model's parameters with a learning rate defined by `lr`. The `discriminator.parameters()` and `generator.parameters()` methods collect all trainable parameters of each model, allowing the optimizer to adjust them during training to minimize the models' loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27b5b9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    ">**REMIND** *The Adam optimization algorithm is a method used to update the weights in neural networks more efficiently. It combines ideas from two other optimizations - RMSprop and Momentum - to calculate adaptive learning rates for each parameter. Adam is particularly useful for large datasets and high-dimensional spaces because of its efficient computation of gradient descent, making it faster and more precise in reaching the minimum loss. This makes Adam a popular choice for training deep learning models, especially when dealing with complex data and neural network architectures. For a deeper understanding of the Adam optimization algorithm and its practical applications, you might find the original paper by Diederik P. Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", to be a valuable resource. [Here](https://towardsdatascience.com/the-math-behind-adam-optimizer-c41407efe59b) you can find a deep dive into the math of this method. Additionally, the TensorFlow and PyTorch documentation provide excellent overviews and implementation details that are useful for applying Adam in deep learning projects*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f05cc1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "optimizer_generator     = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f71a4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, you need to implement a training loop for the Generative Adversarial Network, iterating through epochs to train both the discriminator and generator models. It processes batches of real and generated samples, updating model weights to minimize respective loss functions. The discriminator learns to differentiate real data from fake, while the generator aims to produce increasingly realistic samples. Losses are calculated using a specified loss function and backpropagated to update the models. Progress is logged at intervals, showing the discriminator's and generator's loss at the end of certain epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22a7bf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    ">**NOTE** *The `zero_grad()` function in PyTorch is used to reset the gradients of all model parameters to zero. It's typically called before performing backpropagation (`.backward()`) during training. This is necessary because gradients accumulate by default for each `.backward()` call to support training of recurrent neural networks. Resetting gradients ensures that the model updates are based only on the most recent loss computation and not affected by previous iterations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d3a1c94",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss D.: 0.5474756360054016\n",
      "Epoch: 0 Loss G.: 0.7210572957992554\n",
      "Epoch: 10 Loss D.: 0.6470173597335815\n",
      "Epoch: 10 Loss G.: 1.1324470043182373\n",
      "Epoch: 20 Loss D.: 0.6397479772567749\n",
      "Epoch: 20 Loss G.: 0.6925271153450012\n",
      "Epoch: 30 Loss D.: 0.6996584534645081\n",
      "Epoch: 30 Loss G.: 0.7217584848403931\n",
      "Epoch: 40 Loss D.: 0.6998773813247681\n",
      "Epoch: 40 Loss G.: 0.7146339416503906\n",
      "Epoch: 50 Loss D.: 0.750548779964447\n",
      "Epoch: 50 Loss G.: 0.5630319714546204\n",
      "Epoch: 60 Loss D.: 0.7442445755004883\n",
      "Epoch: 60 Loss G.: 0.6551475524902344\n",
      "Epoch: 70 Loss D.: 0.740602970123291\n",
      "Epoch: 70 Loss G.: 0.6492887139320374\n",
      "Epoch: 80 Loss D.: 0.6799754500389099\n",
      "Epoch: 80 Loss G.: 0.706553041934967\n",
      "Epoch: 90 Loss D.: 0.65231853723526\n",
      "Epoch: 90 Loss G.: 0.7631711363792419\n",
      "Epoch: 100 Loss D.: 0.6554206013679504\n",
      "Epoch: 100 Loss G.: 1.0435595512390137\n",
      "Epoch: 110 Loss D.: 0.7210283279418945\n",
      "Epoch: 110 Loss G.: 0.692363977432251\n",
      "Epoch: 120 Loss D.: 0.6150066256523132\n",
      "Epoch: 120 Loss G.: 0.7609152793884277\n",
      "Epoch: 130 Loss D.: 0.6476543545722961\n",
      "Epoch: 130 Loss G.: 0.8277788162231445\n",
      "Epoch: 140 Loss D.: 0.6817062497138977\n",
      "Epoch: 140 Loss G.: 0.6958695650100708\n",
      "Epoch: 150 Loss D.: 0.713088870048523\n",
      "Epoch: 150 Loss G.: 0.728378176689148\n",
      "Epoch: 160 Loss D.: 0.6423600316047668\n",
      "Epoch: 160 Loss G.: 0.7789769172668457\n",
      "Epoch: 170 Loss D.: 0.6814597845077515\n",
      "Epoch: 170 Loss G.: 0.8190695643424988\n",
      "Epoch: 180 Loss D.: 0.6458570957183838\n",
      "Epoch: 180 Loss G.: 0.9006453156471252\n",
      "Epoch: 190 Loss D.: 0.6712561249732971\n",
      "Epoch: 190 Loss G.: 0.7210622429847717\n",
      "Epoch: 200 Loss D.: 0.6786187291145325\n",
      "Epoch: 200 Loss G.: 0.6853533387184143\n",
      "Epoch: 210 Loss D.: 0.6836484670639038\n",
      "Epoch: 210 Loss G.: 0.6889458298683167\n",
      "Epoch: 220 Loss D.: 0.6875288486480713\n",
      "Epoch: 220 Loss G.: 0.7742190361022949\n",
      "Epoch: 230 Loss D.: 0.7138185501098633\n",
      "Epoch: 230 Loss G.: 0.6620906591415405\n",
      "Epoch: 240 Loss D.: 0.7368113398551941\n",
      "Epoch: 240 Loss G.: 0.6687039136886597\n",
      "Epoch: 250 Loss D.: 0.7224494218826294\n",
      "Epoch: 250 Loss G.: 0.7078099846839905\n",
      "Epoch: 260 Loss D.: 0.6887062191963196\n",
      "Epoch: 260 Loss G.: 0.7319473028182983\n",
      "Epoch: 270 Loss D.: 0.6934670805931091\n",
      "Epoch: 270 Loss G.: 0.6670147776603699\n",
      "Epoch: 280 Loss D.: 0.6795879602432251\n",
      "Epoch: 280 Loss G.: 0.719042181968689\n",
      "Epoch: 290 Loss D.: 0.7574735879898071\n",
      "Epoch: 290 Loss G.: 0.7059986591339111\n",
      "Epoch: 300 Loss D.: 0.6264572739601135\n",
      "Epoch: 300 Loss G.: 0.9099782705307007\n",
      "Epoch: 310 Loss D.: 0.6048278212547302\n",
      "Epoch: 310 Loss G.: 0.9384064674377441\n",
      "Epoch: 320 Loss D.: 0.6632850766181946\n",
      "Epoch: 320 Loss G.: 0.7649537324905396\n",
      "Epoch: 330 Loss D.: 0.7789669632911682\n",
      "Epoch: 330 Loss G.: 0.6689742803573608\n",
      "Epoch: 340 Loss D.: 0.6439772248268127\n",
      "Epoch: 340 Loss G.: 0.8051192164421082\n",
      "Epoch: 350 Loss D.: 0.6886505484580994\n",
      "Epoch: 350 Loss G.: 0.8567400574684143\n",
      "Epoch: 360 Loss D.: 0.6618316769599915\n",
      "Epoch: 360 Loss G.: 0.8599533438682556\n",
      "Epoch: 370 Loss D.: 0.6827583312988281\n",
      "Epoch: 370 Loss G.: 0.8688250184059143\n",
      "Epoch: 380 Loss D.: 0.6527978777885437\n",
      "Epoch: 380 Loss G.: 0.9663605093955994\n",
      "Epoch: 390 Loss D.: 0.6983525156974792\n",
      "Epoch: 390 Loss G.: 0.6767913699150085\n",
      "Epoch: 400 Loss D.: 0.6778525114059448\n",
      "Epoch: 400 Loss G.: 0.8085935711860657\n",
      "Epoch: 410 Loss D.: 0.6366009712219238\n",
      "Epoch: 410 Loss G.: 0.7931649684906006\n",
      "Epoch: 420 Loss D.: 0.6368823647499084\n",
      "Epoch: 420 Loss G.: 0.7693437337875366\n",
      "Epoch: 430 Loss D.: 0.6383600831031799\n",
      "Epoch: 430 Loss G.: 0.7651591300964355\n",
      "Epoch: 440 Loss D.: 0.6228747963905334\n",
      "Epoch: 440 Loss G.: 1.0066814422607422\n",
      "Epoch: 450 Loss D.: 0.7083519697189331\n",
      "Epoch: 450 Loss G.: 0.7116200923919678\n",
      "Epoch: 460 Loss D.: 0.6899852752685547\n",
      "Epoch: 460 Loss G.: 0.6996151804924011\n",
      "Epoch: 470 Loss D.: 0.68932044506073\n",
      "Epoch: 470 Loss G.: 0.7198930978775024\n",
      "Epoch: 480 Loss D.: 0.6351802349090576\n",
      "Epoch: 480 Loss G.: 0.9696967005729675\n",
      "Epoch: 490 Loss D.: 0.5724969506263733\n",
      "Epoch: 490 Loss G.: 0.8254655599594116\n",
      "Epoch: 500 Loss D.: 0.6954020857810974\n",
      "Epoch: 500 Loss G.: 0.7152976393699646\n",
      "Epoch: 510 Loss D.: 0.6942195892333984\n",
      "Epoch: 510 Loss G.: 0.7315757274627686\n",
      "Epoch: 520 Loss D.: 0.6829089522361755\n",
      "Epoch: 520 Loss G.: 0.714316189289093\n",
      "Epoch: 530 Loss D.: 0.7086907625198364\n",
      "Epoch: 530 Loss G.: 0.6977155804634094\n",
      "Epoch: 540 Loss D.: 0.6278496980667114\n",
      "Epoch: 540 Loss G.: 0.7475690841674805\n",
      "Epoch: 550 Loss D.: 0.6769739985466003\n",
      "Epoch: 550 Loss G.: 0.7544028162956238\n",
      "Epoch: 560 Loss D.: 0.7052686214447021\n",
      "Epoch: 560 Loss G.: 0.6834628582000732\n",
      "Epoch: 570 Loss D.: 0.6179627180099487\n",
      "Epoch: 570 Loss G.: 0.7447846531867981\n",
      "Epoch: 580 Loss D.: 0.5590733885765076\n",
      "Epoch: 580 Loss G.: 1.110215187072754\n",
      "Epoch: 590 Loss D.: 0.5950443148612976\n",
      "Epoch: 590 Loss G.: 1.003898024559021\n",
      "Epoch: 600 Loss D.: 0.686699390411377\n",
      "Epoch: 600 Loss G.: 0.7325402498245239\n",
      "Epoch: 610 Loss D.: 0.6787427663803101\n",
      "Epoch: 610 Loss G.: 0.7159377932548523\n",
      "Epoch: 620 Loss D.: 0.6245059370994568\n",
      "Epoch: 620 Loss G.: 0.9174848198890686\n",
      "Epoch: 630 Loss D.: 0.6549631357192993\n",
      "Epoch: 630 Loss G.: 0.748214602470398\n",
      "Epoch: 640 Loss D.: 0.6608362793922424\n",
      "Epoch: 640 Loss G.: 0.7461031079292297\n",
      "Epoch: 650 Loss D.: 0.6784706711769104\n",
      "Epoch: 650 Loss G.: 0.6950779557228088\n",
      "Epoch: 660 Loss D.: 0.7083858847618103\n",
      "Epoch: 660 Loss G.: 0.6573154330253601\n",
      "Epoch: 670 Loss D.: 0.7209352850914001\n",
      "Epoch: 670 Loss G.: 0.7230735421180725\n",
      "Epoch: 680 Loss D.: 0.6473193168640137\n",
      "Epoch: 680 Loss G.: 0.8708541393280029\n",
      "Epoch: 690 Loss D.: 0.6268609762191772\n",
      "Epoch: 690 Loss G.: 1.315911889076233\n",
      "Epoch: 700 Loss D.: 0.7049204707145691\n",
      "Epoch: 700 Loss G.: 0.806608259677887\n",
      "Epoch: 710 Loss D.: 0.687905490398407\n",
      "Epoch: 710 Loss G.: 0.7335232496261597\n",
      "Epoch: 720 Loss D.: 0.6996482014656067\n",
      "Epoch: 720 Loss G.: 0.7340841889381409\n",
      "Epoch: 730 Loss D.: 0.6808397769927979\n",
      "Epoch: 730 Loss G.: 0.7193813920021057\n",
      "Epoch: 740 Loss D.: 0.6342591047286987\n",
      "Epoch: 740 Loss G.: 0.9082876443862915\n",
      "Epoch: 750 Loss D.: 0.7216358184814453\n",
      "Epoch: 750 Loss G.: 0.6580764055252075\n",
      "Epoch: 760 Loss D.: 0.6847379803657532\n",
      "Epoch: 760 Loss G.: 0.6965480446815491\n",
      "Epoch: 770 Loss D.: 0.6841510534286499\n",
      "Epoch: 770 Loss G.: 0.7302783727645874\n",
      "Epoch: 780 Loss D.: 0.6997148990631104\n",
      "Epoch: 780 Loss G.: 0.71905916929245\n",
      "Epoch: 790 Loss D.: 0.6999216079711914\n",
      "Epoch: 790 Loss G.: 0.7137193083763123\n",
      "Epoch: 800 Loss D.: 0.774779736995697\n",
      "Epoch: 800 Loss G.: 0.6894002556800842\n",
      "Epoch: 810 Loss D.: 0.6976791620254517\n",
      "Epoch: 810 Loss G.: 0.7015202045440674\n",
      "Epoch: 820 Loss D.: 0.6963527202606201\n",
      "Epoch: 820 Loss G.: 0.6937397718429565\n",
      "Epoch: 830 Loss D.: 0.688630998134613\n",
      "Epoch: 830 Loss G.: 0.699382483959198\n",
      "Epoch: 840 Loss D.: 0.6904332637786865\n",
      "Epoch: 840 Loss G.: 0.6948557496070862\n",
      "Epoch: 850 Loss D.: 0.7071725726127625\n",
      "Epoch: 850 Loss G.: 0.7818828225135803\n",
      "Epoch: 860 Loss D.: 0.6870396733283997\n",
      "Epoch: 860 Loss G.: 0.7104583978652954\n",
      "Epoch: 870 Loss D.: 0.6858582496643066\n",
      "Epoch: 870 Loss G.: 0.6920899748802185\n",
      "Epoch: 880 Loss D.: 0.6889554262161255\n",
      "Epoch: 880 Loss G.: 0.6956188082695007\n",
      "Epoch: 890 Loss D.: 0.7014852166175842\n",
      "Epoch: 890 Loss G.: 0.6841786503791809\n",
      "Epoch: 900 Loss D.: 0.6931365132331848\n",
      "Epoch: 900 Loss G.: 0.702639639377594\n",
      "Epoch: 910 Loss D.: 0.660254716873169\n",
      "Epoch: 910 Loss G.: 0.784682035446167\n",
      "Epoch: 920 Loss D.: 0.6977882385253906\n",
      "Epoch: 920 Loss G.: 0.6782868504524231\n",
      "Epoch: 930 Loss D.: 0.689402163028717\n",
      "Epoch: 930 Loss G.: 0.7415420413017273\n",
      "Epoch: 940 Loss D.: 0.6876204013824463\n",
      "Epoch: 940 Loss G.: 0.6889052987098694\n",
      "Epoch: 950 Loss D.: 0.69117271900177\n",
      "Epoch: 950 Loss G.: 0.6881434321403503\n",
      "Epoch: 960 Loss D.: 0.69512939453125\n",
      "Epoch: 960 Loss G.: 0.6866259574890137\n",
      "Epoch: 970 Loss D.: 0.6744028329849243\n",
      "Epoch: 970 Loss G.: 0.7617506980895996\n",
      "Epoch: 980 Loss D.: 0.6897830963134766\n",
      "Epoch: 980 Loss G.: 0.8024033904075623\n",
      "Epoch: 990 Loss D.: 0.7036836743354797\n",
      "Epoch: 990 Loss G.: 0.7578887343406677\n"
     ]
    }
   ],
   "source": [
    "latent_space_samples_for_test = torch.randn(100, 2)\n",
    "# As is generally done for all neural networks, the training process consists of two loops, \n",
    "# one for the training epochs and the other for the batches for each epoch.\n",
    "# Loop through each epoch.\n",
    "for epoch in range(num_epochs):\n",
    "    # Enumerate over the training loader to get batches of real samples.\n",
    "    for n, (real_samples, _) in enumerate(train_loader):\n",
    "        # This instruction creates a tensor filled with ones, having a shape defined by (batch_size, 1). \n",
    "        # This tensor is used to label the real samples which are typically labeled with 1s, while generated (fake) \n",
    "        # samples are labeled with 0s. Here, batch_size determines the number of samples per batch.\n",
    "        # Notice that the first dimension of the tensor has the number of elements equal to batch_size. \n",
    "        # This is the standard way of organizing data in PyTorch, with each line of the tensor representing \n",
    "        # one sample from the batch.\n",
    "        real_samples_labels = torch.ones((batch_size, 1))\n",
    "        # Generate random points in the latent space.\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "        # Generate fake samples from the latent space samples.\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        # Assign labels of 0 to generated (fake) samples.\n",
    "        generated_samples_labels = torch.zeros((batch_size, 1))\n",
    "        # Concatenate real and generated samples to form a mixed dataset.\n",
    "        all_samples = torch.cat((real_samples, generated_samples))\n",
    "        all_samples_labels = torch.cat(\n",
    "            (real_samples_labels, generated_samples_labels)\n",
    "        )\n",
    "        #\n",
    "        # -----> Discriminator Training\n",
    "        #\n",
    "        # Zero the gradients of the discriminator (see note).\n",
    "        discriminator.zero_grad()\n",
    "        # Pass the mixed dataset through the discriminator.\n",
    "        output_discriminator = discriminator(all_samples)\n",
    "        # Calculate the loss for the discriminator.\n",
    "        loss_discriminator = loss_function(\n",
    "            output_discriminator, all_samples_labels)\n",
    "        # Backpropagate the loss.\n",
    "        loss_discriminator.backward()\n",
    "        # Update the discriminator's weights.\n",
    "        optimizer_discriminator.step()\n",
    "        #\n",
    "        # -----> Generator Training\n",
    "        #\n",
    "        # Prepare new latent space samples for training the generator. You store random data in \n",
    "        # latent_space_samples, with a number of lines equal to batch_size. You use two columns \n",
    "        # since youâ€™re providing two-dimensional data as input to the generator.\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "        # Zero the gradients of the generator (see note).\n",
    "        generator.zero_grad()\n",
    "        # Generate fake samples using the generator.\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        # Pass the generated samples through the discriminator.\n",
    "        output_discriminator_generated = discriminator(generated_samples)\n",
    "        # Calculate the generator's loss.\n",
    "        loss_generator = loss_function(\n",
    "            output_discriminator_generated, real_samples_labels\n",
    "        )\n",
    "        # Backpropagate the generator's loss.\n",
    "        loss_generator.backward()\n",
    "        # Update the generator's weights.\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        \n",
    "        # Optionally print the losses every 10 epochs.\n",
    "        if epoch % 10 == 0 and n == batch_size - 1:\n",
    "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n",
    "            fig = plt.figure()    \n",
    "            generated_samples_for_test = torch.zeros((batch_size, 2))\n",
    "            generated_samples_for_test = generator(latent_space_samples_for_test)\n",
    "            generated_samples_for_test = generated_samples_for_test.detach()\n",
    "            plt.plot(generated_samples_for_test[:, 0], generated_samples_for_test[:, 1], \".\")\n",
    "            plt.savefig('./pics/gan_' + str(epoch).zfill(3))\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcdf35",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Checking the Samples Generated by the GAN**\n",
    "\n",
    "Generative adversarial networks are designed to generate data. So, after the training process is finished, you can get some random samples from the latent space and feed them to the generator to obtain some generated samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7278789",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "latent_space_samples = torch.randn(100, 2)\n",
    "generated_samples    = generator(latent_space_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eda06f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then you can plot the generated samples and check if they resemble the training data. Before plotting the generated_samples data, youâ€™ll need to use `.detach()` to return a tensor from the PyTorch computational graph. The `detach()` method in PyTorch is like telling PyTorch to not worry about remembering the steps you took to create it. When training models, PyTorch keeps track of how you arrived at a result so it can learn from it. Sometimes, though, you just want to look at the result without changing the learning. Using `detach()` is like saying, \"This part is just for showing, no need to learn from it.\" It helps when you're checking your work without affecting the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be342d5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a4aef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "# Assuming you have images stored in 'path/to/images' directory\n",
    "image_files = [os.path.join('./pics', img) for img in sorted(os.listdir('./pics')) if img[:4]=='gan_']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def animate(i):\n",
    "    img = plt.imread(image_files[i])\n",
    "    ax.clear()  # Clear the previous image\n",
    "    ax.imshow(img)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(image_files), interval=200)\n",
    "\n",
    "# Display the animation in Jupyter\n",
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce1ebc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference and Credits\n",
    "\n",
    "[1] - Renato Candido, [\"Generative Adversarial Networks: Build Your First Models\"](https://realpython.com/generative-adversarial-networks/), RealPython"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "8",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
