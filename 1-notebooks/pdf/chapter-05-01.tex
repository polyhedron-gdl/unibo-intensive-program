\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{chapter-05-01}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Advanced Clustering
Techniques}\label{advanced-clustering-techniques}

    \subsection{Spectral Clustering}\label{spectral-clustering}

    \subsubsection{Introduction}\label{introduction}

Spectral clustering is a technique used in machine learning to group
data into clusters that may not be linearly separable. It works by using
the eigenvalues (spectra) of a similarity matrix of the data to perform
dimensionality reduction before clustering in fewer dimensions. The
method is particularly effective for identifying clusters that are
connected through a graph structure but are not necessarily compact or
evenly distributed in space.

Here's a step-by-step breakdown of how spectral clustering works:

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Similarity Matrix Creation}: The first step involves creating
  a similarity matrix that represents how similar each pair of points in
  the dataset is to each other. This is often done using the Gaussian
  (RBF) kernel to convert the Euclidean distances between data points
  into similarity scores.
\end{enumerate}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Graph Representation}: The similarity matrix is then used to
  represent the data as a graph, with data points as nodes and the
  similarities as weights on the edges between nodes.
\end{enumerate}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Laplacian Matrix}: From this graph, a Laplacian matrix is
  computed. The Laplacian is a matrix representation that captures the
  structure of the graph. It is used to find the minimum number of cuts
  needed to partition the graph into clusters.
\end{enumerate}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Eigenvalue Decomposition}: The next step involves computing
  the eigenvalues and eigenvectors of the Laplacian matrix. The
  eigenvectors corresponding to the smallest eigenvalues (except for the
  smallest one, which is always zero) are used. These eigenvectors are
  then stacked to form a new dataset with reduced dimensions.
\end{enumerate}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Clustering}: The reduced dataset is then clustered using a
  conventional algorithm like k-means. Since the data is now in a space
  where clusters are more distinguishable, k-means or similar algorithms
  can effectively identify the clusters.
\end{enumerate}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Assignment Back to Original Data}: Finally, points are
  assigned to clusters based on their membership in the reduced
  dimensional space.
\end{enumerate}

Let's see in more details each point.

    \subsubsection{What is a Similarity
Matrix?}\label{what-is-a-similarity-matrix}

The first step in spectral clustering, ``Similarity Matrix Creation,''
is crucial as it lays the foundation for the entire clustering process.
Let's delve into the details of this step.

A \textbf{similarity matrix}, sometimes also referred to as an affinity
matrix, is a \emph{square matrix} used to represent the similarity
between each pair of points in a dataset.

In the context of spectral clustering, the dataset is typically
comprised of multidimensional points, and \textbf{the goal is to measure
how close or similar these points are to one another}.

The similarity matrix is \textbf{symmetric}, with its diagonal elements
representing the similarity of each point with itself (usually the
maximum similarity score) and off-diagonal elements representing the
similarity between different points.

    \textbf{How is the Similarity Matrix Created?}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Calculating Pairwise Distances}: The process begins by
  calculating the pairwise distances between all points in the dataset.
  This is often done using Euclidean distance, which measures the
  straight-line distance between two points in Euclidean space. However,
  other distance metrics can also be used depending on the nature of the
  data and the specific requirements of the clustering task.
\item
  \textbf{Converting Distances to Similarities}: Directly using
  distances as measures of similarity is not intuitive because in a
  similarity context, we expect similar points to have a high score.
  Hence, the distances need to be converted into similarity scores. This
  conversion is typically achieved using a similarity function, such as
  the Gaussian (Radial Basis Function, RBF) kernel.
\item
  \textbf{Using the Gaussian (RBF) Kernel}: The Gaussian kernel is a
  popular choice for transforming Euclidean distances into similarity
  scores. It is defined as follows:

  \[ S(x, y) = \exp\left(-\frac{\|x - y\|^2}{2\sigma^2}\right) \]
\end{enumerate}

    Here, \(S(x, y)\) is the similarity between points \(x\) and \(y\),
\(\|x - y\|\) is the Euclidean distance between \(x\) and \(y\), and
\(\sigma\) is a parameter that controls the width of the neighborhood or
the spread of the Gaussian kernel. The effect of this transformation is
that points closer to each other (smaller distances) result in higher
similarity scores (closer to 1), while points further apart have lower
scores (closer to 0).

    \textbf{Key Considerations}

\begin{itemize}
\item
  \textbf{Choice of \(\sigma\)}: The parameter \(\sigma\) plays a
  crucial role in determining the scale of similarity. A small
  \(\sigma\) makes the similarity drop off quickly with distance,
  leading to a sparser similarity matrix where only very close points
  are considered similar. A larger \(\sigma\) makes the similarity drop
  off more slowly, considering a broader range of points as similar.
  Choosing an appropriate \(\sigma\) is essential for the success of the
  clustering.
\item
  \textbf{Sparsity of the Matrix}: In practice, for large datasets, the
  similarity matrix can be made sparse by setting all similarity scores
  below a certain threshold to zero. This reduces the computational
  complexity and focuses on stronger relationships, assuming that very
  distant points (low similarity) do not contribute significantly to the
  clustering structure.
\end{itemize}

The similarity matrix creation is a foundational step in spectral
clustering that impacts the quality and characteristics of the resulting
clusters. By carefully transforming distances into similarities, this
step effectively captures the underlying structure of the data, setting
the stage for identifying clusters based on the global relationships
among data points.

    \textbf{Example}

For example, assume that we have the following dataset of 10
two-dimensional points:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}moons}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k+kn}{import} \PY{n}{SpectralClustering}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} This array is constructed using the np.array() function from the NumPy library,}
\PY{c+c1}{\PYZsh{} with a shape of 10 rows and 2 columns}
\PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,} 
              \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
(12, 2)
    \end{Verbatim}

    Let's plot the dataset:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
\end{Verbatim}
\end{tcolorbox}

    \begin{quote}
\textbf{NOTE} Seaborn is a Python visualization library based on
matplotlib that provides a high-level interface for drawing attractive
and informative statistical graphics. It is designed to make
visualization a central part of exploring and understanding data.
Seaborn is built on top of matplotlib and integrates closely with pandas
data structures, offering a more user-friendly and aesthetically
pleasing interface compared to matplotlib. Seaborn does not replace
matplotlib but rather complements it. While Seaborn simplifies many
plotting tasks, matplotlib remains powerful for customizing plots down
to the finest detail. Users often start with Seaborn for quick and
attractive visualizations and then use matplotlib for fine-tuning.
\end{quote}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define a function named plot\PYZus{}data that takes a single argument, X, which is expected to be a 2D array or list.}
\PY{k}{def} \PY{n+nf}{plot\PYZus{}data}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Create a new figure with a specified size of 4x4 inches for plotting.}
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Use seaborn\PYZsq{}s scatterplot function to plot the data points from X. }
    \PY{c+c1}{\PYZsh{} X[:, 0] selects all rows from the first column of X (x\PYZhy{}axis values),}
    \PY{c+c1}{\PYZsh{} and X[:, 1] selects all rows from the second column of X (y\PYZhy{}axis values).}
    \PY{c+c1}{\PYZsh{} \PYZsq{}edgecolor\PYZsq{} sets the color of the edges of each point to black (\PYZsq{}k\PYZsq{}),}
    \PY{c+c1}{\PYZsh{} \PYZsq{}s\PYZsq{} sets the size of the points to 20, and \PYZsq{}legend=False\PYZsq{} omits the legend from the plot.}
    \PY{n}{sns}\PY{o}{.}\PY{n}{scatterplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{edgecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}    
    
    \PY{c+c1}{\PYZsh{} Label the x\PYZhy{}axis with the text \PYZsq{}\PYZdl{}x\PYZus{}1\PYZdl{}\PYZsq{}. The dollar signs are used to denote LaTeX formatting, emphasizing mathematical notation.}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZus{}1\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Label the y\PYZhy{}axis with the text \PYZsq{}\PYZdl{}x\PYZus{}2\PYZdl{}\PYZsq{}, also using LaTeX formatting for consistency.}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZus{}2\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}    
    \PY{c+c1}{\PYZsh{} Add a grid to the plot for better readability, with the transparency (alpha) set to 0.5.}
    \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Call the plot\PYZus{}data function, passing the previously defined array X as an argument.}
\PY{c+c1}{\PYZsh{} This will execute the function, resulting in the generation and display of the scatter plot.}
\PY{n}{plot\PYZus{}data}\PY{p}{(}\PY{n}{X}\PY{p}{)}     
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We now build a fully-connected similarity graph from this dataset, using
the RBF similarity function. The adjacency matrix of this graph can be
easily computed using the following Python function:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define the function similarity\PYZus{}graph with parameters X (the dataset) and sigma (the Gaussian kernel width parameter).}
\PY{k}{def} \PY{n+nf}{similarity\PYZus{}graph}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{sigma}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Initialize a square matrix W of zeros with dimensions equal to the number of data points in X.}
    \PY{c+c1}{\PYZsh{} This matrix will store the computed similarities between each pair of data points.}
    \PY{n}{W} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Iterate over all rows of X using index i to access each data point as the reference point.}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Iterate over all rows of X again using index j to compare each data point with the reference point i.}
        \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Ensure that we are not comparing a data point with itself.}
            \PY{k}{if} \PY{n}{i} \PY{o}{!=} \PY{n}{j}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Compute the Gaussian similarity between data points X[i] and X[j].}
                \PY{c+c1}{\PYZsh{} This is done by taking the Euclidean distance between the points, squaring it,}
                \PY{c+c1}{\PYZsh{} dividing by twice the square of sigma, and applying the exponential function.}
                \PY{c+c1}{\PYZsh{} The result is a similarity score that is high for close points and low for distant points.}
                \PY{n}{W}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{X}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{sigma}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Return the computed similarity matrix W.}
    \PY{k}{return} \PY{n}{W}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{W} \PY{o}{=} \PY{n}{similarity\PYZus{}graph}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{W}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[0.    0.574 0.027 0.757 0.108 0.04  0.056 0.2   0.082 0.053 0.108 0.895]
 [0.574 0.    0.249 0.757 0.574 0.329 0.368 0.389 0.249 0.128 0.236 0.801]
 [0.027 0.249 0.    0.062 0.757 0.757 0.486 0.056 0.062 0.018 0.034 0.066]
 [0.757 0.757 0.062 0.    0.249 0.128 0.2   0.574 0.329 0.236 0.389 0.946]
 [0.108 0.574 0.757 0.249 0.    0.895 0.801 0.249 0.249 0.103 0.169 0.236]
 [0.04  0.329 0.757 0.128 0.895 0.    0.895 0.2   0.249 0.103 0.151 0.108]
 [0.056 0.368 0.486 0.2   0.801 0.895 0.    0.389 0.486 0.249 0.329 0.151]
 [0.2   0.389 0.056 0.574 0.249 0.2   0.389 0.    0.895 0.801 0.946 0.389]
 [0.082 0.249 0.062 0.329 0.249 0.249 0.486 0.895 0.    0.895 0.946 0.2  ]
 [0.053 0.128 0.018 0.236 0.103 0.103 0.249 0.801 0.895 0.    0.946 0.128]
 [0.108 0.236 0.034 0.389 0.169 0.151 0.329 0.946 0.946 0.946 0.    0.236]
 [0.895 0.801 0.066 0.946 0.236 0.108 0.151 0.389 0.2   0.128 0.236 0.   ]]
    \end{Verbatim}

    \subsubsection{Graph Representation in Spectral
Clustering}\label{graph-representation-in-spectral-clustering}

After creating the similarity matrix, which quantifies the similarity
between every pair of data points in the dataset, the next step is to
\textbf{interpret this matrix as a graph}.

In this context, a graph is a mathematical structure used to model
pairwise relations between objects. This step is crucial for
understanding the data's structure from a new perspective, enabling the
application of graph theory concepts to identify clusters.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Nodes (Vertices)}: Each data point in the dataset is
  represented as a node (or vertex) in the graph. If you have \(N\) data
  points, the graph will have \(N\) nodes.
\item
  \textbf{Edges}: Connections between nodes are called edges. In the
  context of spectral clustering, an edge is drawn between every pair of
  nodes. The existence of an edge between two nodes indicates that there
  is some level of similarity between the corresponding data points.
\item
  \textbf{Weights}: Each edge in the graph is assigned a weight that
  quantifies the strength of the connection (or similarity) between the
  two nodes it connects. These weights are directly taken from the
  similarity matrix. A higher weight indicates a stronger similarity
  between the nodes. In the case of using the Gaussian (RBF) kernel for
  generating the similarity matrix, the weights would reflect the
  Gaussian similarity between data points, with closer points having
  higher weights.
\end{enumerate}

    

    \textbf{Understanding the Constructed Graph}

\begin{itemize}
\item
  \textbf{Type of Graph}: The resulting graph is typically
  \textbf{undirected} since the similarity between two points is mutual.
  It is also \textbf{fully connected} because the similarity matrix
  provides a similarity score for every pair of points, implying an edge
  between every pair of nodes.
\item
  \textbf{Sparsity and Thresholding}: Although the initial
  representation suggests a fully connected graph, in practice, the
  graph can be made sparse by removing edges with weights below a
  certain threshold, keeping only the most significant connections. This
  sparsification helps in reducing computational complexity and focusing
  on the most meaningful relationships in the data.
\item
  \textbf{Interpretation}: The graph effectively models the dataset's
  structure, with \textbf{\emph{clusters in the data expected to form
  tightly connected subgraphs}}. In other words, \textbf{\emph{points
  within the same cluster are highly interconnected with strong edges
  (high weights), while points from different clusters have weaker
  connections (low weights or no edge at all)}}.
\end{itemize}

    \textbf{Role in Spectral Clustering}

This graph representation is fundamental to spectral clustering because
it allows the application of graph-theoretic algorithms to detect
communities or clusters.

The intuition is that cutting the graph into subgraphs (clusters) can be
done in such a way that the connections (edges) between different
clusters are weak (low similarity), while connections within a cluster
are strong (high similarity).

In the subsequent steps of spectral clustering, the graph's structure is
analyzed using eigendecomposition of the Laplacian matrix derived from
the graph, which ultimately leads to the identification of clusters
based on the spectral properties of the graph.

This graph-based approach enables spectral clustering to identify
clusters that may not be linearly separable and might have complex
shapes, leveraging the topology of the data represented by the graph.

    \subsubsection{What is the Laplacian
Matrix?}\label{what-is-the-laplacian-matrix}

Step 3 of the spectral clustering process involves the computation of
the Laplacian matrix from the graph constructed in Step 2. The Laplacian
matrix is a crucial concept in graph theory and spectral clustering, in
a nutshell it is a matrix representation that captures the graph's
structure, focusing on how nodes (data points) are connected to each
other via edges (similarities). It plays a pivotal role in understanding
the graph's topology and is instrumental in identifying clusters within
the graph.

    \textbf{How is the Laplacian Matrix Computed?}

Given a graph \(G\) with \(N\) nodes, represented by its weighted
adjacency matrix \(W\) (the similarity matrix from Step 1), and a degree
matrix \(D\) (a diagonal matrix where each diagonal element \(D_{ii}\)
represents the sum of the weights of all edges connected to node \(i\)),
the Laplacian matrix \(L\) is computed as follows:

\[ L = D - W \]

Here's what each component represents:

\begin{itemize}
\item
  \textbf{Weighted Adjacency Matrix (\(W\))}: This matrix represents the
  graph, where each element \(W_{ij}\) is the weight of the edge between
  nodes \(i\) and \(j\). If \(i\) and \(j\) are not directly connected,
  \(W_{ij} = 0\).
\item
  \textbf{Degree Matrix (\(D\))}: This is a diagonal matrix where each
  entry \(D_{ii}\) is the degree of node \(i\), which, in the context of
  weighted graphs, is the sum of the weights of all edges connected to
  node \(i\). In mathematical terms, \(D_{ii} = \sum_{j} W_{ij}\).
\end{itemize}

    \textbf{Variants of the Laplacian Matrix}

There are primarily two variants of the Laplacian matrix used in
spectral clustering:

\begin{itemize}
\item
  \textbf{Unnormalized Laplacian (\(L\))}: As defined above,
  \[L = D - W\].
\item
  \textbf{Normalized Laplacian}: There are two forms of the normalized
  Laplacian:

  \textbf{Symmetric Normalized Laplacian}:
  \[L_{sym} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} W D^{-1/2}\]
  \textbf{Random Walk Normalized Laplacian}:
  \[L_{rw} = D^{-1} L = I - D^{-1} \]
\end{itemize}

In these formulas, \(I\) denotes the identity matrix.

    \textbf{Role and Significance}

\begin{itemize}
\item
  \textbf{Capturing Graph Structure}: The Laplacian matrix highlights
  the differences in connectivity within the graph. It effectively
  captures how each node differs from its neighbors in terms of
  connectivity, which is essential for identifying separations or cuts
  in the graph.
\item
  \textbf{Spectral Clustering}: The eigenvalues and eigenvectors of the
  Laplacian matrix reveal critical information about the graph's cluster
  structure. Specifically, the number of zero eigenvalues of \(L\)
  corresponds to the number of connected components in the graph. The
  eigenvectors associated with the smallest non-zero eigenvalues (known
  as Fiedler values) provide the means to partition the graph. By
  analyzing these eigenvectors, spectral clustering algorithms can
  identify the minimum number of cuts needed to separate the graph into
  disjoint clusters, ensuring that the separation minimizes the
  similarity between different clusters while maximizing the similarity
  within clusters.
\item
  \textbf{Partitioning}: The process of using the Laplacian's
  eigenvectors to partition the graph is based on the insight that these
  eigenvectors can serve as coordinates in a new space where clustering
  algorithms like k-means can be effectively applied to identify
  clusters.
\end{itemize}

    The Laplacian matrix is a powerful tool in spectral clustering, bridging
the gap between graph theory and clustering by leveraging the graph's
spectral properties to discover inherent cluster structures in the data.

    \textbf{Example}

Let's compute the Degree Matrix:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{degree\PYZus{}matrix}\PY{p}{(}\PY{n}{W}\PY{p}{)}\PY{p}{:}
    \PY{n}{D} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{W}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{W}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{D}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{W}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
    \PY{k}{return} \PY{n}{D}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{D} \PY{o}{=} \PY{n}{degree\PYZus{}matrix}\PY{p}{(}\PY{n}{W}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{D}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[2.9   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    4.654 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    2.575 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    4.628 0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    4.391 0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    3.855 0.    0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    4.409 0.    0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    5.086 0.    0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    4.642 0.    0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    3.659 0.    0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    4.49  0.   ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    4.155]]
    \end{Verbatim}

    We can now derive the graph Laplacian matrix, which is defined as the
difference between the degree matrix and the adjacency matrix:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{L} \PY{o}{=} \PY{n}{D} \PY{o}{\PYZhy{}} \PY{n}{W}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{L}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[ 2.9   -0.574 -0.027 -0.757 -0.108 -0.04  -0.056 -0.2   -0.082 -0.053
  -0.108 -0.895]
 [-0.574  4.654 -0.249 -0.757 -0.574 -0.329 -0.368 -0.389 -0.249 -0.128
  -0.236 -0.801]
 [-0.027 -0.249  2.575 -0.062 -0.757 -0.757 -0.486 -0.056 -0.062 -0.018
  -0.034 -0.066]
 [-0.757 -0.757 -0.062  4.628 -0.249 -0.128 -0.2   -0.574 -0.329 -0.236
  -0.389 -0.946]
 [-0.108 -0.574 -0.757 -0.249  4.391 -0.895 -0.801 -0.249 -0.249 -0.103
  -0.169 -0.236]
 [-0.04  -0.329 -0.757 -0.128 -0.895  3.855 -0.895 -0.2   -0.249 -0.103
  -0.151 -0.108]
 [-0.056 -0.368 -0.486 -0.2   -0.801 -0.895  4.409 -0.389 -0.486 -0.249
  -0.329 -0.151]
 [-0.2   -0.389 -0.056 -0.574 -0.249 -0.2   -0.389  5.086 -0.895 -0.801
  -0.946 -0.389]
 [-0.082 -0.249 -0.062 -0.329 -0.249 -0.249 -0.486 -0.895  4.642 -0.895
  -0.946 -0.2  ]
 [-0.053 -0.128 -0.018 -0.236 -0.103 -0.103 -0.249 -0.801 -0.895  3.659
  -0.946 -0.128]
 [-0.108 -0.236 -0.034 -0.389 -0.169 -0.151 -0.329 -0.946 -0.946 -0.946
   4.49  -0.236]
 [-0.895 -0.801 -0.066 -0.946 -0.236 -0.108 -0.151 -0.389 -0.2   -0.128
  -0.236  4.155]]
    \end{Verbatim}

    \subsubsection{Eigenvalue Decomposition}\label{eigenvalue-decomposition}

In Step 4 the structural properties encoded in the Laplacian matrix are
used to transform the data into a form where clusters can be easily
identified. Remember that \textbf{Eigenvalue Decomposition} is a method
of decomposing a matrix into its constituent parts in such a way that it
reveals the matrix's intrinsic properties. For a square matrix \(A\),
the decomposition finds eigenvalues \(\lambda\) and eigenvectors \(v\)
that satisfy the equation:

\[ A v = \lambda v \]

In the context of spectral clustering, \(A\) is the Laplacian matrix
\(L\).

    \textbf{Role in Spectral Clustering}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Computing Eigenvalues and Eigenvectors}: The first step is to
  compute the eigenvalues and eigenvectors of the Laplacian matrix
  \(L\). The eigenvalues give a spectrum that represents the graph's
  connectivity, and the corresponding eigenvectors provide directions
  along which the graph can be ``cut'' to form clusters.
\item
  \textbf{Selection of Eigenvectors}: The eigenvalues are sorted from
  smallest to largest. The smallest eigenvalue is typically zero (or
  close to zero for numerical reasons) and corresponds to a constant
  eigenvector (due to the graph's connectedness). This eigenvalue and
  its corresponding eigenvector are generally discarded because they do
  not contribute to distinguishing between clusters (see
  \href{https://medium.com/@roiyeho/spectral-clustering-50aee862d300}{this
  blog} for a proof of this result).
\item
  \textbf{Dimensionality Reduction}: \textbf{\emph{The next few smallest
  eigenvalues and their corresponding eigenvectors are selected}}. These
  eigenvectors are critical because they capture the most significant
  structure of the data as encoded in the graph. By stacking these
  eigenvectors column-wise, we form a new dataset. Each row of this new
  dataset represents the original data points, but now they are
  expressed in terms of these eigenvectors.
\item
  \textbf{Creating a Low-Dimensional Representation}: The process
  effectively transforms the original high-dimensional data into a
  lower-dimensional space. The dimensionality of this new space is
  determined by the number of selected eigenvectors. This transformed
  dataset is easier to cluster because the transformation has teased
  apart the clusters along the dimensions defined by the eigenvectors.
\end{enumerate}

    \textbf{Significance of the Eigenvectors}

The selected eigenvectors serve as a basis for a new feature space where
the clustering structure of the data is more apparent. Points that are
part of the same cluster in the original space will be closer together
in this new space, making traditional clustering techniques like k-means
more effective.

This transformation is possible because the eigenvectors corresponding
to the smallest non-zero eigenvalues capture the slowest changes in the
graph's connectivity. In other words, they highlight the broadest
separations between clusters, where cutting the graph would incur the
smallest ``cost'' in terms of disconnectedness.

\textbf{Practical Implications}

This step is where the ``spectral'' in spectral clustering truly comes
into play, as it leverages the spectrum (eigenvalues) of the Laplacian
matrix to find a meaningful representation of the data. The process
ensures that the inherent cluster structure, possibly obscured in the
original high-dimensional space, becomes evident, facilitating the final
clustering step.

    \textbf{Example}

Let's find the eigenvalues and eigenvectors of our Laplacian matrix, and
sort them by increasing order of the eigenvalues:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{eigen\PYZus{}vals}\PY{p}{,} \PY{n}{eigen\PYZus{}vecs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{eig}\PY{p}{(}\PY{n}{L}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Sort the eigenvectors by their corresponding eigenvalues}
\PY{n}{sorted\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argsort}\PY{p}{(}\PY{n}{eigen\PYZus{}vals}\PY{p}{)}
\PY{n}{eigen\PYZus{}vals} \PY{o}{=} \PY{n}{eigen\PYZus{}vals}\PY{p}{[}\PY{n}{sorted\PYZus{}idx}\PY{p}{]}
\PY{n}{eigen\PYZus{}vecs} \PY{o}{=} \PY{n}{eigen\PYZus{}vecs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{sorted\PYZus{}idx}\PY{p}{]}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{eigen\PYZus{}vals}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[-0.     1.825  2.287  3.717  4.233  4.878  5.013  5.211  5.358  5.479
  5.539  5.905]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{eigen\PYZus{}vecs}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[-0.289 -0.373  0.572  0.196 -0.629 -0.098 -0.054  0.031  0.024 -0.027
   0.003  0.02 ]
 [-0.289 -0.069  0.178 -0.149  0.323  0.084 -0.263  0.296 -0.427 -0.625
   0.131 -0.025]
 [-0.289  0.654  0.174  0.661  0.11  -0.055  0.052 -0.05  -0.04  -0.017
  -0.005  0.004]
 [-0.289 -0.209  0.159 -0.029  0.335 -0.01   0.112  0.102 -0.448  0.671
  -0.12  -0.213]
 [-0.289  0.265  0.054 -0.293  0.001  0.034 -0.569  0.366  0.469  0.269
  -0.084 -0.008]
 [-0.289  0.357  0.013 -0.449 -0.296  0.287  0.615  0.181 -0.053 -0.05
  -0.015  0.005]
 [-0.289  0.202 -0.076 -0.346 -0.149 -0.241 -0.265 -0.714 -0.233  0.035
   0.185 -0.024]
 [-0.289 -0.14  -0.218  0.031  0.08  -0.215  0.084  0.077 -0.03   0.064
  -0.063  0.88 ]
 [-0.289 -0.11  -0.32   0.032 -0.037 -0.348  0.079 -0.013  0.082 -0.251
  -0.712 -0.311]
 [-0.289 -0.186 -0.483  0.282 -0.208  0.681 -0.205 -0.094 -0.085  0.041
   0.028 -0.044]
 [-0.289 -0.157 -0.337  0.107  0.007 -0.376  0.192  0.218  0.214  0.005
   0.645 -0.284]
 [-0.289 -0.236  0.283 -0.042  0.463  0.257  0.221 -0.4    0.526 -0.117
   0.008  0.001]]
    \end{Verbatim}

    \subsubsection{Clustering}\label{clustering}

This step is where the actual grouping of data points into clusters
occurs. After transforming the dataset into a lower-dimensional space
using the eigenvectors of the Laplacian matrix (as described in Step 4),
the structure of the data is now more conducive to clustering.

\textbf{Preparing the Data for Clustering}

\begin{itemize}
\item
  \textbf{New Feature Space}: The eigenvectors selected in the previous
  step form a new dataset where each data point is represented in the
  reduced dimensional space. This space is constructed specifically to
  highlight the separation between clusters that was implicit in the
  original data's graph structure.
\item
  \textbf{Rows as Points}: Each row of the new dataset (formed by
  stacking the selected eigenvectors) represents an original data point,
  but now encoded in terms of the principal components of the graph's
  structure. In this space, data points that belong to the same cluster
  in the original dataset are expected to be closer together, making
  them more distinguishable by clustering algorithms.
\end{itemize}

    \textbf{Applying a Clustering Algorithm}

\begin{itemize}
\item
  \textbf{Choosing an Algorithm}: The most common algorithm used at this
  stage is k-means because of its simplicity and effectiveness. However,
  other clustering algorithms can also be applied depending on the
  specific characteristics of the data or the requirements of the task.
\item
  \textbf{Number of Clusters}: The number of clusters, \(k\), to be used
  by the clustering algorithm can be determined based on prior knowledge
  about the data, or by using methods such as the silhouette score, the
  elbow method, or other heuristic techniques to estimate the optimal
  \(k\).
\item
  \textbf{Execution}: The chosen clustering algorithm is applied to the
  dataset in the reduced dimensional space. Because the transformation
  has already enhanced the cluster structure, the algorithm can more
  easily identify and assign the data points to their respective
  clusters.
\end{itemize}

    \textbf{Outcome}

\begin{itemize}
\item
  \textbf{Cluster Labels}: The result of this clustering step is a set
  of labels that indicate the cluster membership for each data point in
  the original dataset. These labels can then be used for further
  analysis, visualization, or as input for other machine learning tasks.
\item
  \textbf{Interpretation}: The clusters identified in this
  lower-dimensional space correspond to groups of points that are
  similar according to the graph structure of the original data. This
  means that even if the original features did not clearly delineate
  clusters, the spectral clustering process has revealed the underlying
  structure.
\end{itemize}

    \textbf{Significance}

This final step of clustering in the reduced dimensional space is
significant because it leverages the preparatory work of transforming
the data to make inherent clusters more apparent. By addressing the
clustering problem in a space that explicitly reflects the connectivity
and density of the original data, spectral clustering can uncover
groupings that might be missed by applying traditional clustering
algorithms directly to the original features.

    \textbf{Practical Considerations}

\begin{itemize}
\item
  \textbf{Algorithm Parameters}: The effectiveness of this step can
  depend heavily on the choice of parameters for the clustering
  algorithm, such as the number of clusters and the initialization
  method for k-means.
\item
  \textbf{Scalability}: While spectral clustering can provide superior
  results for complex cluster structures, the computational cost of
  earlier steps (especially eigenvalue decomposition) can limit its
  scalability to very large datasets.
\end{itemize}

    In summary, this step is where the spectral clustering process
culminates in the identification of meaningful clusters, utilizing a
lower-dimensional representation of the data that emphasizes its
intrinsic clustering structure. This approach allows for the discovery
of complex patterns and relationships that are not readily apparent in
the original feature space.

    \textbf{Example}

As we have said, instead of manually examining the eigenvectors, we can
apply a standard clustering algorithm such as k-means on the components
of the eigenvectors in order to extract the clusters from them.

We first build a matrix \(U \in \mathbb{R}^{n \, \times \, k}\) whose
columns are the first \(k\) eigenvectors of \(L\) (corresponding to the
\(k\) smallest eigenvalues of \(L\)), where \(k\) is typically the
number of desired clusters. Then, we use the rows of \(U\) as new
representations of the data points in a reduced \(k\)-dimensional space,
and apply k-means clustering on these rows.

For example, let's build the matrix \(U\) from the first three
eigenvectors of the Laplacian and the apply k-means clustering on the
rows of \(U\):

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{k} \PY{o}{=} \PY{l+m+mi}{3}  \PY{c+c1}{\PYZsh{} number of clusters}
\PY{n}{U} \PY{o}{=} \PY{n}{eigen\PYZus{}vecs}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{n}{k}\PY{p}{]}
\PY{n}{U}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[-0.28867513, -0.37276966,  0.5724692 ],
       [-0.28867513, -0.06930779,  0.17759156],
       [-0.28867513,  0.65407641,  0.17415158],
       [-0.28867513, -0.20901531,  0.15872478],
       [-0.28867513,  0.26542069,  0.05372716],
       [-0.28867513,  0.35725744,  0.0133131 ],
       [-0.28867513,  0.20156787, -0.07555769],
       [-0.28867513, -0.13969945, -0.21810807],
       [-0.28867513, -0.10972579, -0.3203228 ],
       [-0.28867513, -0.18550573, -0.48257005],
       [-0.28867513, -0.15672828, -0.33652115],
       [-0.28867513, -0.23557041,  0.28310237]])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k+kn}{import} \PY{n}{KMeans}

\PY{n}{kmeans} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{n}{k}\PY{p}{,} \PY{n}{n\PYZus{}init}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{labels} \PY{o}{=} \PY{n}{kmeans}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{U}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{labels}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[2 2 0 2 0 0 0 1 1 1 1 2]
    \end{Verbatim}

    \subsubsection{Purpose of Assignment Back to Original
Data}\label{purpose-of-assignment-back-to-original-data}

The primary goal of this step is to integrate the clustering outcomes
with the original dataset, making the results meaningful and applicable
to real-world problems or further analysis. The clustering performed in
the reduced dimensional space, while revealing the intrinsic structure
of the data, abstracts away from the original features of the dataset.
Therefore, assigning the discovered cluster memberships back to the
original data points is crucial for interpretation and application.

    \textbf{How the Assignment Works}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Cluster Labels}: During the clustering phase in the reduced
  dimensional space, each data point is assigned a cluster label,
  indicating which cluster it belongs to. These labels are the key
  outcome of the clustering algorithm (e.g., k-means) used in the
  transformed space.
\item
  \textbf{Mapping Labels}: The cluster labels are directly mapped back
  to the corresponding data points in the original dataset. Since the
  rows of the transformed dataset represent the same data points as in
  the original dataset (albeit in a different feature space), this
  mapping is straightforward. Each data point in the original dataset is
  assigned the cluster label of its representation in the reduced
  dimensional space.
\item
  \textbf{Resulting Clustered Dataset}: The end result is the original
  dataset with an additional piece of information for each data point:
  its cluster membership. This allows for the clusters identified
  through spectral clustering to be analyzed and interpreted in the
  context of the original features and dimensions of the data.
\end{enumerate}

    \textbf{Significance of This Step}

\begin{itemize}
\item
  \textbf{Interpretability}: Assigning the clusters back to the original
  data facilitates interpretability. Stakeholders and decision-makers
  can understand the clustering results in terms of the original
  features and dimensions, which is essential for making informed
  decisions.
\item
  \textbf{Visualization}: With cluster assignments mapped back to the
  original dataset, it becomes possible to visualize the clusters in the
  context of the original features, enhancing understanding of how and
  why the data points are grouped together.
\item
  \textbf{Further Analysis}: The cluster assignments can be used as
  labels for further analysis, such as profiling clusters based on the
  original features, conducting hypothesis testing between groups, or
  using the clusters as input for supervised learning tasks.
\end{itemize}

\textbf{Practical Considerations}

\begin{itemize}
\item
  \textbf{Consistency and Coherence}: The effectiveness of the spectral
  clustering process must ensure that the clusters identified in the
  reduced dimensional space are coherent and consistent when mapped back
  to the original space. The interpretability of the results depends on
  the meaningfulness of these clusters in the context of the original
  data.
\item
  \textbf{Actionable Insights}: The ultimate test of the clustering
  process is whether it provides actionable insights. By mapping cluster
  memberships back to the original data, practitioners can derive
  specific actions or recommendations based on the clustering, such as
  targeted marketing strategies, personalized recommendations, or
  efficient resource allocation.
\end{itemize}

In summary, Step 6 bridges the gap between the abstract representation
of the data used for clustering and the concrete, original dataset. It
ensures that the insights gained from spectral clustering are directly
applicable and interpretable in the real-world context that the data
represents.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{To plot points using different colors for each cluster, you can use libraries such as matplotlib or seaborn, }
\PY{l+s+sd}{which offer flexibility in customizing plots. Here\PYZsq{}s a basic approach using matplotlib, assuming you have }
\PY{l+s+sd}{an array of data points X (with dimensions n by 2 for simplicity, where n is the number of points, and each }
\PY{l+s+sd}{point has two coordinates) and a corresponding array of cluster labels labels for each point in X. }
\PY{l+s+sd}{The cluster labels are mapped into an array where each element corresponds to the cluster of the }
\PY{l+s+sd}{respective point in X.}
\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{c+c1}{\PYZsh{} Unique labels to identify the clusters}
\PY{n}{unique\PYZus{}labels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{labels}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot each cluster with a different color. The loop iterates through each unique cluster }
\PY{c+c1}{\PYZsh{} label, selects the points belonging to that cluster from X, and plots them with a unique }
\PY{c+c1}{\PYZsh{} color automatically chosen by matplotlib. The label parameter in plt.scatter is used }
\PY{c+c1}{\PYZsh{} to assign a label to each cluster for the legend.}
\PY{k}{for} \PY{n}{label} \PY{o+ow}{in} \PY{n}{unique\PYZus{}labels}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Select all points that belong to the current cluster}
    \PY{n}{cluster\PYZus{}points} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{labels} \PY{o}{==} \PY{n}{label}\PY{p}{]}
    
    \PY{c+c1}{\PYZsh{} Plot the points with a different color and label}
    \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{cluster\PYZus{}points}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{cluster\PYZus{}points}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cluster }\PY{l+s+si}{\PYZob{}}\PY{n}{label}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Add legend to the plot to differentiate clusters}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Add title and labels for clarity}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Clustered Data Points}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X coordinate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y coordinate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Display the plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_55_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Examples of Spectral Clustering
Applications}\label{examples-of-spectral-clustering-applications}

\begin{itemize}
\item
  \textbf{Image Segmentation}: Spectral clustering can be used to
  segment an image into regions based on the similarity of colors or
  textures. This is particularly useful in computer vision for object
  and boundary detection.
\item
  \textbf{Social Network Analysis}: It can help identify communities
  within social networks by finding clusters of users that are more
  closely connected to each other than to the rest of the network.
\item
  \textbf{Bioinformatics}: In gene expression data analysis, spectral
  clustering can be used to identify groups of genes that exhibit
  similar expression patterns, which may indicate a shared role in
  cellular processes.
\end{itemize}

Spectral clustering is powerful because it can identify complex cluster
structures that traditional clustering algorithms might not detect.
However, it also has its limitations, such as sensitivity to the choice
of similarity measure and scalability issues with very large datasets.

    \subsubsection{A Simple Example}\label{a-simple-example}

    First of all let's have a look to a simple example of spectral
clustering using Python's scikit-learn library. This example will create
a synthetic dataset with two moons shapes, which are not linearly
separable, to demonstrate how spectral clustering can effectively
identify the clusters.

First, ensure you have scikit-learn and matplotlib installed for this
example. If not uncomment and run the following lines:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+ch}{\PYZsh{}!pip3 install scikit\PYZhy{}learn matplotlib}
\end{Verbatim}
\end{tcolorbox}

    Now, here's the Python code for spectral clustering\ldots{}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{60}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{warnings}
\PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    We first generate a synthetic dataset using \texttt{make\_moons} from
\texttt{scikit-learn}, which creates two interleaving half circles
(moons) that are difficult to separate using linear methods.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Generate a synthetic dataset with two moons shape}
\PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{make\PYZus{}moons}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{noise}\PY{o}{=}\PY{l+m+mf}{0.07}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    We then apply \texttt{SpectralClustering} from \texttt{scikit-learn}. We
specify \texttt{n\_clusters=2} because we know there are two groups. The
\texttt{affinity=\textquotesingle{}nearest\_neighbors\textquotesingle{}}
parameter tells the algorithm to use the nearest neighbors approach to
construct the affinity matrix, which represents the graph. The
\texttt{assign\_labels=\textquotesingle{}kmeans\textquotesingle{}}
parameter specifies that k-means is used in the final step to assign
points to clusters based on the spectral embedding.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Apply Spectral Clustering}
\PY{n}{sc} \PY{o}{=} \PY{n}{SpectralClustering}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{affinity}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest\PYZus{}neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{assign\PYZus{}labels}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kmeans}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
\PY{n}{y\PYZus{}sc} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Finally, we plot the original dataset and the clustering result. The two
plots show how spectral clustering effectively separates the two moons
based on their shape, not relying on their linear separability.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plotting the results}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y\PYZus{}sc}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Spectral Clustering Result}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}plt.legend()}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_67_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Comparison with other
methods}\label{comparison-with-other-methods}

    Below is a Python code that reproduces the example comparing K-Means and
Spectral Clustering on a synthetic dataset with two concentric circles.
This dataset poses a challenge for traditional clustering algorithms
like K-Means but is well-suited for Spectral Clustering due to its
ability to handle complex cluster shapes.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}circles}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k+kn}{import} \PY{n}{KMeans}\PY{p}{,} \PY{n}{SpectralClustering}

\PY{c+c1}{\PYZsh{} Data Generation: make\PYZus{}circles from sklearn.datasets creates a dataset with two concentric circles.}
\PY{n}{X}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{make\PYZus{}circles}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{factor}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{noise}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Apply K\PYZhy{}Means clustering}
\PY{n}{kmeans} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{n\PYZus{}init}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{y\PYZus{}kmeans} \PY{o}{=} \PY{n}{kmeans}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Spectral Clustering: SpectralClustering, also from sklearn.cluster, is used with the nearest\PYZus{}neighbors }
\PY{c+c1}{\PYZsh{} affinity to better capture the dataset\PYZsq{}s structure. The n\PYZus{}neighbors parameter is set to 10 to define }
\PY{c+c1}{\PYZsh{} the local neighborhood size for the graph construction.}
\PY{n}{spectral} \PY{o}{=} \PY{n}{SpectralClustering}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{affinity}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest\PYZus{}neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
\PY{n}{y\PYZus{}spectral} \PY{o}{=} \PY{n}{spectral}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{65}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plotting the results}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} K\PYZhy{}Means Result}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y\PYZus{}kmeans}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{K\PYZhy{}Means Clustering}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Spectral Clustering Result}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y\PYZus{}spectral}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Spectral Clustering}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_71_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{itemize}
\item
  \textbf{K-Means Clustering}: The left plot shows the result of
  applying K-Means to the dataset. K-Means assumes clusters are convex
  and isotropic, which leads to a poor clustering outcome for this
  particular shape of data. It tries to partition the data into two
  groups based on distance to the cluster centroids, which doesn't work
  well for concentric circles.
\item
  \textbf{Spectral Clustering}: The right plot displays the outcome of
  spectral clustering. This technique, which uses the nearest neighbors
  to construct a graph representation of the dataset, effectively
  captures the essence of the two circles and correctly identifies the
  two clusters. Spectral clustering does not assume any particular shape
  for the clusters, allowing it to handle this complex structure
  successfully.
\end{itemize}

This example demonstrates the strength of spectral clustering in dealing
with datasets where the clusters are not linearly separable or have
unusual shapes, making it a powerful tool for real-world data analysis
challenges where traditional clustering methods fall short.

    Another advantage of \textbf{\emph{Spectral Clustering}} is that it can
handle clusters with varying shapes, sizes, and densities. For example,
let's look at the composite dataset below. The points are drawn from an
elliptical cluster and two additional concentric circles with some
random noise:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{compound\PYZus{}data}\PY{p}{(}\PY{n}{random\PYZus{}seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
    \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{n}{random\PYZus{}seed}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Generate the three clusters    }
    \PY{c+c1}{\PYZsh{} 1. Main elliptical cluster}
    \PY{n}{x1} \PY{o}{=} \PY{n}{random\PYZus{}state}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{)}
    \PY{n}{y1} \PY{o}{=} \PY{n}{x1} \PY{o}{/} \PY{l+m+mi}{3} \PY{o}{+} \PY{n}{random\PYZus{}state}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} 2. Small circle cluster}
    \PY{n}{angle2} \PY{o}{=} \PY{n}{random\PYZus{}state}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
    \PY{n}{x2} \PY{o}{=} \PY{l+m+mi}{5} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{angle2}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{10}
    \PY{n}{y2} \PY{o}{=} \PY{l+m+mi}{5} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{angle2}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{10}

    \PY{c+c1}{\PYZsh{} 3. Larger circle cluster}
    \PY{n}{angle3} \PY{o}{=} \PY{n}{random\PYZus{}state}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}
    \PY{n}{x3} \PY{o}{=} \PY{l+m+mi}{15} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{angle3}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{15}
    \PY{n}{y3} \PY{o}{=} \PY{l+m+mi}{15} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{angle3}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{15}

    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{x1}\PY{p}{,} \PY{n}{x2}\PY{p}{,} \PY{n}{x3}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{y1}\PY{p}{,} \PY{n}{y2}\PY{p}{,} \PY{n}{y3}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{T}

\PY{n}{X} \PY{o}{=} \PY{n}{compound\PYZus{}data}\PY{p}{(}\PY{p}{)}
\PY{n}{plot\PYZus{}data}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_74_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{67}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{plot\PYZus{}clusters}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
    \PY{n}{sns}\PY{o}{.}\PY{n}{scatterplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{n}{labels}\PY{p}{,} \PY{n}{palette}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tab10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{edgecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZus{}1\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZus{}2\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k+kn}{import} \PY{n}{KMeans}\PY{p}{,} \PY{n}{AgglomerativeClustering}\PY{p}{,} \PY{n}{DBSCAN}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{mixture} \PY{k+kn}{import} \PY{n}{GaussianMixture}

\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{sca}\PY{p}{(}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{kmeans} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} 
\PY{n}{labels} \PY{o}{=} \PY{n}{kmeans}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{plot\PYZus{}clusters}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{KMeans}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{sca}\PY{p}{(}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{agg} \PY{o}{=} \PY{n}{AgglomerativeClustering}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{linkage}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{single}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{labels} \PY{o}{=} \PY{n}{agg}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{plot\PYZus{}clusters}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Agglomerative (Single linkage)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{sca}\PY{p}{(}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\PY{n}{agg} \PY{o}{=} \PY{n}{AgglomerativeClustering}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{linkage}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ward}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{labels} \PY{o}{=} \PY{n}{agg}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{plot\PYZus{}clusters}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Agglomerative (Ward}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{s method)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{sca}\PY{p}{(}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{dbscan} \PY{o}{=} \PY{n}{DBSCAN}\PY{p}{(}\PY{p}{)}
\PY{n}{labels} \PY{o}{=} \PY{n}{dbscan}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{plot\PYZus{}clusters}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DBSCAN}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{sca}\PY{p}{(}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{gmm} \PY{o}{=} \PY{n}{GaussianMixture}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{labels} \PY{o}{=} \PY{n}{gmm}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{plot\PYZus{}clusters}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GMM}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{sca}\PY{p}{(}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\PY{n}{sc} \PY{o}{=} \PY{n}{SpectralClustering}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{affinity}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest\PYZus{}neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} 
\PY{n}{labels} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{plot\PYZus{}clusters}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Spectral clustering}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}plt.savefig(\PYZsq{}figures/compound\PYZus{}clustering\PYZus{}comparison.pdf\PYZsq{})}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, 'Spectral clustering')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_76_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{An Example from Kaggle: Credit Card Data
Clustering}\label{an-example-from-kaggle-credit-card-data-clustering}

The below steps demonstrate how to implement Spectral Clustering using
Sklearn. The data for the following steps is the Credit Card Data which
can be downloaded from Kaggle.

\textbf{Step 1: Loading and Cleaning the Data}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{74}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}\PY{p}{,} \PY{n}{normalize}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k+kn}{import} \PY{n}{PCA}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{silhouette\PYZus{}score}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Loading the data}
\PY{n}{X} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data/CC\PYZus{}GENERAL.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Dropping the CUST\PYZus{}ID column from the data}
\PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CUST\PYZus{}ID}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Handling the missing values if any}
\PY{n}{X}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{method} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ffill}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{inplace} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}

\PY{n}{X}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
       BALANCE  BALANCE\_FREQUENCY  PURCHASES  ONEOFF\_PURCHASES
0    40.900749           0.818182      95.40              0.00  \textbackslash{}
1  3202.467416           0.909091       0.00              0.00
2  2495.148862           1.000000     773.17            773.17
3  1666.670542           0.636364    1499.00           1499.00
4   817.714335           1.000000      16.00             16.00

   INSTALLMENTS\_PURCHASES  CASH\_ADVANCE  PURCHASES\_FREQUENCY
0                    95.4      0.000000             0.166667  \textbackslash{}
1                     0.0   6442.945483             0.000000
2                     0.0      0.000000             1.000000
3                     0.0    205.788017             0.083333
4                     0.0      0.000000             0.083333

   ONEOFF\_PURCHASES\_FREQUENCY  PURCHASES\_INSTALLMENTS\_FREQUENCY
0                    0.000000                          0.083333  \textbackslash{}
1                    0.000000                          0.000000
2                    1.000000                          0.000000
3                    0.083333                          0.000000
4                    0.083333                          0.000000

   CASH\_ADVANCE\_FREQUENCY  CASH\_ADVANCE\_TRX  PURCHASES\_TRX  CREDIT\_LIMIT
0                0.000000                 0              2        1000.0  \textbackslash{}
1                0.250000                 4              0        7000.0
2                0.000000                 0             12        7500.0
3                0.083333                 1              1        7500.0
4                0.000000                 0              1        1200.0

      PAYMENTS  MINIMUM\_PAYMENTS  PRC\_FULL\_PAYMENT  TENURE
0   201.802084        139.509787          0.000000      12
1  4103.032597       1072.340217          0.222222      12
2   622.066742        627.284787          0.000000      12
3     0.000000        627.284787          0.000000      12
4   678.334763        244.791237          0.000000      12
\end{Verbatim}
\end{tcolorbox}
        
    \textbf{Step 2: Preprocessing the data to make the data visualizable}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{76}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Preprocessing the data to make it visualizable}

\PY{c+c1}{\PYZsh{} Scaling the Data}
\PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{X\PYZus{}scaled} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Normalizing the Data}
\PY{n}{X\PYZus{}normalized} \PY{o}{=} \PY{n}{normalize}\PY{p}{(}\PY{n}{X\PYZus{}scaled}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Converting the numpy array into a pandas DataFrame}
\PY{n}{X\PYZus{}normalized} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}normalized}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Reducing the dimensions of the data}
\PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{X\PYZus{}principal} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}normalized}\PY{p}{)}
\PY{n}{X\PYZus{}principal} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}principal}\PY{p}{)}
\PY{n}{X\PYZus{}principal}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{P1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{P2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{X\PYZus{}principal}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{76}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
         P1        P2
0 -0.489949 -0.679976
1 -0.519099  0.544827
2  0.330633  0.268880
3 -0.481656 -0.097613
4 -0.563512 -0.482505
\end{Verbatim}
\end{tcolorbox}
        
    \textbf{Step 3: Building the Clustering models and Visualizing the
Clustering}

In the below steps, two different Spectral Clustering models with
different values for the parameter \texttt{affinity}. You can read about
the documentation of the Spectral Clustering class
\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html}{here}.
a) affinity = \texttt{rbf}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{77}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Building the clustering model}
\PY{n}{spectral\PYZus{}model\PYZus{}rbf} \PY{o}{=} \PY{n}{SpectralClustering}\PY{p}{(}\PY{n}{n\PYZus{}clusters} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{affinity} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Training the model and Storing the predicted cluster labels}
\PY{n}{labels\PYZus{}rbf} \PY{o}{=} \PY{n}{spectral\PYZus{}model\PYZus{}rbf}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X\PYZus{}principal}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{78}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Building the label to colour mapping}
\PY{n}{colours} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
\PY{n}{colours}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{colours}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}

\PY{c+c1}{\PYZsh{} Building the colour vector for each data point}
\PY{n}{cvec} \PY{o}{=} \PY{p}{[}\PY{n}{colours}\PY{p}{[}\PY{n}{label}\PY{p}{]} \PY{k}{for} \PY{n}{label} \PY{o+ow}{in} \PY{n}{labels\PYZus{}rbf}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Plotting the clustered scatter plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n}{b} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}principal}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{P1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}principal}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{P2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{color} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
\PY{n}{y} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}principal}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{P1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}principal}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{P2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{color} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}principal}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{P1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{X\PYZus{}principal}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{P2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{c} \PY{o}{=} \PY{n}{cvec}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Label 0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Label 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_84_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_84_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  affinity = \texttt{nearest\_neighbors}
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{79}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Building the clustering model}
\PY{n}{spectral\PYZus{}model\PYZus{}nn} \PY{o}{=} \PY{n}{SpectralClustering}\PY{p}{(}\PY{n}{n\PYZus{}clusters} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{affinity} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest\PYZus{}neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Training the model and Storing the predicted cluster labels}
\PY{n}{labels\PYZus{}nn} \PY{o}{=} \PY{n}{spectral\PYZus{}model\PYZus{}nn}\PY{o}{.}\PY{n}{fit\PYZus{}predict}\PY{p}{(}\PY{n}{X\PYZus{}principal}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \textbf{Step 5: Evaluating the performances}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{80}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} List of different values of affinity}
\PY{n}{affinity} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest\PYZhy{}neighbours}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{c+c1}{\PYZsh{} List of Silhouette Scores}
\PY{n}{s\PYZus{}scores} \PY{o}{=} \PY{p}{[}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Evaluating the performance}
\PY{n}{s\PYZus{}scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{silhouette\PYZus{}score}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{labels\PYZus{}rbf}\PY{p}{)}\PY{p}{)}
\PY{n}{s\PYZus{}scores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{silhouette\PYZus{}score}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{labels\PYZus{}nn}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{s\PYZus{}scores}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[0.05300611480757429, 0.05667039590382262]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{81}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plotting a Bar Graph to compare the models}
\PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{affinity}\PY{p}{,} \PY{n}{s\PYZus{}scores}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Affinity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Silhouette Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Comparison of different Clustering Models}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_89_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Conclusions}\label{conclusions}

\textbf{Advantages of Spectral Clustering}:

\begin{itemize}
\item
  Scalability: Spectral clustering can handle large datasets and
  high-dimensional data, as it reduces the dimensionality of the data
  before clustering.
\item
  Flexibility: Spectral clustering can be applied to non-linearly
  separable data, as it does not rely on traditional distance-based
  clustering methods.
\item
  Robustness: Spectral clustering can be more robust to noise and
  outliers in the data, as it considers the global structure of the
  data, rather than just local distances between data points.
\end{itemize}

\textbf{Disadvantages of Spectral Clustering}:

\begin{itemize}
\item
  Complexity: Spectral clustering can be computationally expensive,
  especially for large datasets, as it requires the calculation of
  eigenvectors and eigenvalues.
\item
  Model selection: Choosing the right number of clusters and the right
  similarity matrix can be challenging and may require expert knowledge
  or trial and error.
\end{itemize}
\newpage
    \subsection{Reference and Credits}\label{reference-and-credits}

{[}1{]} V. Gandhi,
\href{https://www.kaggle.com/code/vipulgandhi/spectral-clustering-detailed-explanation}{``Spectral
Clustering - Detailed Explanation''}

{[}2{]} Roi Yehoshua,
\href{https://medium.com/@roiyeho/spectral-clustering-50aee862d300}{``Spectral
Clustering: Step-by-step derivation of the spectral clustering algorithm
including an implementation in Python''}

{[}3{]} A.Y. Ng, M.I. Jordan, Y. Weiss (2002). On spectral clustering:
Analysis and an algorithm, Advances in neural information processing
systems, pp.~849-- 856.

{[}4{]} U. von Luxburg (2007), A Tutorial on Spectral Clustering,
Statistics and computing, 17, 395--416.

{[}5{]} Lanczos, C. (1950). An iteration method for the solution of the
eigenvalue problem of linear differential and integral operators,
Journal of Research of the National Bureau of Standards. 45 (4):
255--282.

{[}6{]} Stoer, M., \& Wagner, F. (1997). A simple min-cut algorithm.
Journal of the ACM (JACM), 44(4), 585--591.

{[}7{]} Hagen, L., \& Kahng, A. B. (1992). New spectral methods for
ratio cut partitioning and clustering. IEEE transactions on
computer-aided design of integrated circuits and systems, 11(9),
1074--1085.
\newpage
    \subsection{Appendix}\label{appendix}

    As we have see, the eigenvectors associated with the smallest non-zero
eigenvalues capture the most significant structure of the data with
respect to the graph's connectivity. By partitioning the data based on
these eigenvectors, spectral clustering seeks to identify clusters that
minimize intra-cluster connections while maximizing inter-cluster
connections. Let's break it down for more intuitive understanding:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Graph's Connectivity}: Imagine your dataset as a network or
  graph, where each data point is a node, and the connections (edges)
  between nodes represent some form of similarity or relationship (e.g.,
  how close or similar two points are). In this network, you're looking
  to find groups (clusters) of nodes that are more closely connected to
  each other than to nodes outside their group.
\item
  \textbf{Eigenvectors and Eigenvalues}: Without diving deep into the
  mathematical details, eigenvalues and eigenvectors come from linear
  algebra and are properties of matrices, including the Laplacian
  matrix, which is derived from the graph of your data. Each eigenvalue
  has a corresponding eigenvector. The eigenvalues give you a sense of
  the ``spread'' or ``spectrum'' of the graph in various dimensions, and
  the eigenvectors give you the direction of these dimensions.
\item
  \textbf{Smallest Non-Zero Eigenvalues}: The smallest non-zero
  eigenvalues (and their eigenvectors) are special because they reveal
  the least obvious but most meaningful ways to ``cut'' or divide the
  graph. Why not the absolute smallest, which is zero? Because the zero
  eigenvalue corresponds to the overall connectedness of the graph and
  doesn't help to distinguish between different groups. It's like
  saying, ``everything is connected,'' which we already know.
\item
  \textbf{Capturing the Most Significant Structure}: The eigenvectors
  associated with these small but non-zero eigenvalues essentially tell
  us how we can view the data to see its groups most clearly. Think of
  it as finding the right angle to hold a cut gemstone so that its
  internal structure and the way it splits light become apparent. These
  eigenvectors help illuminate the underlying structure of the data in
  terms of connectivity.
\item
  \textbf{Partitioning Based on Eigenvectors}: By using these special
  eigenvectors, spectral clustering effectively finds the best way to
  split the graph into clusters. Each cluster is tightly knit internally
  (minimizing intra-cluster connections, meaning the nodes within a
  cluster are closely related), and there's clear separation between
  different clusters (maximizing inter-cluster connections, meaning the
  clusters themselves are distinct and not closely connected).
\end{enumerate}

\textbf{Intuitive Thought}: Imagine you're at a large social gathering
where some people are friends, family, or colleagues, forming natural
groups based on their relationships. If you wanted to organize everyone
into these groups without knowing their relationships, you might observe
who talks to whom and how groups form during the event. The process of
spectral clustering is akin to observing these interactions (graph's
connectivity) and finding the least noticeable yet most telling signs
(smallest non-zero eigenvalues and their eigenvectors) of how to
naturally divide the room into its social circles (clusters). This
method ensures that within each circle, connections are strong and
abundant, while between circles, interactions are fewer, highlighting
the distinct groups present at the gathering.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
