{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc3212d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/unibo-intensive-program-2024/blob/main/1-notebooks/chapter-08-02.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95511547",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning in Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ad9e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative AI\n",
    "\n",
    "- In recent years, generative models based on deep learning have attracted increasing attention, thanks to remarkable advancements in the domain. Leveraging vast datasets, meticulously crafted network structures, and sophisticated training methods, these deep generative models have demonstrated an extraordinary capacity for generating highly realistic content across different forms, including images, text, and audio. \n",
    "\n",
    "- Within the realm of these deep generative models, two primary categories emerge as particularly noteworthy: **Generative Adversarial Networks (GANs)** and **Variational Autoencoders (VAEs)**, each meriting special focus for their contributions and capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e8fc7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd3cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition\n",
    "\n",
    "- Variational Autoencoders (VAEs) represent a groundbreaking approach in the field of generative models, offering a unique blend of deep learning and Bayesian inference to generate new data samples that are similar to the input data. \n",
    "\n",
    "- Unlike traditional autoencoders, which aim to encode an input into a lower-dimensional space and then reconstruct it back to its original form, VAEs introduce a probabilistic twist to this process, enabling the generation of new data points.\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-02-00.jpg\" width=\"500\" height=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514455f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Structure\n",
    "\n",
    "- A VAE consists of two main components: the encoder and the decoder, just like a traditional autoencoder. \n",
    "\n",
    "- However, the encoder in a VAE, also known as the recognition model, ***maps inputs to a distribution over the latent space rather than a single point***. \n",
    "\n",
    "- This distribution is typically assumed to be Gaussian, characterized by a mean and a variance. \n",
    "\n",
    "- The decoder, also referred to as the generative model, then samples points from this distribution and attempts to reconstruct the input data from these samples.\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-02-01.jpg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1038e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*image source: Rocca J. see References and Credits Section*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a5cec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Functioning\n",
    "\n",
    "1. **Encoding**: The encoder takes an input $x$ and transforms it into two parameters in a latent space, $\\mu$ and $\\sigma^2$, which represent the mean and variance of a Gaussian distribution. This step introduces the element of probabilistic inference, as instead of encoding an input as a single point, it is encoded as a distribution over possible points.\n",
    "\n",
    "2. **Reparameterization Trick**: To enable backpropagation through random sampling, the VAE employs the so called reparameterization trick. Instead of sampling $z$ directly from the Gaussian distribution defined by $\\mu$ and $\\sigma^2$, the model samples $\\epsilon$ from a standard Gaussian, and then $z$ is computed as $z = \\mu + \\sigma \\odot \\epsilon$. This trick allows the gradient of the loss function to be backpropagated through the encoder.\n",
    "\n",
    "3. **Decoding**: The sampled latent variable $z$ is then passed to the decoder, which generates a reconstruction $\\hat{x}$ of the original input $x$. The goal of the decoder is to learn a distribution $p(x|z)$ over the possible inputs $x$ given a latent representation $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19cc74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-02-02.jpg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5305a44",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*image source: Rocca J. see References and Credits Section*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c0dc38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- As we have seen, in practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. \n",
    "\n",
    "- The reason why an input is encoded as a distribution with some variance instead of a single point is that it makes possible to express very naturally the latent space regularisation: the distributions returned by the encoder are enforced to be close to a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa7a52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training and Loss Function\n",
    "\n",
    "Thus, the loss function that is minimised when training a VAE is composed of a “reconstruction term” (on the final layer), that tends to make the encoding-decoding scheme as performant as possible, and a “regularisation term” (on the latent layer), that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution.:\n",
    "\n",
    "- **Reconstruction Loss**: This measures how well the reconstructed data $\\hat{x}$ matches the original data $x$. It ensures that the decoder learns to accurately reconstruct the input from the latent variables. A common choice for this loss is the Mean Squared Error (MSE) for continuous data or Binary Cross-Entropy for binary data.\n",
    "\n",
    "- **KL Divergence**: This term acts as a regularizer, ensuring that the distribution learned by the encoder ( $q(z|x)$ ) is similar to the prior distribution of the latent variables, typically assumed to be a standard Gaussian $p(z)$. The Kullback-Leibler (KL) divergence measures the difference between these two distributions. We can notice that the Kullback-Leibler divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48358e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Calculation of the KL Divergence**\n",
    "\n",
    "The calculation of the Kullback-Leibler (KL) divergence is a crucial part of training variational autoencoders (VAEs) since It measures the discrepancy between two probability distributions. In the context of VAEs, it's used to ensure that the distribution of latent variables learned by the encoder network is close to a predefined distribution, typically a standard normal distribution. The KL divergence between two probability distributions $ P $ and $ Q $ is defined as:\n",
    "\n",
    "$$ D_{KL}(P || Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)} $$\n",
    "\n",
    "However, in the case of VAEs, we typically deal with multivariate Gaussian distributions. If we consider $ P $ as the distribution learned by the encoder (latent space distribution) and $ Q $ as the standard normal distribution, the KL divergence can be expressed analytically:\n",
    "\n",
    "$$D_{KL}(q(z|x)||p(z)) = \\frac{1}{2} \\sum_{i=1}^{N}(1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2)$$\n",
    "\n",
    "Where:\n",
    "- $q(z|x)$ is the approximate posterior distribution (encoder output).\n",
    "- $p(z)$ is the prior distribution (often chosen to be a standard normal distribution).\n",
    "- $\\mu_i$ and $\\sigma_i^2$ are the mean and variance of the approximate posterior distribution respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c61cba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-02-03.jpg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c10f60",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*image source: Rocca J. see References and Credits Section*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a48e02",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "VAEs have a wide range of applications in the field of generative modeling, including but not limited to:\n",
    "\n",
    "- **Image Generation**: VAEs can generate new images that resemble the training set, useful in art, design, and entertainment.\n",
    "- **Anomaly Detection**: By learning to reconstruct normal data, VAEs can be used to detect anomalies or outliers in datasets.\n",
    "- **Data Augmentation**: VAEs can create new data samples for training machine learning models, especially useful in scenarios where data is scarce.\n",
    "- **Dimensionality Reduction**: Similar to PCA or t-SNE, VAEs can be used for dimensionality reduction, providing a way to visualize high-dimensional data in lower-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1e757",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Implementation of a Simple Variational Autoencoder using Keras\n",
    "\n",
    "The following code defines and trains a Variational Autoencoder (VAE) using TensorFlow and Keras. A VAE is a generative model that learns to encode inputs into a latent space and then decode from this space to reconstruct the inputs. It's particularly useful for tasks like unsupervised learning of complex distributions, dimensionality reduction, and generative tasks. Here, the VAE is applied to the MNIST dataset, which consists of 28x28 pixel grayscale images of handwritten digits (0 through 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ef4f6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "268af609",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35af08",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Define VAE Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "20dd4bb0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fcc183",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***latent_dim*** is the dimensionality of the latent space. A smaller dimension forces the model to learn a more compact representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80adca18",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Encoder**\n",
    "\n",
    "Now we define an encoder network using Convolutional Neural Network (CNN) layers to map input images (28x28 grayscale) to a latent space representation. The encoder consists of two convolutional layers followed by flattening and two dense layers to compute both the mean and the log variance of the latent space. The encoder model takes input images and outputs the mean and log variance vectors, representing the parameters of a Gaussian distribution in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "10245e23",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the input shape for the encoder network, which is a 28x28 grayscale image\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Adding the first convolutional layer with 32 filters, a kernel size of 3x3, ReLU activation function, \n",
    "# a stride of 2, and padding to maintain the input size\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "\n",
    "# Adding the second convolutional layer with 64 filters, a kernel size of 3x3, ReLU activation function, \n",
    "# a stride of 2, and padding to maintain the input size\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "# Flattening the output from the convolutional layers to prepare for the dense layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Creating the mean of the latent space representation using a dense layer\n",
    "# The size of the dense layer is defined by the variable latent_dim\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "\n",
    "# Creating the log variance of the latent space representation using another dense layer\n",
    "# The size of the dense layer is also defined by the variable latent_dim\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "# Creating the encoder model that maps input images to both mean and log variance\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a062ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Reparametrization Trick**\n",
    "\n",
    "The purpose of the following user defined function is to implement the reparameterization trick, which enables the VAE to backpropagate gradients through the stochastic sampling process during training. \n",
    "\n",
    "Explanation:\n",
    "1. The `args` parameter consists of two tensors, `z_mean` and `z_log_var`, representing the mean and log variance vectors of the latent space distribution for a batch of input samples.\n",
    "2. The batch size (`batch`) and dimensionality of the latent space (`dim`) are determined dynamically from the shape of `z_mean`.\n",
    "3. Random noise (`epsilon`) is sampled from a Gaussian distribution with mean 0 and standard deviation 1. This noise has the same shape as `z_mean`.\n",
    "4. The reparameterization trick is applied to sample from the latent space distribution. It involves adding a random perturbation (`epsilon`) scaled by the standard deviation (square root of the variance, computed as `exp(0.5 * z_log_var)`) to the mean (`z_mean`). This operation ensures that the sampling process is differentiable, allowing gradients to flow through the network during training.\n",
    "5. The sampled vector is returned as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54894c86",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a function for sampling from the latent space in the context of a Variational Autoencoder (VAE)\n",
    "# This function takes the mean and log variance of the latent space as input arguments\n",
    "def sampling(args):\n",
    "    # Unpacking the input arguments\n",
    "    z_mean, z_log_var = args\n",
    "    \n",
    "    # Determining the batch size dynamically\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    \n",
    "    # Determining the dimensionality of the latent space dynamically\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    \n",
    "    # Generating random noise from a Gaussian distribution with mean 0 and standard deviation 1\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    # Reparameterization trick: sampling from the learned distribution in the latent space\n",
    "    # The sampled vector is computed as the mean plus a random perturbation scaled by the standard deviation\n",
    "    # The scaling ensures that the perturbation respects the learned variance\n",
    "    sampled_vector = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    # Returning the sampled vector\n",
    "    return sampled_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07cc47e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9c9bef55",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8fa56f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "utilizes the `Lambda` layer in Keras to apply the custom `sampling` function as part of the model's computation graph. Let's break down the components and the significance of this line within the context of a Variational Autoencoder (VAE):\n",
    "\n",
    "*Lambda Layer*\n",
    "\n",
    "- **`layers.Lambda`**: This is a way to wrap arbitrary expressions as a Layer object in the Keras model. The Lambda layer is used for executing arbitrary code while building Keras models. It's particularly useful for operations that don't require learning weights.\n",
    "\n",
    "*Sampling Function*\n",
    "\n",
    "- **`sampling`**: This refers to the custom function defined earlier in the code. The sampling function takes the mean (`z_mean`) and log variance (`z_log_var`) of the latent variables as inputs to generate a sample from the latent distribution. This sampling involves adding stochasticity to the model, enabling the VAE to generate diverse outputs from the latent space.\n",
    "\n",
    "*Inputs to the Lambda Layer*\n",
    "\n",
    "- **`([z_mean, z_log_var])`**: These are the inputs to the Lambda layer, specifically the outputs from the encoder model that represent the parameters (mean and log variance) of the Gaussian distribution in the latent space. By providing these as inputs to the `Lambda` layer, the `sampling` function can use them to generate a sample `z` from the latent space.\n",
    "\n",
    "*Output of the Lambda Layer*\n",
    "\n",
    "- **`output_shape=(latent_dim,)`**: This specifies the shape of the output from the Lambda layer, which is the shape of the latent space vector `z`. Since `latent_dim` is set to 2, this means each sample `z` from the latent space will be a 2-dimensional vector.\n",
    "\n",
    "*Purpose in the VAE*\n",
    "\n",
    "The purpose of this line in the VAE is crucial:\n",
    "- It bridges the encoder and decoder by generating a latent vector `z` that is sampled from the distribution defined by the encoder's output (`z_mean` and `z_log_var`). This introduces the variational aspect of the VAE, as the model learns to encode inputs into a distribution over the latent space rather than a fixed point. The randomness introduced by the sampling is essential for the generative process, allowing the model not only to reconstruct inputs but also to generate new data after training.\n",
    "\n",
    "This approach allows the VAE to effectively learn a probabilistic mapping from the input space to the latent space and back, facilitating the generation of new samples that are similar to the input data by decoding points sampled from the latent space distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d9680",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9b6d6f25",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define an input layer for the latent space with the specified dimensionality\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# Apply a fully connected (dense) layer to the latent inputs to expand them to \n",
    "# the size required for the convolutional layers\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "\n",
    "# Reshape the flattened output from the dense layer into a 3D tensor\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "\n",
    "# Apply a transposed convolutional layer to upsample the feature map\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "# Apply another transposed convolutional layer for further upsampling\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "# Apply a final transposed convolutional layer to generate the output image\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "# Create the decoder model that maps latent space inputs to output images\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea99be",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**VAE Model**\n",
    "\n",
    "The VAE model combines the encoder and decoder. The encoder inputs are passed through the encoder to get the latent space parameters, which are then sampled and decoded to produce the output images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c52a7637",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# VAE model\n",
    "outputs = decoder(z)\n",
    "vae = keras.Model(encoder_inputs, outputs, name=\"vae\")\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e059b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Loss Function**\n",
    "\n",
    "The loss function combines reconstruction loss (to ensure the output images match the input images) and KL divergence (to regularize the latent space). The KL divergence encourages the latent space to approximate a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30175f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the context of the our code, the calculation of the KL divergence breaks down as follows:\n",
    "\n",
    "1. `kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)`: This line computes the terms inside the KL divergence formula, using the log-variance (`z_log_var`) and mean (`z_mean`) vectors obtained from the encoder network.\n",
    "\n",
    "2. `kl_loss = tf.reduce_mean(kl_loss)`: The computed terms are then averaged over the batch dimension, resulting in a single scalar value representing the mean KL divergence across the entire batch.\n",
    "\n",
    "3. `kl_loss *= -0.5`: Finally, the result is multiplied by $-0.5$. This scaling factor is applied to match the formula of the KL divergence for Gaussian distributions.\n",
    "\n",
    "By minimizing the KL divergence term along with the reconstruction loss, the VAE learns to encode input data into a latent space distribution that closely resembles a standard normal distribution, facilitating smooth interpolation and generation of new samples during the decoding process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ec846bb5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Reconstruction loss calculation using binary crossentropy\n",
    "reconstruction_loss = tf.keras.losses.binary_crossentropy(encoder_inputs, outputs)\n",
    "\n",
    "# Scaling the reconstruction loss by the image size\n",
    "reconstruction_loss *= 28 * 28\n",
    "\n",
    "# KL divergence loss calculation\n",
    "kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "kl_loss = tf.reduce_mean(kl_loss)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "# Total VAE loss, which is the sum of reconstruction loss and KL divergence loss\n",
    "vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "# Add the VAE loss to the VAE model\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "# Compile the VAE model with Adam optimizer\n",
    "vae.compile(optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baed96f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Data Load and Model Fitting**\n",
    "\n",
    "Loads the MNIST dataset, normalizes the images, and reshapes them for the model.\n",
    "Trains the VAE using the training data for 30 epochs, using a batch size of 128. Validation is performed using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "12299a8f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "x_test  = np.expand_dims(x_test, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "837582d9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "vae.fit(x_train, x_train, epochs=30, batch_size=128, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0812fd4c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_images(num_images=10):\n",
    "    # Sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(num_images, latent_dim))\n",
    "    \n",
    "    # Decode them to images\n",
    "    generated_images = decoder.predict(random_latent_vectors)\n",
    "    \n",
    "    # Display the generated images\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.savefig('generated_image_02')\n",
    "    plt.show()\n",
    "\n",
    "# Generate and display images\n",
    "generate_images(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04062d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References and Credits\n",
    "\n",
    "[1] - Rocca J. - [\"Understanding Variational Autoencoders (VAEs)\"](https://medium.com/towards-data-science/understanding-variational-autoencoders-vaes-f70510919f73), Published in Towards Data Science Sep 24, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bcfcfb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e56e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This function `viz_latent_space` takes an encoder model and data as input. The encoder model is expected to have a method `predict` that can generate predictions from input data. It then extracts the mean (`mu`) of the latent space representation from the encoder's predictions. It visualizes this latent space by creating a scatter plot where each point represents a data instance projected into a 2-dimensional latent space. The color of each point is determined by the corresponding target data. Finally, it displays the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca639546",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def viz_latent_space(encoder, data):\n",
    "    \"\"\"\n",
    "    Visualizes the latent space representation of input data using the encoder model.\n",
    "\n",
    "    Parameters:\n",
    "        encoder (keras.Model): Encoder model that maps input data to latent space.\n",
    "        data (tuple): Tuple containing input data and corresponding target data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Unpack input and target data\n",
    "    input_data, target_data = data\n",
    "    \n",
    "    # Predict the mean and other latent variables from the encoder model\n",
    "    #mu, _, _ = encoder.predict(input_data)\n",
    "    mu, _ = encoder.predict(input_data, verbose=0)\n",
    "    \n",
    "    # Create a scatter plot of the latent space, with colors representing target data\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    plt.scatter(mu[:, 0], mu[:, 1], c=target_data)\n",
    "    \n",
    "    # Label the axes\n",
    "    plt.xlabel('z - dim 1')\n",
    "    plt.ylabel('z - dim 2')\n",
    "    \n",
    "    # Add color bar to the plot for reference of target values\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b9af232f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "z_sample = np.array([[-1, 0]])\n",
    "# Decode the latent sample into an image using the decoder model\n",
    "x_decoded = decoder.predict(z_sample, verbose=0)\n",
    "# Reshape the decoded image to its original dimensions\n",
    "digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n",
    "plt.imshow(digit.reshape(28, 28), cmap='gray')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d7d189d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "img_width, img_height = x_train.shape[1], x_train.shape[2]\n",
    "num_channels = 1\n",
    "print(img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0676d93a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = (x_train, y_train)\n",
    "\n",
    "viz_latent_space(encoder, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c4c98e9b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = (x_test, y_test)\n",
    "\n",
    "viz_latent_space(encoder, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c70b623",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def viz_decoded(encoder, decoder, data):\n",
    "    # Define the number of samples per dimension in the grid\n",
    "    num_samples = 15\n",
    "    # Initialize an array to hold the grid of images, setting its size based on the number of samples and each image's dimensions\n",
    "    figure = np.zeros((img_width * num_samples, img_height * num_samples, num_channels))\n",
    "    # Create a linear space for the x dimension of the latent variables, intended for sampling\n",
    "    grid_x = np.linspace(-4, 4, num_samples)\n",
    "    # Create a linear space for the y dimension of the latent variables inverted to display correctly in the plot\n",
    "    grid_y = np.linspace(-4, 4, num_samples)[::-1]\n",
    "    # Loop over each point in the grid (latent space)\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            # Prepare a latent space sample for the current grid point\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            # Decode the latent sample into an image using the decoder model\n",
    "            x_decoded = decoder.predict(z_sample, verbose=0)\n",
    "            # Reshape the decoded image to its original dimensions\n",
    "            digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n",
    "            # Place the decoded image in the appropriate position on the grid\n",
    "            figure[i * img_width: (i + 1) * img_width,\n",
    "                   j * img_height: (j + 1) * img_height] = digit\n",
    "    # Create a plot with specified figure size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # Calculate starting and ending ranges for the ticks on the axes\n",
    "    start_range = img_width // 2\n",
    "    end_range = num_samples * img_width + start_range + 1\n",
    "\n",
    "    # Create a range for pixel values to set x-ticks\n",
    "    # Instead of calculating end_range with img_width, directly use num_samples to ensure alignment\n",
    "    # This step calculates the positions for ticks to align them with the center of each image in the grid\n",
    "    pixel_range = np.linspace(start_range, start_range + num_samples * img_width, num_samples)\n",
    "    \n",
    "    # Round the grid values for labelling ticks on the x-axis\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    # Round the grid values for labelling ticks on the y-axis\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    # Use these lines to set the x and y ticks\n",
    "    # Set the x-ticks with the rounded grid values\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    # Set the y-ticks with the rounded grid values\n",
    "    #plt.yticks(pixel_range[::-1], sample_range_y) # Ensure y-ticks go from top to bottom\n",
    "    plt.yticks(pixel_range, sample_range_y) # Ensure y-ticks go from top to bottom\n",
    "    \n",
    "    # Label the x-axis to indicate it represents the first dimension of the latent space\n",
    "    plt.xlabel('z - dim 1')\n",
    "    # Label the y-axis to indicate it represents the second dimension of the latent space\n",
    "    plt.ylabel('z - dim 2')\n",
    "    # Check if the images are grayscale (1 channel) and reshape the array for plotting if necessary\n",
    "    fig_shape = np.shape(figure)\n",
    "    if fig_shape[2] == 1:\n",
    "        figure = figure.reshape((fig_shape[0], fig_shape[1]))\n",
    "    # Display the grid of images\n",
    "    plt.imshow(figure)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143789ff",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Function Definition**\n",
    "- `viz_decoded(encoder, decoder, data)`: This function visualizes the decoded images from a given encoder and decoder model pair. Specifically, it generates a grid of images decoded from sampled points in the latent space. The purpose is to see how changes in the latent variables correspond to changes in the generated images. It provides insight into how the VAE has learned to represent and generate data and helps in interpreting the continuous nature of the latent space and how different dimensions influence the generated outputs.\n",
    "\n",
    "**Initialization**\n",
    "- `num_samples = 15`: Sets the number of samples along each dimension of the grid in the latent space.\n",
    "- `figure = np.zeros((img_width * num_samples, img_height * num_samples, num_channels))`: Initializes a blank image array to hold the grid of decoded images. The dimensions are determined by the number of samples, the dimensions of each image (`img_width`, `img_height`), and the number of color channels (`num_channels`).\n",
    "\n",
    "**Generating a Grid in the Latent Space**\n",
    "- `grid_x` and `grid_y` are created using `np.linspace(-4, 4, num_samples)`, generating linearly spaced values between -4 and 4 for each axis of the latent space. This range is typical for visualizing VAEs as it often encapsulates significant variations. The `[::-1]` on `grid_y` inverts the array to ensure the y-axis goes from top to bottom.\n",
    "\n",
    "**Decoding and Arranging Images**\n",
    "- The nested `for` loops iterate over each point `(xi, yi)` in the latent space grid.\n",
    "- `z_sample = np.array([[xi, yi]])`: Prepares a latent space sample.\n",
    "- `x_decoded = decoder.predict(z_sample)`: Decodes the latent sample into an image.\n",
    "- `digit = x_decoded[0].reshape(img_width, img_height, num_channels)`: Reshapes the decoded image to its original dimensions.\n",
    "- The decoded image is placed into the appropriate location in the `figure` array, creating a grid of images corresponding to the latent space coordinates.\n",
    "\n",
    "**Visualization**\n",
    "- `plt.figure(figsize=(10, 10))`: Creates a figure for plotting.\n",
    "- The next few lines calculate the positions and labels for the ticks on both axes to correspond to the latent space coordinates.\n",
    "- `plt.xlabel('z - dim 1')` and `plt.ylabel('z - dim 2')`: Label the axes to indicate they represent dimensions in the latent space.\n",
    "- The check for `if fig_shape[2] == 1:` is to ensure compatibility with `plt.imshow()`, which expects either a 2D array for grayscale images or a 3D array with 3 (RGB) or 4 (RGBA) channels. If there's only one channel, the array is reshaped to 2D.\n",
    "- `plt.imshow(figure)`: Displays the grid of images.\n",
    "- `plt.show()`: Renders the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1dc6e32e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "viz_decoded(encoder, decoder, data)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "8",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
