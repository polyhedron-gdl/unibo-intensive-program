{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc3212d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/polyhedron-gdl/unibo-intensive-program-2024/blob/main/1-notebooks/chapter-08-02.ipynb\">\n",
    "        <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95511547",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning in Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ad9e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative AI\n",
    "\n",
    "- In recent years, generative models based on deep learning have attracted increasing attention, thanks to remarkable advancements in the domain. Leveraging vast datasets, meticulously crafted network structures, and sophisticated training methods, these deep generative models have demonstrated an extraordinary capacity for generating highly realistic content across different forms, including images, text, and audio. \n",
    "\n",
    "- Within the realm of these deep generative models, two primary categories emerge as particularly noteworthy: **Generative Adversarial Networks (GANs)** and **Variational Autoencoders (VAEs)**, each meriting special focus for their contributions and capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e8fc7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd3cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition\n",
    "\n",
    "- Variational Autoencoders (VAEs) represent a groundbreaking approach in the field of generative models, offering a unique blend of deep learning and Bayesian inference to generate new data samples that are similar to the input data. \n",
    "\n",
    "- Unlike traditional autoencoders, which aim to encode an input into a lower-dimensional space and then reconstruct it back to its original form, VAEs introduce a probabilistic twist to this process, enabling the generation of new data points.\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-02-00.jpg\" width=\"500\" height=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514455f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Structure\n",
    "\n",
    "- A VAE consists of two main components: the encoder and the decoder, just like a traditional autoencoder. \n",
    "\n",
    "- However, the encoder in a VAE, also known as the recognition model, ***maps inputs to a distribution over the latent space rather than a single point***. \n",
    "\n",
    "- This distribution is typically assumed to be Gaussian, characterized by a mean and a variance. \n",
    "\n",
    "- The decoder, also referred to as the generative model, then samples points from this distribution and attempts to reconstruct the input data from these samples.\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-02-01.jpg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1038e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*source:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a5cec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Functioning\n",
    "\n",
    "1. **Encoding**: The encoder takes an input $x$ and transforms it into two parameters in a latent space, $\\mu$ and $\\sigma^2$, which represent the mean and variance of a Gaussian distribution. This step introduces the element of probabilistic inference, as instead of encoding an input as a single point, it is encoded as a distribution over possible points.\n",
    "\n",
    "2. **Reparameterization Trick**: To enable backpropagation through random sampling, the VAE employs the so called reparameterization trick. Instead of sampling $z$ directly from the Gaussian distribution defined by $\\mu$ and $\\sigma^2$, the model samples $\\epsilon$ from a standard Gaussian, and then $z$ is computed as $z = \\mu + \\sigma \\odot \\epsilon$. This trick allows the gradient of the loss function to be backpropagated through the encoder.\n",
    "\n",
    "3. **Decoding**: The sampled latent variable $z$ is then passed to the decoder, which generates a reconstruction $\\hat{x}$ of the original input $x$. The goal of the decoder is to learn a distribution $p(x|z)$ over the possible inputs $x$ given a latent representation $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19cc74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-02-02.jpg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c0dc38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- As we have seen, in practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. \n",
    "\n",
    "- The reason why an input is encoded as a distribution with some variance instead of a single point is that it makes possible to express very naturally the latent space regularisation: the distributions returned by the encoder are enforced to be close to a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa7a52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training and Loss Function\n",
    "\n",
    "Thus, the loss function that is minimised when training a VAE is composed of a “reconstruction term” (on the final layer), that tends to make the encoding-decoding scheme as performant as possible, and a “regularisation term” (on the latent layer), that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution.:\n",
    "\n",
    "- **Reconstruction Loss**: This measures how well the reconstructed data $\\hat{x}$ matches the original data $x$. It ensures that the decoder learns to accurately reconstruct the input from the latent variables. A common choice for this loss is the Mean Squared Error (MSE) for continuous data or Binary Cross-Entropy for binary data.\n",
    "\n",
    "- **KL Divergence**: This term acts as a regularizer, ensuring that the distribution learned by the encoder ( $q(z|x)$ ) is similar to the prior distribution of the latent variables, typically assumed to be a standard Gaussian $p(z)$. The Kullback-Leibler (KL) divergence measures the difference between these two distributions. We can notice that the Kullback-Leibler divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48358e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Calculation of the KL Divergence**\n",
    "\n",
    "The calculation of the Kullback-Leibler (KL) divergence is a crucial part of training variational autoencoders (VAEs) since It measures the discrepancy between two probability distributions. In the context of VAEs, it's used to ensure that the distribution of latent variables learned by the encoder network is close to a predefined distribution, typically a standard normal distribution. The KL divergence between two probability distributions $ P $ and $ Q $ is defined as:\n",
    "\n",
    "$$ D_{KL}(P || Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)} $$\n",
    "\n",
    "However, in the case of VAEs, we typically deal with multivariate Gaussian distributions. If we consider $ P $ as the distribution learned by the encoder (latent space distribution) and $ Q $ as the standard normal distribution, the KL divergence can be expressed analytically:\n",
    "\n",
    "$$D_{KL}(q(z|x)||p(z)) = \\frac{1}{2} \\sum_{i=1}^{N}(1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2)$$\n",
    "\n",
    "Where:\n",
    "- $q(z|x)$ is the approximate posterior distribution (encoder output).\n",
    "- $p(z)$ is the prior distribution (often chosen to be a standard normal distribution).\n",
    "- $\\mu_i$ and $\\sigma_i^2$ are the mean and variance of the approximate posterior distribution respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c61cba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"./pics/ch-08-02-03.jpg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a48e02",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "VAEs have a wide range of applications in the field of generative modeling, including but not limited to:\n",
    "\n",
    "- **Image Generation**: VAEs can generate new images that resemble the training set, useful in art, design, and entertainment.\n",
    "- **Anomaly Detection**: By learning to reconstruct normal data, VAEs can be used to detect anomalies or outliers in datasets.\n",
    "- **Data Augmentation**: VAEs can create new data samples for training machine learning models, especially useful in scenarios where data is scarce.\n",
    "- **Dimensionality Reduction**: Similar to PCA or t-SNE, VAEs can be used for dimensionality reduction, providing a way to visualize high-dimensional data in lower-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1e757",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Implementation of a Simple Variational Autoencoder using Keras\n",
    "\n",
    "The following code defines and trains a Variational Autoencoder (VAE) using TensorFlow and Keras. A VAE is a generative model that learns to encode inputs into a latent space and then decode from this space to reconstruct the inputs. It's particularly useful for tasks like unsupervised learning of complex distributions, dimensionality reduction, and generative tasks. Here, the VAE is applied to the MNIST dataset, which consists of 28x28 pixel grayscale images of handwritten digits (0 through 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ef4f6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "268af609",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35af08",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Define VAE Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dd4bb0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fcc183",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***latent_dim*** is the dimensionality of the latent space. A smaller dimension forces the model to learn a more compact representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80adca18",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Encoder**\n",
    "\n",
    "Now we define an encoder network using Convolutional Neural Network (CNN) layers to map input images (28x28 grayscale) to a latent space representation. The encoder consists of two convolutional layers followed by flattening and two dense layers to compute both the mean and the log variance of the latent space. The encoder model takes input images and outputs the mean and log variance vectors, representing the parameters of a Gaussian distribution in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10245e23",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the input shape for the encoder network, which is a 28x28 grayscale image\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Adding the first convolutional layer with 32 filters, a kernel size of 3x3, ReLU activation function, \n",
    "# a stride of 2, and padding to maintain the input size\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "\n",
    "# Adding the second convolutional layer with 64 filters, a kernel size of 3x3, ReLU activation function, \n",
    "# a stride of 2, and padding to maintain the input size\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "# Flattening the output from the convolutional layers to prepare for the dense layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Creating the mean of the latent space representation using a dense layer\n",
    "# The size of the dense layer is defined by the variable latent_dim\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "\n",
    "# Creating the log variance of the latent space representation using another dense layer\n",
    "# The size of the dense layer is also defined by the variable latent_dim\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "# Creating the encoder model that maps input images to both mean and log variance\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a062ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Reparametrization Trick**\n",
    "\n",
    "The purpose of the following user defined function is to implement the reparameterization trick, which enables the VAE to backpropagate gradients through the stochastic sampling process during training. \n",
    "\n",
    "Explanation:\n",
    "1. The `args` parameter consists of two tensors, `z_mean` and `z_log_var`, representing the mean and log variance vectors of the latent space distribution for a batch of input samples.\n",
    "2. The batch size (`batch`) and dimensionality of the latent space (`dim`) are determined dynamically from the shape of `z_mean`.\n",
    "3. Random noise (`epsilon`) is sampled from a Gaussian distribution with mean 0 and standard deviation 1. This noise has the same shape as `z_mean`.\n",
    "4. The reparameterization trick is applied to sample from the latent space distribution. It involves adding a random perturbation (`epsilon`) scaled by the standard deviation (square root of the variance, computed as `exp(0.5 * z_log_var)`) to the mean (`z_mean`). This operation ensures that the sampling process is differentiable, allowing gradients to flow through the network during training.\n",
    "5. The sampled vector is returned as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54894c86",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a function for sampling from the latent space in the context of a Variational Autoencoder (VAE)\n",
    "# This function takes the mean and log variance of the latent space as input arguments\n",
    "def sampling(args):\n",
    "    # Unpacking the input arguments\n",
    "    z_mean, z_log_var = args\n",
    "    \n",
    "    # Determining the batch size dynamically\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    \n",
    "    # Determining the dimensionality of the latent space dynamically\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    \n",
    "    # Generating random noise from a Gaussian distribution with mean 0 and standard deviation 1\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    # Reparameterization trick: sampling from the learned distribution in the latent space\n",
    "    # The sampled vector is computed as the mean plus a random perturbation scaled by the standard deviation\n",
    "    # The scaling ensures that the perturbation respects the learned variance\n",
    "    sampled_vector = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    # Returning the sampled vector\n",
    "    return sampled_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07cc47e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9bef55",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8fa56f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "utilizes the `Lambda` layer in Keras to apply the custom `sampling` function as part of the model's computation graph. Let's break down the components and the significance of this line within the context of a Variational Autoencoder (VAE):\n",
    "\n",
    "*Lambda Layer*\n",
    "\n",
    "- **`layers.Lambda`**: This is a way to wrap arbitrary expressions as a Layer object in the Keras model. The Lambda layer is used for executing arbitrary code while building Keras models. It's particularly useful for operations that don't require learning weights.\n",
    "\n",
    "*Sampling Function*\n",
    "\n",
    "- **`sampling`**: This refers to the custom function defined earlier in the code. The sampling function takes the mean (`z_mean`) and log variance (`z_log_var`) of the latent variables as inputs to generate a sample from the latent distribution. This sampling involves adding stochasticity to the model, enabling the VAE to generate diverse outputs from the latent space.\n",
    "\n",
    "*Inputs to the Lambda Layer*\n",
    "\n",
    "- **`([z_mean, z_log_var])`**: These are the inputs to the Lambda layer, specifically the outputs from the encoder model that represent the parameters (mean and log variance) of the Gaussian distribution in the latent space. By providing these as inputs to the `Lambda` layer, the `sampling` function can use them to generate a sample `z` from the latent space.\n",
    "\n",
    "*Output of the Lambda Layer*\n",
    "\n",
    "- **`output_shape=(latent_dim,)`**: This specifies the shape of the output from the Lambda layer, which is the shape of the latent space vector `z`. Since `latent_dim` is set to 2, this means each sample `z` from the latent space will be a 2-dimensional vector.\n",
    "\n",
    "*Purpose in the VAE*\n",
    "\n",
    "The purpose of this line in the VAE is crucial:\n",
    "- It bridges the encoder and decoder by generating a latent vector `z` that is sampled from the distribution defined by the encoder's output (`z_mean` and `z_log_var`). This introduces the variational aspect of the VAE, as the model learns to encode inputs into a distribution over the latent space rather than a fixed point. The randomness introduced by the sampling is essential for the generative process, allowing the model not only to reconstruct inputs but also to generate new data after training.\n",
    "\n",
    "This approach allows the VAE to effectively learn a probabilistic mapping from the input space to the latent space and back, facilitating the generation of new samples that are similar to the input data by decoding points sampled from the latent space distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d9680",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b6d6f25",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define an input layer for the latent space with the specified dimensionality\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# Apply a fully connected (dense) layer to the latent inputs to expand them to \n",
    "# the size required for the convolutional layers\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "\n",
    "# Reshape the flattened output from the dense layer into a 3D tensor\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "\n",
    "# Apply a transposed convolutional layer to upsample the feature map\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "# Apply another transposed convolutional layer for further upsampling\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "# Apply a final transposed convolutional layer to generate the output image\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "# Create the decoder model that maps latent space inputs to output images\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea99be",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**VAE Model**\n",
    "\n",
    "The VAE model combines the encoder and decoder. The encoder inputs are passed through the encoder to get the latent space parameters, which are then sampled and decoded to produce the output images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52a7637",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# VAE model\n",
    "outputs = decoder(z)\n",
    "vae = keras.Model(encoder_inputs, outputs, name=\"vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e059b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Loss Function**\n",
    "\n",
    "The loss function combines reconstruction loss (to ensure the output images match the input images) and KL divergence (to regularize the latent space). The KL divergence encourages the latent space to approximate a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30175f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the context of the our code, the calculation of the KL divergence breaks down as follows:\n",
    "\n",
    "1. `kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)`: This line computes the terms inside the KL divergence formula, using the log-variance (`z_log_var`) and mean (`z_mean`) vectors obtained from the encoder network.\n",
    "\n",
    "2. `kl_loss = tf.reduce_mean(kl_loss)`: The computed terms are then averaged over the batch dimension, resulting in a single scalar value representing the mean KL divergence across the entire batch.\n",
    "\n",
    "3. `kl_loss *= -0.5`: Finally, the result is multiplied by $-0.5$. This scaling factor is applied to match the formula of the KL divergence for Gaussian distributions.\n",
    "\n",
    "By minimizing the KL divergence term along with the reconstruction loss, the VAE learns to encode input data into a latent space distribution that closely resembles a standard normal distribution, facilitating smooth interpolation and generation of new samples during the decoding process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec846bb5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Reconstruction loss calculation using binary crossentropy\n",
    "reconstruction_loss = tf.keras.losses.binary_crossentropy(encoder_inputs, outputs)\n",
    "\n",
    "# Scaling the reconstruction loss by the image size\n",
    "reconstruction_loss *= 28 * 28\n",
    "\n",
    "# KL divergence loss calculation\n",
    "kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "kl_loss = tf.reduce_mean(kl_loss)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "# Total VAE loss, which is the sum of reconstruction loss and KL divergence loss\n",
    "vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "# Add the VAE loss to the VAE model\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "# Compile the VAE model with Adam optimizer\n",
    "vae.compile(optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baed96f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Data Load and Model Fitting**\n",
    "\n",
    "Loads the MNIST dataset, normalizes the images, and reshapes them for the model.\n",
    "Trains the VAE using the training data for 30 epochs, using a batch size of 128. Validation is performed using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12299a8f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837582d9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "469/469 [==============================] - 65s 134ms/step - loss: 202.1346 - val_loss: 168.6634\n",
      "Epoch 2/30\n",
      "469/469 [==============================] - 66s 141ms/step - loss: 163.0874 - val_loss: 159.7561\n",
      "Epoch 3/30\n",
      "469/469 [==============================] - 61s 131ms/step - loss: 158.0744 - val_loss: 156.2722\n",
      "Epoch 4/30\n",
      "469/469 [==============================] - 62s 132ms/step - loss: 155.8115 - val_loss: 154.6574\n",
      "Epoch 5/30\n",
      "469/469 [==============================] - 63s 135ms/step - loss: 154.4551 - val_loss: 153.4707\n",
      "Epoch 6/30\n",
      "469/469 [==============================] - 63s 134ms/step - loss: 153.4612 - val_loss: 152.9804\n",
      "Epoch 7/30\n",
      "469/469 [==============================] - 60s 129ms/step - loss: 152.6361 - val_loss: 152.8952\n",
      "Epoch 8/30\n",
      "469/469 [==============================] - 60s 129ms/step - loss: 151.9424 - val_loss: 152.1724\n",
      "Epoch 9/30\n",
      "469/469 [==============================] - 60s 129ms/step - loss: 151.4809 - val_loss: 151.2854\n",
      "Epoch 10/30\n",
      "469/469 [==============================] - 61s 130ms/step - loss: 150.9991 - val_loss: 151.1277\n",
      "Epoch 11/30\n",
      "469/469 [==============================] - 65s 139ms/step - loss: 150.5818 - val_loss: 150.6238\n",
      "Epoch 12/30\n",
      "469/469 [==============================] - 60s 127ms/step - loss: 150.2163 - val_loss: 150.4015\n",
      "Epoch 13/30\n",
      "469/469 [==============================] - 63s 134ms/step - loss: 149.8940 - val_loss: 149.9458\n",
      "Epoch 14/30\n",
      "469/469 [==============================] - 63s 135ms/step - loss: 149.6372 - val_loss: 149.7415\n",
      "Epoch 15/30\n",
      "469/469 [==============================] - 68s 144ms/step - loss: 149.3920 - val_loss: 151.3875\n",
      "Epoch 16/30\n",
      "469/469 [==============================] - 69s 146ms/step - loss: 149.0645 - val_loss: 149.7955\n",
      "Epoch 17/30\n",
      "469/469 [==============================] - 70s 149ms/step - loss: 148.8661 - val_loss: 149.2719\n",
      "Epoch 18/30\n",
      "469/469 [==============================] - 60s 129ms/step - loss: 148.6308 - val_loss: 149.4101\n",
      "Epoch 19/30\n",
      "469/469 [==============================] - 63s 134ms/step - loss: 148.5191 - val_loss: 149.5578\n",
      "Epoch 20/30\n",
      "469/469 [==============================] - 66s 141ms/step - loss: 148.2601 - val_loss: 149.3972\n",
      "Epoch 21/30\n",
      "469/469 [==============================] - 64s 136ms/step - loss: 148.1210 - val_loss: 149.1705\n",
      "Epoch 22/30\n",
      "469/469 [==============================] - 65s 139ms/step - loss: 147.9224 - val_loss: 148.7342\n",
      "Epoch 23/30\n",
      "469/469 [==============================] - 65s 139ms/step - loss: 147.6717 - val_loss: 148.4935\n",
      "Epoch 24/30\n",
      "469/469 [==============================] - 64s 136ms/step - loss: 147.6588 - val_loss: 148.7110\n",
      "Epoch 25/30\n",
      "469/469 [==============================] - 64s 137ms/step - loss: 147.4446 - val_loss: 148.9971\n",
      "Epoch 26/30\n",
      "469/469 [==============================] - 65s 138ms/step - loss: 147.4032 - val_loss: 149.1598\n",
      "Epoch 27/30\n",
      "469/469 [==============================] - 60s 129ms/step - loss: 147.2300 - val_loss: 148.4522\n",
      "Epoch 28/30\n",
      "469/469 [==============================] - 58s 124ms/step - loss: 147.1065 - val_loss: 148.2710\n",
      "Epoch 29/30\n",
      "469/469 [==============================] - 58s 124ms/step - loss: 147.0124 - val_loss: 148.5325\n",
      "Epoch 30/30\n",
      "469/469 [==============================] - 58s 123ms/step - loss: 146.8845 - val_loss: 148.6812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x250988f2980>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the VAE\n",
    "vae.fit(x_train, x_train, epochs=30, batch_size=128, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0812fd4c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABiEAAACXCAYAAABzwvhEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzS0lEQVR4nO3d6Y/X1fn/8ctaZHMQh2Vk3xn2fVVRqgiiUqvSNjXV1qZJ09om9U9o0ju90/tNrGlM3GLdd1oqiOyIDDvDsAwDw7ANm4MCbf3e+OWX9lzXS+Ywnfesz8e9c+Xw4TPzOe9zzvvzzpzXDV9//fXXBgAAAAAAAAAA0My+1dpvAAAAAAAAAAAAdEw8hAAAAAAAAAAAAIXgIQQAAAAAAAAAACgEDyEAAAAAAAAAAEAheAgBAAAAAAAAAAAKwUMIAAAAAAAAAABQCB5CAAAAAAAAAACAQvAQAgAAAAAAAAAAFIKHEAAAAAAAAAAAoBDfzu14ww03FPk+0M58/fXXLfL/MO7w31pi3DHm8N+Y69AaGHdoDayxaGnMdWgNzHVoacx1aA2MO7SGxsYdfwkBAAAAAAAAAAAKwUMIAAAAAAAAAABQCB5CAAAAAAAAAACAQmRnQgBo2/6Xs/ha6rxANJ9vfSs+Q+7evXuodevWLdSuXr2atC9fvhz6XLlyJdQYJwDwv8tdr5lzAQAAAHQU/CUEAAAAAAAAAAAoBA8hAAAAAAAAAABAIXgIAQAAAAAAAAAACkEmRBvT1HP9OTe48/GZACoPoF+/fqH27W/Hy/78+fPXbJuZ/fOf/wy1nHHH2Lx+ah7w2Q7l5eWhz9133x1qpaWlodbQ0JC0t23bFvpUVlaGWl1dXdJWY+Jf//pXqDEG0Fb4a6upZ/MzpmGmx0/Xrl1DrX///kl78ODBoc+FCxdC7ejRo0n7iy++CH2YcwEAHY1aX2+88cZGayozL+d+5d///vf1vkUAQBPwlxAAAAAAAAAAAKAQPIQAAAAAAAAAAACF4CEEAAAAAAAAAAAoBA8hAAAAAAAAAABAIQimbgY+ACknNEn9O7MYGpwb/OprBBV2LDlBxZMmTQp9HnnkkazXX7t2bdKuqKgIferr60PNj8+rV6+GPiroi7F4bWpu8MGmixcvDn0WLFgQauozqaqqStq54W833XRTo32++uqrUGMMoGhqDOeEBnfp0iX0UWPTj2HGeeej5rvu3buHmgqdfuyxx5K2n8/NzCorK0Nt9erVSfvQoUOhz+XLl0NN7R0BoDnlrLuqT866SEhwx5HzfYe/vzAz69WrV6ipNdfXzp8/H/qcPXs21Pw+To1L9nAdmxqbuTW/J8y5d1A1vrNDZ8RfQgAAAAAAAAAAgELwEAIAAAAAAAAAABSChxAAAAAAAAAAAKAQPIQAAAAAAAAAAACFIJj6GlQIjQpO6tu3b9KeNm1a6DNmzJhQu+WWW0Lt4sWLSXv79u2hT01NTaidOnUqaatQJoIK2y8f4GUWgy2XL18e+sydOzfU9u/fH2plZWVJu0ePHqHPlStXQk2Fs3pq3DEW/yMnPNfMbNy4cUl74sSJoY+fi8x0KNaJEyca/f8U/15VmFbOv1MI4UKu3CB1Na4HDBiQtNWaruY6H/5bX1+f9e/UXMdYbx/8mFLr4tSpU0PtqaeeCjW/Lzx9+nToc+HChVDza7/aC6hxh5aj7hX8HKXmpxxqrsgN+s2ZZ5q6NueEbaL9UuNCzT1qjS0pKUnaXbp0CX3U3tGvsV988UXoc/Xq1VBT4441tvWoz9vfr5qZDR8+PGnPmjUr9Bk9enSoqfvOgwcPJu3169eHPmrs+HGoxk3ufQ5aV+59gd/HjRw5MvRR99e33XZbqPk5sba2NvSprq4OtUOHDiXturq60EeNV6Aj4S8hAAAAAAAAAABAIXgIAQAAAAAAAAAACsFDCAAAAAAAAAAAUAgeQgAAAAAAAAAAgEIQTP1ffLhcz549Q59BgwaF2tKlS5P24sWLQx8fhmmWF4i5YcOG0Ofdd98NNR9oqALEVLgS4V2tLyfAsLS0NNQeeuihpL1w4cLQ58svvww1H0pspsPOPTVefeiqeu9q3OUGK3ZEfp5R1+qtt94aaj4A9a677gp9unfvHmp79uwJtXXr1iXtysrK0EeNHf9ZNuec0pnHBK7Njw11zaj1evDgwaHmw+VU+KUKCPbhlyqkWL2vhoaGUFNh1Wg5aq5R4cJ+PlUh1L///e9DbebMmaF26dKlpK2CqauqqkLNj0X2ccXx40KNCbUP6t27d6jdcsstSbtPnz6hjwq67NatW9JWobvnzp0LtbNnz4aaHzsqwFyNHT8/qflKzZFqz8Bc1z7k3If4sWmmx7W/HtS6qMaFD2JV+1m1nuYEWDNHNg+1dvogahVCPXfu3FBbtmxZo33UPqu+vj7UKioqkvbhw4dDn5MnT4aaml/RPvixmDs/zZ49O2n7cWimw6pLSkpCzf+fx48fD33Wrl0bau+//37SVnPY+fPnQ43xWhw1t+VgbWk6/hICAAAAAAAAAAAUgocQAAAAAAAAAACgEDyEAAAAAAAAAAAAhei0mRDqvEt/pvSYMWNCH38Ov5nZY489lrTVOa/+zEQzfbasP+9Xvc/q6upQO3jwYNJW57XmngeL4qgz5/x5qb169Qp9ZsyYEWqPPvroNV/HzGzHjh2htmvXrlDzZw927do19Pnqq69Czf+f6j2ocdeZ+d/RzTffHPqoTIjp06cn7b59+4Y+6pzxnTt3htrGjRuTtsoJUfw5vv/L+ZT+WmAuglneOexqjpwwYUKolZeXh5pfG9XarMaifw/qfFh1rqs6+9pfN5zzWqyc887VPDxv3ryk/cwzz4Q+8+fPDzX1mdfW1ibtVatWhT4qv8evu2psMndeW24GiL/G1ZhQ+W5qfzZu3LikrfLkBg4c2Oh7UC5evBhqfnyZxXPRjx49GvqoLIkzZ84kbTWvKSqvhLmu7cm5D1Hnq5eVlYXa+PHjQ83fS6sxre4nfKbI5cuXQx91rn/OuFZjE9eWM07MYgbErFmzQp/vfve7obZo0aKkrfZ1agyoz9LnkKgcRfXe0T6o9dp/TzFkyJDQR2W0/uAHP0jaw4cPD33U/KcyavyYUnkoarz67+zU+q3W+c60fqr5Jye3q6n/Lnd+8P82NzvMU/cJTc3VUn3aw30BfwkBAAAAAAAAAAAKwUMIAAAAAAAAAABQCB5CAAAAAAAAAACAQvAQAgAAAAAAAAAAFKJTpPSokBIVOuND4ubMmRP6LF++PNR8GI4Kjjl37lyoqRAUH7SjAoJHjhwZaiNGjEjaKrxLBYF1ppCbtkAFYvoAGxVsrsadD073IYTfVKuvrw81H4ajwnFUyE1OkE97CMcpSk4gpgow8iFrZmaTJk1K2j169Ah9ampqQm3Tpk2h5sN5c8JzFRXI1Jyftwr0ygm07sxjri3zn13OGmgWg+N8SLuZ2bBhw0JNBbxfunQpaTc0NIQ+av67cuVK0lbvXQVkqjmguro6abMONx/1+/brrgoY9POrmdkvf/nLpK0CN/14MjPbuXNnqK1fvz5p79+/P/RRIZz+veeE7nV2OSGEap7xYaYqiHf27Nmhdvfdd4eaD6j0Yb3f9L58CHSXLl1CH/Xe1Xv141y9BzVn5YRJq/fFOGx71BhT9yF+z6lCXgcPHhxqkydPbrTf1atXQx91P6rmP2/37t2hpvavfo9LMPX1U9fzzTffHGp+7zV//vzQZ8GCBaHm733UvakPGDfLWydVQLD63sf/u5wQWBRLjTu15g0YMCBpz5s3L/R54oknQm3s2LGNvgcVCq3Ghn9faq1UIelTp05N2hUVFaHPqVOnst5De5OzRzfT16uvqflIfafi+/Xq1avRPmb6XsHvo9R9pvoOx48VNbcdOXIk1I4fPx5qfmycPn069FFj2M+dary25Pcn/CUEAAAAAAAAAAAoBA8hAAAAAAAAAABAIXgIAQAAAAAAAAAACsFDCAAAAAAAAAAAUIgOF0zd1EAbM7OHH344af/kJz8JfUaNGhVqPrCyrq4u9NmzZ0+oqUCQoUOHJm0VnjJu3LhQ80E7O3bsCH1UODaaR27QjgrM8aE23/ve90KfRx55JNR80NrevXtDn6qqqlBTITfnz59P2ipITtX8z50bsNpZgoNVKKAPdOzbt2/os2zZslDz172fd8zMVqxYEWqbN28ONR+8qz63nEBD9TnmhlPmjB31M/r/UwV1dZbx1ZblhMHecsstoY8KnV66dGnSVmugCgJTwV9+/lPvU70vP9+q8Opjx46FmgoH8/3U3IrG5a67fh81cuTI0OeZZ54JtYULFzb6HrZv3x5qL7/8cqj5AGAVzKrCfv34VOMV16ZC5FXgoF+LJ0yYEPpMmTIl1AYNGhRqfg1S9wU1NTWhduLEiaStwhLVfYGas/xYyQ2T9jUVvq7Gb2uHHCLOf+qeo1+/fqHmA6ZVyKsK+x0+fHio+XVerYEqrNW/9wMHDoQ+PrjdTId5qnUA30ytK2qOVHPdgw8+mLQff/zx0EeNOb8mqu8t1NhRn62fe0pKSkIfVfOvlTMfmjGvNRf1u1VzVs53diqEWt0r+M/85MmToc++fftCTd2P+nmsT58+oY+q+WBqdV2p99AR5H5P6wOgzcxuu+22pK2+Pxk4cGCojRkzJmkPGTIk9FHrm5oz/PhU712NFf9zNzQ0hD5qr1VbWxtq/nud3bt3hz6HDh0KNX9PrPZxBFMDAAAAAAAAAIB2j4cQAAAAAAAAAACgEDyEAAAAAAAAAAAAhWj3mRD+jC115qk6e3LBggWh9vOf/zxpq7Mu1Tlf/lzDf/zjH6HPtm3bQk2d9Tp37tykrc6fVefl+Zo6Jz33vH5cP3WepjqHWJ1xN378+KS9fPny0MfnRpiZrVmzJmmvXLky9FFZJOpMVT82mpoJofr4s9Q7O39+YFlZWehz5513hpofT+q8XHWmqjoXX81jnhrT3bp1S9rqvER1Nn/OmapqzlJj1dd8nomZPucQxck9T9ifw/md73wn9Pn1r38daj4rSa2B6nxNdT34sa/2DGfOnAk1P17VXKfOllU1NdZxbWoOyZmjzGJm1tNPPx36LFq0KNT8nFtRURH6vPvuu6G2a9euUPPnEKtzrVXOiB9naj1lb5fKySVS48SfN+zHjZnZiBEjQk2dCezP8VVn9u7cuTPU/JnAKv8h9z34ue3y5cuhj6r5OVKtp2oPocYmZ6cXR91j+PGixvCTTz4ZauXl5UlbnautzudX+1c/L6sxoNZPn4eirlG1r+Ae4/o19buT22+/PdT8Pas6p72+vj7U1q9fn7RXr14d+qjPVo1pv0dUGTlqzc3JWGIOaz45406NnyVLloSa38cNGzYs9FHjx+/vN23aFPp8+umnoab4rIHZs2eHPn5fYaa///M66rymrqfcPay/htXvUWV5zZw5M2mrjBqV3aHeq/9c1B5Krc2euo9V2SDqvfo92YULF0IflQPs/53ax7Xk/QR/CQEAAAAAAAAAAArBQwgAAAAAAAAAAFAIHkIAAAAAAAAAAIBC8BACAAAAAAAAAAAUot0HU+eElEyfPj3UVDDh6NGjk7YKJFHhch9++GHS3rhxY+ijQkNU4JwPVxo3blzoo0KKfcCJChbJCetDHh9mpUJo1FhUoTPLli1L2qNGjQp9jh8/HmovvfRS0t6yZUvoo4J9c0JR1fjJCUkiIDOlQs98OLkKAFRBRD7IT4Wk1tTUhJoKTfJUYJsa0/59DRgwIPSZOHFiqPXv3z/U/Disq6sLfVRA8OHDh5N2VVVV6KNCD9XYJHDu+qmxouY6FZ66dOnSpP3UU0+FPj7ozSyGtapxvn379lDbsGFDqJ06dSppq9DVpgabV1dXZ9U6auBckdS469GjR6gNHz481H77298mbb/mmumQuM8++yxp/+lPfwp99u3bF2oqEK53795JW/08at7KCabGtal1WK1vJSUlSVuF7qq5ToX7HT16NGmrewe1r/NrkhqXKsxTBfb6NVbtBU6fPt1o7fz586GPmiPZ/xVHjVc/p5jFYNRf/OIXoc+CBQtCzY87Naa//PLLRv+dWZyz1Dg/cuRIqPmgdnX/UllZGWo+BN4s7z6nM2tqwOtPf/rTUBs5cmTSbmhoCH1WrlwZaq+//nrSVvs6NcZV6LT/7kT9POq7E/97UN+TqBr3Dk3j5zH1+d5xxx2h9qtf/SrU1Hcl3oEDB0Jt7dq1SXvVqlWhj1+/zfSezVPf2fnrwyz+Hvw9Tkemrie1r1VrkN9bl5aWZv2f/vXVWqY+czWX+VpOALRZ/MzVPDZ27NhQU3s7f91069Yt9FF7R/8eWnse4y8hAAAAAAAAAABAIXgIAQAAAAAAAAAACsFDCAAAAAAAAAAAUAgeQgAAAAAAAAAAgEK0q2BqFS7XtWvXpD148ODQRwVzTZkyJdR8CMqhQ4dCn1dffTXUfPilCsny79NMh4b4n1GFK6lQFx8kpwIOWzuApL1Sv2//OanPV4XJqCClOXPmJG0VxqOCk/bs2ZO0VRCbeu8q1ND/POq1VOigf6+EZjbOjws1Z6mx44OOVIiSCvJT170fF2ou6tWrV6iNHj06aavgusmTJ4eaD/w0i2PHv7aZHqubN29O2rmhmarGnNg4H8amguTKy8tDbcmSJY3WVKiYCnDdtm1b0v7kk09CHxXUnhMwrfqo0DIf6qrmOhVipsJgCXBtnA9QUyHUAwcODDUVnOnHnQp59eupmdlzzz2XtLds2RL6qDlX7Qc8NX6ausYyj10/NQb8GFP3HOr3r+YQPxeoPZWq9ezZM2kPGzYs9FFrpbpX8Pcw6p5GhQafPXs2aav5UN1jMK81DzV/9OvXL9RUgOvjjz+etOfNmxf6qHXKB6NWV1eHPnv37g01Fa557NixpO0Dp83M6urqQs3fO6u18+LFi1k1gqn/I+c+sKysLPR58sknQ23mzJmh5tcotRd7//33Q83PR2qe8fOhmZ57ctZJNeejOCrI2d8LqiDnp59+OtRUPz8/qLB79Z3dp59+mrRVeLXaU6l7H99P3Ter+1i/xqo5rKPu6/6X8Hd/7at5Xu3JT506lbT9d6Zm8TMx0+ugH2fq/1OfuV8r1fcnffv2bfTfmcVxpq41tR/z82JrjzH+EgIAAAAAAAAAABSChxAAAAAAAAAAAKAQPIQAAAAAAAAAAACF4CEEAAAAAAAAAAAoRJtN6ckN1PVhXT/+8Y9Dn3vvvTfrtfbt25e033jjjdBnxYoVoeaD3VSYlgpXUkFjPqRWBXeeOHEi1M6dO5e0CaZuPjljUQXJ9e/fP9QWL14cakOGDEnaKiRpx44doebDi1WgjQriUuPAhyKpoB0VmJ0TctOZx50KC/Lhl2rsqEChnGtcUeGafuzceuutoY8KxPShdLfffnvoo8a9uoZOnjyZtFWQnJoj/VxaU1MT+qigUBXAqcZ0Z5YzVoYPHx76qHlt2bJloebnOvWZrFy5MtRWr16dtA8ePBj6qIBydR35+Uit12pc+OstZz78pveAlBp33bp1S9pqLlBjTAVT+0DBqqqq0OfZZ58NtU2bNiVtNVbU+1JrsQ+SU0GEKhjPj1fW2OunxpfaG/maunZVeKoKR/SvpYKj1djx6+7EiRNDHxVUeObMmVDz4/zo0aOhjw9sNDO7cOFC0lbjPmduRR6/T1Sf79y5c0PtvvvuC7UpU6Y0+v+poGg/flSIuQp+VQGffpype1a1R/M1dV2pfa9adxmL/6HuQ/wYW7p0aejzwAMPhNpNN90Uanv27EnaH3zwQaN9zOI9jZqTVRhwnz59Gu3n9w/fVPM/j1or0Dj1e1Pfe40fPz5pP/PMM6HPnDlzQk3dQ+7fvz9pv/POO6HPW2+9FWr+nvHSpUuhj/qOUI0fPxZVwLvi50219+uo9w65e1g1//u5TP27+vr6UKutrU3a6v5Nffe2a9euUPPrmVp/1Dzpx8bAgQNDH/Uzq2vL70PVeqpey+/lWnudZLYFAAAAAAAAAACF4CEEAAAAAAAAAAAoBA8hAAAAAAAAAABAIdpMJoQ/702dx6bOAZw3b17S/v73vx/6+DPYzczq6upC7ZVXXknar7/+euijzh/3Z3Op87vUmYzqbG1/Rph6LXWGa3V1ddJWZ5ShcblZJP6cdHWG69ixY0Nt6tSpjb6+PyfTTOcGjBkzJmn789bN9DhQ5+X5cZ17Vn5HPbOwuajx5K/pQYMGNdrHLH5u/po30+eM55wHq86dHj16dKhNmzYtaQ8dOjT0UedHqzP8/RysriF1pqg/V/GOO+4IfdQ5xerMTX++cGufj9iS1NhU84z/fc+YMSP0Udkgan3z41rNdeocS/8e1LnQ6sx1lRPhzztXn7ma18i/aR5q3KnzU31OzV133RX6/OxnPws1NQ589og6O/jYsWOh5uc3NUep+dvnPyhqTlQZKeqcbFybH2Pqd6jGYUNDQ9L2c8U31dR84f/P8vLy0EetlX7MqX2dWst8pp1ZPDNb/bucTBzmteaj9nb+HlXdO6hsw9mzZ4eaH3dqv6/2937eVGusGvsqJ8Kfmd3UvCbyH65fzvgyMxs1alTSfvzxx0Mflcegsmf8ufuffPJJ6KNyBv29iVq7S0pKQm3kyJGh5tdh9T2Jv283i+f8q3UBkf89qTHm8x/MzJ544omkvWDBgtBHrdfqfvfVV19N2m+//Xboo+YnnwGhPnOV/6DO8Pf3yWoMq/P6N27cmLTVvXtHndfUz9Wc87q6r/Rrid/rmcWcym96LX+P2tT3nps7ovj3pcaPGndt7TsP/hICAAAAAAAAAAAUgocQAAAAAAAAAACgEDyEAAAAAAAAAAAAheAhBAAAAAAAAAAAKESbSbzzwTAqQEgFu/mQGxUSqEKxVOi0D1dSwYEq0MsHbKmQRRXao8LHfCidCi6pra1ttKYCvVo7gKQtygkwVGGtPnhSBRbNnz8/1NT49CG6Kjxw8ODBoeaD2lUAsQ9NN9Pjx48NFWijxg8hXteW8/tRc4oKOvJjU/VRY0DNRz6UbsqUKaGPD6E2M+vXr1/SVmNVBYGtXr061HwQ7G233Rb6qMBjH8iurinfx8xs+/btoebD8jrTHKnGiprrfDhhaWlp6KPCA1VAop/rVOiXCrT2rz9gwIDQR63XKqzVf+bqOlLh2H6dJ5g6T84aqwKfZ82albR/85vfhD5+HjPT+z0flHno0KHQR62xfl0fNmxY6KPeuwpd9fO8WmPV9Zcz7pDyc4+a69T85MeqWt969uwZaupz83t5H7RupkMs/b2CCnTdtWtXqPkQarP4/n0gp5me//zvIed3hUj9jtQ9nb+fGDFiROijAsrV3OPnB/UeVLCvX/P27NkT+qiwXxUY68eZuh/NQQh14/znq8ZX//79Q23ZsmVJW+2Z1dzzzjvvhNrKlSuTtt/bm+n7EP89jwrCLisrCzV1r+D/rVpf1e/Gj3vGV6TmEL+PU5/Jgw8+GGpLlixJ2ure4fjx46HmQ6jNYhC1ugdQ48DPK2pPqgKC1b3J1KlTk7Za0w8cOBBqmzdvTtrqWuuoY1H9XGrPrO7D/HdaZ86cCX3UZ+73gGpPoz4D9R78a6mfR813fp/Yt2/f0EfN1ZcvX270vV64cCH0UTX1e25N/CUEAAAAAAAAAAAoBA8hAAAAAAAAAABAIXgIAQAAAAAAAAAACsFDCAAAAAAAAAAAUIhWCaZWgSA+BMYHoJrpkNI5c+Y0+v/t2LEj1P7617+Gmg/YUmEgOeG8Kixs5syZobZo0aJQ8wFl9fX1oc/f//73UPMBJCrQq7PLCYlTwVUqiNDXVDjuxIkTQ00FlNfV1SVtFSatgpNy3ntuiKwPkjt8+HDokxMm1FGDlJpKhRr5kD4VFKTGgA9gUmMpJ6jQLAa6jh8/PvRRQa0XL15M2hUVFaHP1q1bQ00FaXoq3FMFGvqaWk8UH4LXkeUEyak5Rc11OQGkKpzSh1CbxaBU9fn26dMn1HxAsG+b6UA4Fczl34O6RnNqbS3gqy3IGXfqOlRBrMuXL0/aKkxVBcnt3Lkz1Gpra5O2Cn9Te06/pqo5Sv08KoTYz+kqHFvJWS86MzXmfE3NM2p/f+7cuaStQs5VYKWas3xQ6qRJk0IfNU78e62srAx9/DpsZtbQ0BBq/tpTY6ep9wpqX9HZ9385gd5q3fX/Ljf8XI3PK1euJG01xtTr+9BpFQ6bOxb9mGrqnNXZx5On5jofkqrmJxUafNdddyVtFaSqwu59CLVZvIdVn7e6Fvz6qvZwqqbuc/z14a8Ds7zgWcLQI/XZ+XnljjvuCH1UMLX/rsR/92BmtmbNmlB78803Q81/T6ECidVYzJlv1Xr92GOPhdqQIUOSthp3n3zySaj5eyb17zrTuMsNq/b7HLWe+ns8s/iZ+3nTTH8Gau+Ys2dS38f5PaGal9X3On5fahbvr9V3xWqPq+5tWxN/CQEAAAAAAAAAAArBQwgAAAAAAAAAAFAIHkIAAAAAAAAAAIBCtEomhDrX0J/np84of+CBB0LNnw1YU1MT+rz44ouhduDAgVDzZ/bm5D+YxRyH6dOnhz4PP/xwqJWXl4eaP99s3bp1oc+GDRtCzZ+r19nPDc45O9MsntumzndWZ2X6MwTV+fnq3HJ1fqo/11CdkejPazWL5zTmnnOtzi/2Z73mnoNHJsS1qevQn0uqzq1Xv0c/nqZNmxb6nDhxItROnjwZan78qrNS1fs6ePBg0t69e3foU1VVFWrqHEJ/PmLfvn1DH3X2q6euF6UznfWq5jo/t6lsGP+ZmMU16ciRI6GPGivqfFa/xg4YMCD0UWf/Dx8+PGmr+VaNA3U+sn8PuedmdvY1NUfOuc/qM1+4cGGozZ8/P2mra1XlP3z++eeh5s9OV2ul4tdBNVbU2a/qHGs/7tSZ6+osW7UW49r8WFFjR80Xfq1U84f6vNV48muXeq2cc4nVPJqTb6b+rXotNdf596DGYEddO/8X/vNU9xPqzGd//6uyHtS9oPoM/Bp+zz33hD7qnsav69u3bw99fC6ZmR4/OdcfmocfcyoDZPbs2aHmz7JXe7iPP/441FSWkZ8v1Fynxpzfg6r7Vb/3M9Nnqft9qr9XMYvZUGZxTWesRuqz85/V3LlzQx+V9eXHq8oLVJmt6vP061lO/oNZnIMnT54c+vzoRz8KNfUz+tdS8+Zrr70Wan4uJcc1yskvUGtSU+V+R+DHmdrH5cx36l5XvZbP3DGL9w8qN0LtcdvafSx/CQEAAAAAAAAAAArBQwgAAAAAAAAAAFAIHkIAAAAAAAAAAIBC8BACAAAAAAAAAAAUolWCqVV4oQ93GTZsWOijQjN9qJAKhamoqAg1FcTq5QQSm8UQ7Yceeij08SGLZjGMzMxs3759SVsF2qhAQx8c19nDlVQYkRp3vqb65IQTqrGiPgMVTO0DwVSQ8NGjR0PNv1f1PtXPo4JpfLBYQ0ND6EM44fVTQUc+QGjLli2hz6RJk0LNh8tNnz499FHBqSpI2IfSlZaWNvo+zeLYUSGLvXr1CjUVMO1Dy2bOnBn6qBBbf62pnzk3QLEjUHOdD+kzi5+5X7fM9Djwc1ZOoKuZ/lw8NRepecxfDyrEXIUUq0AvH/qZE6yJPGos+utVhUqqsej3e/X19aHP+vXrQ00FZ/r1WgXGKn6/p0I/1XynAoCrq6uTttqXnjp1KtT8+GRsNi4nGFd9Rn7dUCGBKnRczSG+ptZTH1au/p3fm5npeVPNy/6aUYHH6vVz7ic6+zhUe2u/zxkzZkzoU1ZWFmo5c6S/NzTT9xP+/zx27Fjoo8a+n0tVEGxucGdnHxtFUeurn6PUXnvw4MGh5tc29Z2IWktVPz8f5d6L+u9A1LgfNWpUqKnvYfx9zsaNG0Mf9d2Jn4PbWnBrW6C+3/DzjAp3VuvniRMnkvabb74Z+qiwarVW+utBfaem9mdTpkxJ2j/84Q9Dn0WLFoWaGnf+u5kXXngh9FF7PQLRG6d+J35/pL6XUvNkU/+/plLvy89vKrhdzT9qHj5w4EDSVvu/9hB2zl9CAAAAAAAAAACAQvAQAgAAAAAAAAAAFIKHEAAAAAAAAAAAoBA8hAAAAAAAAAAAAIVolWDqnLA3FUytAmZ82NumTZtCHxWaqQJIfFihCiFUAYr3339/0r733ntDHxWkqUKSnn/++aS9du3a0McHGZsRptRU/vemfo8qYNUHvqh/pwK81LjzIdAqwFCFzviQGxXgpYJmVYCrD6E7e/Zs6EOA6/VTvx9//ao5q7y8PNTmzJmTtAcOHBj63H777aE2YcKEUPNBrWpOVuGFJSUl13wdMx0m3b9//1DzYXmqj+Lnza1bt4Y+27ZtCzU1b3bU8auCqQcNGpS058+fn/VaPnhNhT2rOVKF2fnAQhVmN23atFAbPnz4NV/HTAd37t27N9T8fNsewrvaIhX+pmo+LFCFZKq5zL+WCnrLDUr114MKplahg0OHDk3aY8eOzXoPW7ZsCbX33nsvaat5SwXNsre7fn4MqL2LCkr1Ic1q36XmVrXm+TW1srIy9FGv7+8VVNim4gM/zWLQuQr3zL2GkFJznb9nnDdvXuij7mP9mqQCxNWeXH1Oft3t0aNH6KPC1auqqpK2motYK9seP8+o7y383s8szn9qv58bdurnKBWOrUKn/Xp6zz33NNrHTO/l/Xcln332Wejj50OzOOcz90Vqv+S/o1NroNqz+e8f1F5JfXei7jX9+1L7SH/fbBa/s5sxY0boo64jtcb++c9/TtqvvPJK6HP69OlQYy5tmpzrsy1cw+oedcmSJUm7T58+oY8KoVb3tjU1NUnb7yHM2sbvoTH8JQQAAAAAAAAAACgEDyEAAAAAAAAAAEAheAgBAAAAAAAAAAAKwUMIAAAAAAAAAABQiFYJplaBXj50pnfv3qGPCrr0gZgqvEaFQqvX8mFdM2fODH18sIiZ2ezZs5O2ChtRQTvvvPNOqL311ltJWwXaEBDcNOp35GsqBFIFEarx46lQJhUS5wOQhgwZEvqUlZWF2sSJE6/ZNtMh1yqYev/+/UlbBRgSkNk8/PWrPqNVq1aF2t133520VeCWCidXodOeCslSQUc+vM4HBpvpkGJ1LfjwOjVH+rBEsxhAt379+tBHBdCp4L3OTM1FPojXLAYDqs9XhViqOdLXxo0bF/qoAEM/hj/88MPQ59133w01FWBIIFzzyFlPzeK6odYRtSf0VHD01KlTQ00F3Ps1XIVjqzXW7x3Vz7dt27ZQe/vtt0Pt448/Ttpq3mdsXr+cMad+r2oc+poKr1aBgyro16+fmzdvDn3q6upCzd9PlJeXN/o+zfJCZNW/496hadSc5eeokSNHhj5qzvJ7wsOHD4c+x44dCzUV2uv3her+Rb3WmTNnkrZa53PnfLQcf02rfa4PX1b91L9Ta+KIESNCze/P1D2sGvc+EFjdv6jvO7Zv3x5qb775ZtJWAa9qjuS+tnFqDvH3Dz6c3Eyvn76mxpjat6v7WP9vFy9eHPrceeedoebnZbWmq/fwxhtvhNqLL76YtFV4Nfu6jsWv/Wpsqnvp6dOnX/N1zMwOHDgQahUVFaHmvxvOXa/bGv4SAgAAAAAAAAAAFIKHEAAAAAAAAAAAoBA8hAAAAAAAAAAAAIVolUwIdU6VP3NOnQvdvXv3UPPn+c2aNSv0Ued1qTPv/Tlx/mxWM33Ol39f6jzyF154IdT+8pe/hJo/q1Odpd4ezvlqi3LODlZ91LmG/iw3NZ7UWZYqL2TSpElJW51Xrcawv0bUGf5r1qwJtXXr1oWaP5uY3JHi+N+jOqdU5Xb87ne/S9qPPvpo6HPfffeFmpqz/Hmeam7NmW/VODl58mSo1dfXh5o/s3Xr1q2hz969e0Nty5YtSfvo0aOhj7oWOtPZr+pnra2tTdrHjx8PfSZMmBBqAwYMSNo+w8YsL+fJLM6Tan1T51X73I+XXnop9FHnaLfXczLbKzUO/FhU851aP30/lQnh104zfU66fw/qDH91zfhzgTdt2hT6qL3d559/Hmp+DlTnBDM2i5GbheA/EzVO1Hqq5rqDBw8m7X379oU+6qx2v9fLyXQy0/vUzrTmtbScHBufe2Wm11i/po4ZMyb0UZlZaq7z42XDhg2hzwcffBBqR44cSdqcY94++M9JjRNV8+trSUlJ6LNgwYJQU/Of//5m4MCBWf/OZy6pvcGnn34aas8++2yo+ZwIdQ/AmG4atVb6DIjce0ifF/Lggw+GPirPo1evXqHm59LJkyeHPjn3K/7eyMzs+eefz6r5+xW+O+l8VB7K0qVLQ82PRXX/+9lnn4Wauh58hk973evxlxAAAAAAAAAAAKAQPIQAAAAAAAAAAACF4CEEAAAAAAAAAAAoBA8hAAAAAAAAAABAIVolmFrxoRoqVEiFF/qA1Xnz5oU+I0aMCLVu3bqFmg8Nuemmm7Legw9Kfe+990Kfl19+OdRUAKcPTiLQpmlyQqjNYoiQCgq8ePFiqPkQwJqamtBHBeaWlpaGWv/+/ZO2CrNTgVr+9f04NDP76KOPQm3Pnj2h5q83xl3LUb9rFdDmw06rqqpCn1deeSXU7r333lCbOHFi0u7Xr1/oo+ZIX1PztA/kNDM7ceJEqPnwbfXznDp1KtTOnj2btFX4cGcPoFPhaD58fsWKFaGPDxg0M5s7d27SViFuas5SYV0+DLuysjL0WbduXajt2LEjaau5VV0znX0ctDQ1l/k1VX12/vM1i6GVZWVloY8Kh1X7Nh8A50Oizcw2btwYaj4kToXGqXnLB1qbsbdrTSq0WYVJ+3lM7df8PYeZ3jf6MFj1/6kwWB/Aqe45zp8/H2pqzvd7XsZcsXwotPpM1NzgA9B79uwZ+qi12a+nZmaHDx9O2urec/Xq1aF28uTJpK3WTsZP61K/f/85qblh3759oTZjxoyk7e9DzWKI8DfV/NzmrwMzvU/396LqHva1114LNbVf8Pci6tpD49SeSs0Ffr+t5jUVTO33cffff3/oc+7cuVDzc6SqqXGnQtl9iPnbb78d+rz66quh5u+hzFhjOyO/n+zdu3fos3jx4lDz41N9v7Fp06ZQU9dWRxln/CUEAAAAAAAAAAAoBA8hAAAAAAAAAABAIXgIAQAAAAAAAAAACsFDCAAAAAAAAAAAUIhWCaZWgUE+bHTz5s2hjw9TNTMbPXp00vahbmb5Qb++5t+TmdmaNWtC7bnnnkvaPvTGTAchEppZnNzQFh8qpAIGVSiM71dRUdFoHzM9NsaOHZu0VcCqCif04VwqrMuH1JnpMGFCM9sWFaLux5OaU1QonQoi9+GaKsxVBSH6QKacIFozHcrpryv173ygrFleEFhnGr/qZ1VrrA9oU+Pi4sWLoeb7qf/Phwib6fXNB2mqUF8VYu7nPxVyqH7mzjQO2oKceaumpib0ef/990PNf+YjRowIfVT4uZoXz5w5k7QPHDgQ+nz++eeh5oPj1PXx1VdfhZr6PaBtUQGcfn1T40uFbaq10ge9zpkzJ/RRr+9rx44dC32qq6tDTQVwEppZHHWN+89A3S+qfzdq1Khrvo6Zvh/dv39/o7W1a9eGPmqO9PM0c1jbo65f/zmpNUrdd86cOTNpqzD0kpKSrPeQ891JZWVlqPl1X41VNf+p/R/jtXmoz1eNqV27diVttX+aP39+qJWWliZtFTjt70/N9Ofr7w/V9x0fffRRqL388stJW+0H1b00Ywxm8fuM8vLy0GfkyJGh5u9R9+3bF/qo+a4jf1fMX0IAAAAAAAAAAIBC8BACAAAAAAAAAAAUgocQAAAAAAAAAACgEK2SCaHOt/Jn9qpz3Hr37h1qixYtStqDBg0KfdT5gXV1dY3WVqxYEfr87W9/CzV/hjXnFbZNOefhqrGZc/646qPOxdy6dWuo+fOEc9+DP4s690z9zn6GfnvlP6Oc82HN9BhQGSNNeQ/qXO2cs7bNmj4n5ryHzj6e1c/vx4GaZ2pra0PNZ3fknAms/j+zOGepPirbgbPN2wd1TfvPU521q8429+eiq7ODVR6DOu/cv5b6d6rmx7X6+RiLbZ9af3LyjNS+S811KlNp+vTpSVutU2r+8+daq3w8db2oLDHuO1qWPzt99+7doY8ad6tWrUraKrdNZemcO3cu1Pz8p14rZ41F++A/N/V5q/yv119/PWmrLK4hQ4aEmpqzTp8+nbQPHjwY+mzatCnUfCaYyl8k66v1qftFvy6p9U3tqfz5+ep7PfX/qbwHf6a++t5Qfefi5+nc70nQ+ahx7cfswoULQx+VRezHmZon1f1LRx6L/CUEAAAAAAAAAAAoBA8hAAAAAAAAAABAIXgIAQAAAAAAAAAACsFDCAAAAAAAAAAAUIgbvs5MvFDhHM36Rtzr33jjjaFPSUlJqPXp0ydpl5WVNfraZjFIySwGJqrQLxVUlxMY29G01M9Y9Lhrqpz3lRvam/NaOYGYjLvm0VbHXHvW1HGf83kXPSY66lyXGyCu1mIvN7A3Z87qDPNYjo467hQ1xnwtN1xYBQfnjDuCWf+fjrDG5txPdO3aNdT8/cTEiRNDnzlz5oTa0KFDQ82Huqrxq0JkP/zww6S9fv360Ke6ujrUfPi6WQx1batza3uc69Rr+TGlAst79uzZ6GupOUyNFdXPh1/mzIedVUeY63Ko+a9bt25Ju1evXqGPqqnx5AOI1Vi9dOlSqPn5Sb12R9Me5zrFr2dqristLQ01H+qrxpj6HdXV1YWaDzLPWQO/6fU7uo4y7lqamjsnT56ctP/whz+EPiqs+uTJk0n7j3/8Y+jz4osvhpoKq+4oezv+EgIAAAAAAAAAABSChxAAAAAAAAAAAKAQPIQAAAAAAAAAAACF4CEEAAAAAAAAAAAoxLdb+w38fz68QoXJnD17ttFaVVVV874xQGgLgblAe0EAcduTG+rbGcIC0boYdyiSCh1X9xgXL15M2rt37w59ampqQk2FTvvXv3LlSujjA13Ve8gJdDVjPW1p6vftP0/1+fowVbO8ME8+XzRVTtC5CpM+fvx4Ye8J7ZtfU9VcV1tbm1UD2gK1Dnfp0iXUhg4dmrTV2qy+r/Z7R/V9tZqr1f61o+AvIQAAAAAAAAAAQCF4CAEAAAAAAAAAAArBQwgAAAAAAAAAAFAIHkIAAAAAAAAAAIBCtJlgagAAAABoKh8UqIIDVVC0r507d67Z3gPwTRgrAAC0ntxg6vPnzyftXbt2hT5Xr14NtbVr1ybtbdu2hT4XLlwINRVM3VH2DPwlBAAAAAAAAAAAKAQPIQAAAAAAAAAAQCF4CAEAAAAAAAAAAApxw9eZB0ups7LQebXUeWSMO/y3lhh3jDn8N+Y6tAbGHVoDayxaGnMdWgNzHVoacx1aA+OuaW688cZQ69q1a9Lu3r17o33MzM6ePZu0L1++HPqo/If2rLFxx19CAAAAAAAAAACAQvAQAgAAAAAAAAAAFIKHEAAAAAAAAAAAoBA8hAAAAAAAAAAAAIXIDqYGAAAAAAAAAAC4HvwlBAAAAAAAAAAAKAQPIQAAAAAAAAAAQCF4CAEAAAAAAAAAAArBQwgAAAAAAAAAAFAIHkIAAAAAAAAAAIBC8BACAAAAAAAAAAAUgocQAAAAAAAAAACgEDyEAAAAAAAAAAAAheAhBAAAAAAAAAAAKMT/AdoaKltbeLneAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_images(num_images=10):\n",
    "    # Sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(num_images, latent_dim))\n",
    "    \n",
    "    # Decode them to images\n",
    "    generated_images = decoder.predict(random_latent_vectors)\n",
    "    \n",
    "    # Display the generated images\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate and display images\n",
    "generate_images(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04062d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References and Credits\n",
    "\n",
    "[1] - Rocca J. - [\"Understanding Variational Autoencoders (VAEs)\"](https://medium.com/towards-data-science/understanding-variational-autoencoders-vaes-f70510919f73), Published in Towards Data Science Sep 24, 2019"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": "8",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
